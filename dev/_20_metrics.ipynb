{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.imports import *\n",
    "from local.test import *\n",
    "from local.core import *\n",
    "from local.layers import *\n",
    "from local.data.pipeline import *\n",
    "from local.data.source import *\n",
    "from local.data.core import *\n",
    "from local.data.external import *\n",
    "from local.notebook.showdoc import show_doc\n",
    "from local.optimizer import *\n",
    "from local.learner import *\n",
    "from local.callback.progress import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp metrics\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "> Definition of the metrics that can be used in training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the function that converts scikit-learn metrics to fastai metrics is defined. You should skip this section unless you want to know all about the internals of fastai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export core\n",
    "def flatten_check(inp, targ, detach=True):\n",
    "    \"Check that `out` and `targ` have the same number of elements and flatten them.\"\n",
    "    inp,targ = to_detach(inp.contiguous().view(-1)),to_detach(targ.contiguous().view(-1))\n",
    "    test_eq(len(inp), len(targ))\n",
    "    return inp,targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = torch.randn(5,4),torch.randn(20)\n",
    "x1,x2 = flatten_check(x1,x2)\n",
    "test_eq(x1.shape, [20])\n",
    "test_eq(x2.shape, [20])\n",
    "x1,x2 = torch.randn(5,4),torch.randn(21)\n",
    "test_fail(lambda: flatten_check(x1,x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AccMetric(Metric):\n",
    "    \"Stores predictions and targets on CPU in accumulate to perform final calculations with `func`.\"\n",
    "    def __init__(self, func, dim_argmax=None, sigmoid=False, thresh=None, to_np=False, invert_arg=False, **kwargs): \n",
    "        self.func,self.dim_argmax,self.sigmoid,self.thresh = func,dim_argmax,sigmoid,thresh\n",
    "        self.to_np,self.invert_args,self.kwargs = to_np,invert_arg,kwargs\n",
    "    def reset(self): self.targs,self.preds = [],[]\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        pred = learn.pred.argmax(dim=self.dim_argmax) if self.dim_argmax else learn.pred\n",
    "        if self.sigmoid: pred = torch.sigmoid(pred)\n",
    "        if self.thresh:  pred = (pred >= self.thresh)\n",
    "        pred,targ = flatten_check(pred, learn.yb)\n",
    "        self.preds.append(pred)\n",
    "        self.targs.append(targ)\n",
    "    \n",
    "    @property\n",
    "    def value(self): \n",
    "        preds,targs = torch.cat(self.preds),torch.cat(self.targs)\n",
    "        if self.to_np: preds,targs = preds.numpy(),targs.numpy()\n",
    "        return self.func(targs, preds, **self.kwargs) if self.invert_args else self.func(preds, targs, **self.kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`func` is only applied to the accumulated predictions/targets when the `value` attribute is asked for (so at the end of a validation/trianing phase, in use with `Learner` and its `Recorder`).The signature of `func` should be `inp,targ` (where `inp` are the predictions of the model and `targ` the corresponding labels).\n",
    "\n",
    "For classification problems with single label, predictions need to be transformed with a sofmax then an argmax before being compared to the targets. Since a softmax doesn't change the order of the numbers, we can just apply the argmax. Pass along `dim_argmax` to have this done by `AccMetric` (usually -1 will work pretty well).\n",
    "\n",
    "For classification problems with multiple labels, or if your targets are onehot-encoded, predictions may need to pass through a sigmoid (if it wasn't included in your model) then be compared to a given threshold (to decide between 0 and 1), this is done by `AccMetric` if you pass `sigmoid=True` and/or a value for `thresh`.\n",
    "\n",
    "If you want to use a metric function sklearn.metrics, you will need to convert predictions and labels to numpy arrays with `to_np=True`. Also, scikit-learn metrics adopt the convention `y_true`, `y_preds` which is the opposite from us, so you will need to pass `invert_arg=True` to make `AccMetric` do the inversion for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing: a fake learner and a metric that isn't an average\n",
    "class TstLearner():\n",
    "    def __init__(self): self.pred,self.yb = None,None\n",
    "\n",
    "def _l2_mean(x,y): return torch.sqrt((x-y).float().pow(2).mean())\n",
    "\n",
    "#Go through a fake cycle with various batch sizes and computes the value of met\n",
    "def compute_val(met, x1, x2):\n",
    "    met.reset()\n",
    "    vals = [0,6,15,20]\n",
    "    learn = TstLearner()\n",
    "    for i in range(3): \n",
    "        learn.pred,learn.yb = x1[vals[i]:vals[i+1]],x2[vals[i]:vals[i+1]]\n",
    "        met.accumulate(learn)\n",
    "    return met.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "tst = AccMetric(_l2_mean)\n",
    "test_close(compute_val(tst, x1, x2), _l2_mean(x1, x2))\n",
    "test_eq(torch.cat(tst.preds), x1.view(-1))\n",
    "test_eq(torch.cat(tst.targs), x2.view(-1))\n",
    "\n",
    "#test argmax\n",
    "x1,x2 = torch.randn(20,5),torch.randint(0, 5, (20,))\n",
    "tst = AccMetric(_l2_mean, dim_argmax=-1)\n",
    "test_close(compute_val(tst, x1, x2), _l2_mean(x1.argmax(dim=-1), x2))\n",
    "\n",
    "#test thresh\n",
    "x1,x2 = torch.randn(20,5),torch.randint(0, 2, (20,5)).byte()\n",
    "tst = AccMetric(_l2_mean, thresh=0.5)\n",
    "test_close(compute_val(tst, x1, x2), _l2_mean((x1 >= 0.5), x2))\n",
    "\n",
    "#test sigmoid\n",
    "x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "tst = AccMetric(_l2_mean, sigmoid=True)\n",
    "test_close(compute_val(tst, x1, x2), _l2_mean(torch.sigmoid(x1), x2))\n",
    "\n",
    "#test to_np\n",
    "x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "tst = AccMetric(lambda x,y: isinstance(x, np.ndarray) and isinstance(y, np.ndarray), to_np=True)\n",
    "assert compute_val(tst, x1, x2)\n",
    "\n",
    "#test invert_arg\n",
    "x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "tst = AccMetric(lambda x,y: torch.sqrt(x.pow(2).mean()))\n",
    "test_close(compute_val(tst, x1, x2), torch.sqrt(x1.pow(2).mean()))\n",
    "tst = AccMetric(lambda x,y: torch.sqrt(x.pow(2).mean()), invert_arg=True)\n",
    "test_close(compute_val(tst, x1, x2), torch.sqrt(x2.pow(2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def skm_to_fastai(func, is_class=True, thresh=None, axis=-1, sigmoid=None, **kwargs):\n",
    "    \"Convert `func` from sklearn.metrics to a fastai metric\"\n",
    "    dim_argmax = axis if is_class and thresh is None else None\n",
    "    sigmoid = sigmoid if sigmoid is not None else (is_class and thresh is not None)\n",
    "    return AccMetric(func, dim_argmax=dim_argmax, sigmoid=sigmoid, thresh=thresh, \n",
    "                     to_np=True, invert_arg=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the quickest way to use a sckit-learn metric in a fastai training loop. `is_class` indicates if you are in a classification problem or not. In this case:\n",
    "- leaving `thresh` to `None` indicates it's a single-label classification problem and predictions will pass through an argmax over `axis` before being compared to the targets\n",
    "- setting a value for `thresh` indicates it's a multi-label classification problem and predictions will pass through a sigmoid (can be deactivated with `sigmoid=False`) and be compared to `thresh` before being compared to the targets\n",
    "\n",
    "If `is_class=False`, it indicates you are in a regression problem, and predictions are compared to the targets without being modified. In all cases, `kwargs` are extra keyword arguments passed to `func`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_single = skm_to_fastai(skm.precision_score)\n",
    "x1,x2 = torch.randn(20,2),torch.randint(0, 2, (20,))\n",
    "test_close(compute_val(tst_single, x1, x2), skm.precision_score(x2, x1.argmax(dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_multi = skm_to_fastai(skm.precision_score, thresh=0.2)\n",
    "x1,x2 = torch.randn(20),torch.randint(0, 2, (20,))\n",
    "test_close(compute_val(tst_multi, x1, x2), skm.precision_score(x2, torch.sigmoid(x1) >= 0.2))\n",
    "\n",
    "tst_multi = skm_to_fastai(skm.precision_score, thresh=0.2, sigmoid=False)\n",
    "x1,x2 = torch.randn(20),torch.randint(0, 2, (20,))\n",
    "test_close(compute_val(tst_multi, x1, x2), skm.precision_score(x2, x1 >= 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_reg = skm_to_fastai(skm.r2_score, is_class=False)\n",
    "x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "test_close(compute_val(tst_reg, x1, x2), skm.r2_score(x2.view(-1), x1.view(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_func(met, **kwargs):\n",
    "    return partial(met, **kwargs)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision(axis=-1, labels=None, pos_label=1, average='binary', sample_weight=None):\n",
    "    return skm_to_fastai(skm.precision_score, axis=axis,\n",
    "                        labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = metric_func(Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_single = partial(precision, \n",
    "x1,x2 = torch.randn(20,2),torch.randint(0, 2, (20,))\n",
    "test_close(compute_val(tst_single, x1, x2), skm.precision_score(x2, x1.argmax(dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skm.precision_score() 'labels=None', 'pos_label=1', \"average='binary'\", 'sample_weight=None'],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: All functions defined in this section are intended for single-label classification and targets that aren't one-hot encoded. For multi-label problems or one-hot encoded targets, use the `_multi` version of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def accuracy(inp, targ):\n",
    "    \"Compute accuracy with `targ` when `pred` is bs * n_classes\"\n",
    "    pred,targ = flatten_check(inp.argmax(dim=-1), targ)\n",
    "    return (pred == targ).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing\n",
    "def change_targ(targ, n, c):\n",
    "    idx = torch.randperm(len(targ))[:n]\n",
    "    res = targ.clone()\n",
    "    for i in idx: res[i] = (res[i]+random.randint(1,c-1))%c\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4,5)\n",
    "y = x.argmax(dim=1)\n",
    "test_eq(accuracy(x,y), 1)\n",
    "y1 = change_targ(y, 2, 5)\n",
    "test_eq(accuracy(x,y1), 0.5)\n",
    "test_eq(accuracy(x.unsqueeze(1).expand(4,2,5), torch.stack([y,y1], dim=1)), 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def error_rate(inp, targ):\n",
    "    \"1 - `accuracy`\"\n",
    "    return 1 - accuracy(inp, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4,5)\n",
    "y = x.argmax(dim=1)\n",
    "test_eq(error_rate(x,y), 0)\n",
    "y1 = change_targ(y, 2, 5)\n",
    "test_eq(error_rate(x,y1), 0.5)\n",
    "test_eq(error_rate(x.unsqueeze(1).expand(4,2,5), torch.stack([y,y1], dim=1)), 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def top_k_accuracy(inp, targ, k=5):\n",
    "    \"Computes the Top-k accuracy (`targ` is in the top `k` predictions of `inp`)\"\n",
    "    inp = inp.topk(k=k, dim=-1)[1]\n",
    "    targ = targ.unsqueeze(dim=-1).expand_as(inp)\n",
    "    return (inp == targ).sum(dim=-1).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(6,5)\n",
    "y = torch.arange(0,6)\n",
    "test_eq(top_k_accuracy(x[:5],y[:5]), 1)\n",
    "test_eq(top_k_accuracy(x, y), 5/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ap_score(axis=-1, average='macro', pos_label=1, sample_weight=None):\n",
    "    \"Average Precision for single-label classification problems\"\n",
    "    return skm_to_fastai(skm.average_precision_score, axis=axis, \n",
    "                         average=average, pos_label=pos_label, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def balanced_accuracy(axis=-1, sample_weight=None, adjusted=False):\n",
    "    \"Balanced Accuracy for single-label binary classification problems\"\n",
    "    return skm_to_fastai(skm.balanced_accuracy_score, axis=axis, \n",
    "                         sample_weight=sample_weight, adjusted=adjusted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def brier_score(axis=-1, sample_weight=None, pos_label=None):\n",
    "    \"Brier score for single-label classification problems\"\n",
    "    return skm_to_fastai(skm.balanced_accuracy_score, axis=axis, \n",
    "                         sample_weight=sample_weight, pos_label=pos_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html#sklearn.metrics.brier_score_loss) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cohen_kappa(axis=-1, labels=None, weights=None, sample_weight=None):\n",
    "    \"Cohen kappa for single-label classification problems\"\n",
    "    return skm_to_fastai(skm.balanced_accuracy_score, axis=axis, \n",
    "                         sample_weight=sample_weight, pos_label=pos_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(y1, y2, labels=None, weights=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Precision(axis=-1, labels=None, pos_label=1, average='binary', sample_weight=None):\n",
    "    return skm_to_fastai(skm.precision_score, axis=axis,\n",
    "                        labels=labels, pos_label=pos_label, average=average, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_1h_targ(targ, n):\n",
    "    idx = torch.randperm(targ.numel())[:n]\n",
    "    res = targ.clone().view(-1)\n",
    "    for i in idx: res[i] = 1-res[i]\n",
    "    return res.view(targ.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def accuracy_thresh(inp, targ, thresh=0.5, sigmoid=True):\n",
    "    \"Compute accuracy when `inp` and `targ` are the same size.\"\n",
    "    inp,targ = flatten_check(inp,targ)\n",
    "    if sigmoid: inp = inp.sigmoid()\n",
    "    return ((inp>thresh)==targ.byte()).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4,5)\n",
    "y = torch.sigmoid(x) >= 0.5\n",
    "test_eq(accuracy_thresh(x,y), 1)\n",
    "test_eq(accuracy_thresh(x,1-y), 0)\n",
    "y1 = change_1h_targ(y, 5)\n",
    "test_eq(accuracy_thresh(x,y1), 0.75)\n",
    "\n",
    "#Different thresh\n",
    "y = torch.sigmoid(x) >= 0.2\n",
    "test_eq(accuracy_thresh(x,y, thresh=0.2), 1)\n",
    "test_eq(accuracy_thresh(x,1-y, thresh=0.2), 0)\n",
    "y1 = change_1h_targ(y, 5)\n",
    "test_eq(accuracy_thresh(x,y1, thresh=0.2), 0.75)\n",
    "\n",
    "#No sigmoid\n",
    "y = x >= 0.5\n",
    "test_eq(accuracy_thresh(x,y, sigmoid=False), 1)\n",
    "test_eq(accuracy_thresh(x,1-y, sigmoid=False), 0)\n",
    "y1 = change_1h_targ(y, 5)\n",
    "test_eq(accuracy_thresh(x,y1, sigmoid=False), 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@metric()\n",
    "def rmse(inp, targ):\n",
    "    \"Root mean squared error betzeen `inp` and `targ`\"\n",
    "    return torch.sqrt(F.mse_loss(inp, targ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "tst = rmse\n",
    "tst.reset()\n",
    "vals = [0,6,15,20]\n",
    "learn = TstLearner()\n",
    "for i in range(3): \n",
    "    learn.pred,learn.yb = x1[vals[i]:vals[i+1]],x2[vals[i]:vals[i+1]]\n",
    "    tst.accumulate(learn)\n",
    "test_eq(tst.value, torch.sqrt(F.mse_loss(x1,x2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@metric()\n",
    "def exp_rmspe(inp, targ):\n",
    "    \"Root mean square percentage error of the exponential of `inp` and `targ`\"\n",
    "    inp,targ = torch.exp(inp),torch.exp(targ)\n",
    "    return torch.sqrt(((targ - inp)/targ).pow(2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "tst = exp_rmspe\n",
    "tst.reset()\n",
    "vals = [0,6,15,20]\n",
    "learn = TstLearner()\n",
    "for i in range(3): \n",
    "    learn.pred,learn.yb = x1[vals[i]:vals[i+1]],x2[vals[i]:vals[i+1]]\n",
    "    tst.accumulate(learn)\n",
    "test_eq(tst.value, torch.sqrt((((torch.exp(x2) - torch.exp(x1))/torch.exp(x2))**2).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(inp,targ):\n",
    "    \"Mean absolute error between `inp` and `targ`.\"\n",
    "    inp,targ = flatten_check(inp,targ)\n",
    "    return torch.abs(inp - targ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = torch.randn(4,5),torch.randn(4,5)\n",
    "test_eq(mae(x1,x2), torch.abs(x1-x2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(inp,targ):\n",
    "    \"Mean squared error between `inp` and `targ`.\"\n",
    "    return F.mse_loss(*flatten_check(inp,targ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = torch.randn(4,5),torch.randn(4,5)\n",
    "test_close(mse(x1,x2), (x1-x2).pow(2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msle(inp, targ):\n",
    "    \"Mean squared logarithmic error between `inp` and `targ`.\"\n",
    "    inp,targ = flatten_check(inp,targ)\n",
    "    return F.mse_loss(torch.log(1 + inp), torch.log(1 + targ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = torch.randn(4,5),torch.randn(4,5)\n",
    "x1,x2 = torch.relu(x1),torch.relu(x2)\n",
    "test_close(msle(x1,x2), (torch.log(x1+1)-torch.log(x2+1)).pow(2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@metric(to_np=True)\n",
    "def explained_variance(inp, targ):\n",
    "    \"Explained variance between `inp` and `targ`\"\n",
    "    return skmets.explained_variance_score(targ,inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "tst = explained_variance\n",
    "tst.reset()\n",
    "vals = [0,6,15,20]\n",
    "learn = TstLearner()\n",
    "for i in range(3): \n",
    "    learn.pred,learn.yb = x1[vals[i]:vals[i+1]],x2[vals[i]:vals[i+1]]\n",
    "    tst.accumulate(learn)\n",
    "test_close(tst.value, skmets.explained_variance_score(x2.view(-1).numpy(),x1.view(-1).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@metric(to_np=True)\n",
    "def r2_score(inp, targ):\n",
    "    \"R2 score (coefficient of determination)\"\n",
    "    return skmets.r2_score(targ,inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = torch.randn(20,5),torch.randn(20,5)\n",
    "tst = r2_score\n",
    "tst.reset()\n",
    "vals = [0,6,15,20]\n",
    "learn = TstLearner()\n",
    "for i in range(3): \n",
    "    learn.pred,learn.yb = x1[vals[i]:vals[i+1]],x2[vals[i]:vals[i+1]]\n",
    "    tst.accumulate(learn)\n",
    "test_close(tst.value, skmets.r2_score(x2.view(-1),x1.view(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreground_acc(inp, targ, bkg_idx=0):\n",
    "    \"Computes non-background accuracy for multiclass segmentation\"\n",
    "    targ = targ.squeeze(1)\n",
    "    mask = targ != bkg_idx\n",
    "    return (inp.argmax(dim=1)[mask]==targ[mask]).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4,5,3,3)\n",
    "y = x.argmax(dim=1)[:,None]\n",
    "test_eq(foreground_acc(x,y), 1)\n",
    "y[0] = 0 #the 0s are ignored so we get the same value\n",
    "test_eq(foreground_acc(x,y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbeta(y_pred:Tensor, y_true:Tensor, thresh:float=0.2, beta:float=2, eps:float=1e-9, sigmoid:bool=True)->Rank0Tensor:\n",
    "    \"Computes the f_beta between `preds` and `targets`\"\n",
    "    beta2 = beta ** 2\n",
    "    if sigmoid: y_pred = y_pred.sigmoid()\n",
    "    y_pred = (y_pred>thresh).float()\n",
    "    y_true = y_true.float()\n",
    "    TP = (y_pred*y_true).sum(dim=1)\n",
    "    prec = TP/(y_pred.sum(dim=1)+eps)\n",
    "    rec = TP/(y_true.sum(dim=1)+eps)\n",
    "    res = (prec*rec)/(prec*beta2+rec+eps)*(1+beta2)\n",
    "    return res.mean()\n",
    "\n",
    "\n",
    "def dice(input:Tensor, targs:Tensor, iou:bool=False, eps:float=1e-8)->Rank0Tensor:\n",
    "    \"Dice coefficient metric for binary target. If iou=True, returns iou metric, classic for segmentation problems.\"\n",
    "    n = targs.shape[0]\n",
    "    input = input.argmax(dim=1).view(n,-1)\n",
    "    targs = targs.view(n,-1)\n",
    "    intersect = (input * targs).sum().float()\n",
    "    union = (input+targs).sum().float()\n",
    "    if not iou: return (2. * intersect / union if union > 0 else union.new([1.]).squeeze())\n",
    "    else: return (intersect / (union-intersect+eps) if union > 0 else union.new([1.]).squeeze())\n",
    "\n",
    "class ConfusionMatrix(Callback):\n",
    "    \"Computes the confusion matrix.\"\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        self.n_classes = 0\n",
    "\n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.cm = None\n",
    "\n",
    "    def on_batch_end(self, last_output:Tensor, last_target:Tensor, **kwargs):\n",
    "        preds = last_output.argmax(-1).view(-1).cpu()\n",
    "        targs = last_target.cpu()\n",
    "        if self.n_classes == 0:\n",
    "            self.n_classes = last_output.shape[-1]\n",
    "            self.x = torch.arange(0, self.n_classes)\n",
    "        cm = ((preds==self.x[:, None]) & (targs==self.x[:, None, None])).sum(dim=2, dtype=torch.float32)\n",
    "        if self.cm is None: self.cm =  cm\n",
    "        else:               self.cm += cm\n",
    "\n",
    "    def on_epoch_end(self, **kwargs):\n",
    "        self.metric = self.cm\n",
    "\n",
    "@dataclass\n",
    "class CMScores(ConfusionMatrix):\n",
    "    \"Base class for metrics which rely on the calculation of the precision and/or recall score.\"\n",
    "    average:Optional[str]=\"binary\"      # `binary`, `micro`, `macro`, `weigthed` or None\n",
    "    pos_label:int=1                     # 0 or 1\n",
    "    eps:float=1e-9\n",
    "\n",
    "    def _recall(self):\n",
    "        rec = torch.diag(self.cm) / self.cm.sum(dim=1)\n",
    "        if self.average is None: return rec\n",
    "        else:\n",
    "            if self.average == \"micro\": weights = self._weights(avg=\"weighted\")\n",
    "            else: weights = self._weights(avg=self.average)\n",
    "            return (rec * weights).sum()\n",
    "\n",
    "    def _precision(self):\n",
    "        prec = torch.diag(self.cm) / self.cm.sum(dim=0)\n",
    "        if self.average is None: return prec\n",
    "        else:\n",
    "            weights = self._weights(avg=self.average)\n",
    "            return (prec * weights).sum()\n",
    "\n",
    "    def _weights(self, avg:str):\n",
    "        if self.n_classes != 2 and avg == \"binary\":\n",
    "            avg = self.average = \"macro\"\n",
    "            warn(\"average=`binary` was selected for a non binary case. Value for average has now been set to `macro` instead.\")\n",
    "        if avg == \"binary\":\n",
    "            if self.pos_label not in (0, 1):\n",
    "                self.pos_label = 1\n",
    "                warn(\"Invalid value for pos_label. It has now been set to 1.\")\n",
    "            if self.pos_label == 1: return Tensor([0,1])\n",
    "            else: return Tensor([1,0])\n",
    "        elif avg == \"micro\": return self.cm.sum(dim=0) / self.cm.sum()\n",
    "        elif avg == \"macro\": return torch.ones((self.n_classes,)) / self.n_classes\n",
    "        elif avg == \"weighted\": return self.cm.sum(dim=1) / self.cm.sum()\n",
    "\n",
    "\n",
    "class Recall(CMScores):\n",
    "    \"Compute the Recall.\"\n",
    "    def on_epoch_end(self, last_metrics, **kwargs): \n",
    "        return add_metrics(last_metrics, self._recall())\n",
    "\n",
    "class Precision(CMScores):\n",
    "    \"Compute the Precision.\"\n",
    "    def on_epoch_end(self, last_metrics, **kwargs): \n",
    "        return add_metrics(last_metrics, self._precision())\n",
    "\n",
    "@dataclass\n",
    "class FBeta(CMScores):\n",
    "    \"Compute the F`beta` score.\"\n",
    "    beta:float=2\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        self.n_classes = 0\n",
    "        self.beta2 = self.beta ** 2\n",
    "        self.avg = self.average\n",
    "        if self.average != \"micro\": self.average = None\n",
    "\n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        prec = self._precision()\n",
    "        rec = self._recall()\n",
    "        metric = (1 + self.beta2) * prec * rec / (prec * self.beta2 + rec + self.eps)\n",
    "        metric[metric != metric] = 0  # removing potential \"nan\"s\n",
    "        if self.avg: metric = (self._weights(avg=self.avg) * metric).sum()\n",
    "        return add_metrics(last_metrics, metric)\n",
    "\n",
    "    def on_train_end(self, **kwargs): self.average = self.avg\n",
    "\n",
    "@dataclass\n",
    "class KappaScore(ConfusionMatrix):\n",
    "    \"Compute the rate of agreement (Cohens Kappa).\"\n",
    "    weights:Optional[str]=None      # None, `linear`, or `quadratic`\n",
    "\n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        sum0 = self.cm.sum(dim=0)\n",
    "        sum1 = self.cm.sum(dim=1)\n",
    "        expected = torch.einsum('i,j->ij', (sum0, sum1)) / sum0.sum()\n",
    "        if self.weights is None:\n",
    "            w = torch.ones((self.n_classes, self.n_classes))\n",
    "            w[self.x, self.x] = 0\n",
    "        elif self.weights == \"linear\" or self.weights == \"quadratic\":\n",
    "            w = torch.zeros((self.n_classes, self.n_classes))\n",
    "            w += torch.arange(self.n_classes, dtype=torch.float)\n",
    "            w = torch.abs(w - torch.t(w)) if self.weights == \"linear\" else (w - torch.t(w)) ** 2\n",
    "        else: raise ValueError('Unknown weights. Expected None, \"linear\", or \"quadratic\".')\n",
    "        k = torch.sum(w * self.cm) / torch.sum(w * expected)\n",
    "        return add_metrics(last_metrics, 1-k)\n",
    "\n",
    "@dataclass\n",
    "class MatthewsCorreff(ConfusionMatrix):\n",
    "    \"Compute the Matthews correlation coefficient.\"\n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        t_sum = self.cm.sum(dim=1)\n",
    "        p_sum = self.cm.sum(dim=0)\n",
    "        n_correct = torch.trace(self.cm)\n",
    "        n_samples = p_sum.sum()\n",
    "        cov_ytyp = n_correct * n_samples - torch.dot(t_sum, p_sum)\n",
    "        cov_ypyp = n_samples ** 2 - torch.dot(p_sum, p_sum)\n",
    "        cov_ytyt = n_samples ** 2 - torch.dot(t_sum, t_sum)\n",
    "        return add_metrics(last_metrics, cov_ytyp / torch.sqrt(cov_ytyt * cov_ypyp))\n",
    "\n",
    "class Perplexity(Callback):\n",
    "    \"Perplexity metric for language models.\"\n",
    "    def on_epoch_begin(self, **kwargs): self.loss,self.len = 0.,0\n",
    "\n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        self.loss += last_target.size(1) * CrossEntropyFlat()(last_output, last_target)\n",
    "        self.len += last_target.size(1)\n",
    "\n",
    "    def on_epoch_end(self, last_metrics, **kwargs): \n",
    "        return add_metrics(last_metrics, torch.exp(self.loss / self.len))\n",
    "\n",
    "def auc_roc_score(input:Tensor, targ:Tensor):\n",
    "    \"Using trapezoid method to calculate the area under roc curve\"\n",
    "    fpr, tpr = roc_curve(input, targ)\n",
    "    d = fpr[1:] - fpr[:-1]\n",
    "    sl1, sl2 = [slice(None)], [slice(None)]\n",
    "    sl1[-1], sl2[-1] = slice(1, None), slice(None, -1)\n",
    "    return (d * (tpr[tuple(sl1)] + tpr[tuple(sl2)]) / 2.).sum(-1)\n",
    "\n",
    "def roc_curve(input:Tensor, targ:Tensor):\n",
    "    \"Returns the false positive and true positive rates\"\n",
    "    targ = (targ == 1)\n",
    "    desc_score_indices = torch.flip(input.argsort(-1), [-1])\n",
    "    input = input[desc_score_indices]\n",
    "    targ = targ[desc_score_indices]\n",
    "    d = input[1:] - input[:-1]\n",
    "    distinct_value_indices = torch.nonzero(d).transpose(0,1)[0]\n",
    "    threshold_idxs = torch.cat((distinct_value_indices, LongTensor([len(targ) - 1]).to(targ.device)))\n",
    "    tps = torch.cumsum(targ * 1, dim=-1)[threshold_idxs]\n",
    "    fps = (1 + threshold_idxs - tps)\n",
    "    if tps[0] != 0 or fps[0] != 0:\n",
    "        fps = torch.cat((LongTensor([0]), fps))\n",
    "        tps = torch.cat((LongTensor([0]), tps))\n",
    "    fpr, tpr = fps.float() / fps[-1], tps.float() / tps[-1]\n",
    "    return fpr, tpr\n",
    "\n",
    "@dataclass\n",
    "class AUROC(Callback):\n",
    "    \"Calculate the auc score based on the roc curve. Restricted to the binary classification task.\"\n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.targs, self.preds = LongTensor([]), Tensor([])\n",
    "        \n",
    "    def on_batch_end(self, last_output:Tensor, last_target:Tensor, **kwargs):\n",
    "        last_output = F.softmax(last_output, dim=1)[:,-1]\n",
    "        self.preds = torch.cat((self.preds, last_output.cpu()))\n",
    "        self.targs = torch.cat((self.targs, last_target.cpu().long()))\n",
    "    \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        return add_metrics(last_metrics, auc_roc_score(self.preds, self.targs))\n",
    "\n",
    "class MultiLabelFbeta(LearnerCallback):\n",
    "    \"Computes the fbeta score for multilabel classification\"\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "    _order = -20 \n",
    "    def __init__(self, learn, beta=2, eps=1e-15, thresh=0.3, sigmoid=True, average=\"micro\"):\n",
    "        super().__init__(learn)\n",
    "        self.eps, self.thresh, self.sigmoid, self.average, self.beta2 = \\\n",
    "            eps, thresh, sigmoid, average, beta**2\n",
    "\n",
    "    def on_train_begin(self, **kwargs):\n",
    "        self.c = self.learn.data.c\n",
    "        if self.average != \"none\": self.learn.recorder.add_metric_names([f'{self.average}_fbeta'])\n",
    "        else: self.learn.recorder.add_metric_names([f\"fbeta_{c}\" for c in self.learn.data.classes])\n",
    "\n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        dvc = self.learn.data.device\n",
    "        self.tp = torch.zeros(self.c).to(dvc)\n",
    "        self.total_pred = torch.zeros(self.c).to(dvc)\n",
    "        self.total_targ = torch.zeros(self.c).to(dvc)\n",
    "    \n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        pred, targ = (last_output.sigmoid() if self.sigmoid else last_output) > self.thresh, last_target.byte()\n",
    "        m = pred*targ\n",
    "        self.tp += m.sum(0).float()\n",
    "        self.total_pred += pred.sum(0).float()\n",
    "        self.total_targ += targ.sum(0).float()\n",
    "    \n",
    "    def fbeta_score(self, precision, recall):\n",
    "        return (1 + self.beta2)*(precision*recall)/((self.beta2*precision + recall) + self.eps)\n",
    "\n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        self.total_pred += self.eps\n",
    "        self.total_targ += self.eps\n",
    "        if self.average == \"micro\":\n",
    "            precision, recall = self.tp.sum() / self.total_pred.sum(), self.tp.sum() / self.total_targ.sum()\n",
    "            res = self.fbeta_score(precision, recall)\n",
    "        elif self.average == \"macro\":\n",
    "            res = self.fbeta_score((self.tp / self.total_pred), (self.tp / self.total_targ)).mean()\n",
    "        elif self.average == \"weighted\":\n",
    "            scores = self.fbeta_score((self.tp / self.total_pred), (self.tp / self.total_targ))\n",
    "            res = (scores*self.total_targ).sum() / self.total_targ.sum()\n",
    "        elif self.average == \"none\":\n",
    "            res = listify(self.fbeta_score((self.tp / self.total_pred), (self.tp / self.total_targ)))\n",
    "        else:\n",
    "            raise Exception(\"Choose one of the average types: [micro, macro, weighted, none]\")\n",
    "        \n",
    "        return add_metrics(last_metrics, res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
