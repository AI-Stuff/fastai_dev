{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.torch_basics import *\n",
    "from local.test import *\n",
    "from local.core import *\n",
    "from local.layers import *\n",
    "from local.data.all import *\n",
    "from local.notebook.showdoc import show_doc\n",
    "from local.optimizer import *\n",
    "from local.learner import *\n",
    "from local.metrics import *\n",
    "from local.text.core import *\n",
    "from local.text.data import *\n",
    "from local.text.models.core import *\n",
    "from local.text.models.awdlstm import *\n",
    "from local.callback.rnn import *\n",
    "from local.callback.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp text.learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text learner\n",
    "\n",
    "> Convenience functions to easily create a `Learner` for text applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def match_embeds(old_wgts, old_vocab, new_vocab):\n",
    "    \"Convert the embedding in `wgts` to go with a new vocabulary.\"\n",
    "    bias, wgts = old_wgts.get('1.decoder.bias', None), old_wgts['0.encoder.weight']\n",
    "    wgts_m = wgts.mean(0)\n",
    "    new_wgts = wgts.new_zeros((len(new_vocab),wgts.size(1)))\n",
    "    if bias is not None: \n",
    "        bias_m = bias.mean(0)\n",
    "        new_bias = bias.new_zeros((len(new_vocab),))\n",
    "    old_o2i = old_vocab.o2i if hasattr(old_vocab, 'o2i') else {w:i for i,w in enumerate(old_vocab)} \n",
    "    for i,w in enumerate(new_vocab):\n",
    "        idx = old_o2i.get(w, -1)\n",
    "        new_wgts[i] = wgts[idx] if idx>=0 else wgts_m\n",
    "        if bias is not None: new_bias[i] = bias[idx] if idx>=0 else bias_m\n",
    "    old_wgts['0.encoder.weight'] = new_wgts\n",
    "    if '0.encoder_dp.emb.weight' in old_wgts: old_wgts['0.encoder_dp.emb.weight'] = new_wgts.clone()\n",
    "    old_wgts['1.decoder.weight'] = new_wgts.clone()\n",
    "    if bias is not None: old_wgts['1.decoder.bias'] = new_bias\n",
    "    return old_wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = {'0.encoder.weight': torch.randn(5,3)}\n",
    "new_wgts = match_embeds(wgts.copy(), ['a', 'b', 'c'], ['a', 'c', 'd', 'b'])\n",
    "old,new = wgts['0.encoder.weight'],new_wgts['0.encoder.weight']\n",
    "test_eq(new[0], old[0])\n",
    "test_eq(new[1], old[2])\n",
    "test_eq(new[2], old.mean(0))\n",
    "test_eq(new[3], old[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With bias\n",
    "wgts = {'0.encoder.weight': torch.randn(5,3), '1.decoder.bias': torch.randn(5)}\n",
    "new_wgts = match_embeds(wgts.copy(), ['a', 'b', 'c'], ['a', 'c', 'd', 'b'])\n",
    "old_w,new_w = wgts['0.encoder.weight'],new_wgts['0.encoder.weight']\n",
    "old_b,new_b = wgts['1.decoder.bias'],  new_wgts['1.decoder.bias']\n",
    "test_eq(new_w[0], old_w[0])\n",
    "test_eq(new_w[1], old_w[2])\n",
    "test_eq(new_w[2], old_w.mean(0))\n",
    "test_eq(new_w[3], old_w[1])\n",
    "test_eq(new_b[0], old_b[0])\n",
    "test_eq(new_b[1], old_b[2])\n",
    "test_eq(new_b[2], old_b.mean(0))\n",
    "test_eq(new_b[3], old_b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(Learner.__init__)\n",
    "class RNNLearner(Learner):\n",
    "    \"Basic class for a `Learner` in NLP.\"\n",
    "    def __init__(self, model, dbunch, loss_func, alpha=2., beta=1., **kwargs):\n",
    "        super().__init__(model, dbunch, loss_func, **kwargs)\n",
    "        self.add_cb(RNNTrainer(alpha=alpha, beta=beta))\n",
    "        \n",
    "    def save_encoder(self, file):\n",
    "        \"Save the encoder to `self.path/self.model_dir/file`\"\n",
    "        encoder = get_model(self.model)[0]\n",
    "        if hasattr(encoder, 'module'): encoder = encoder.module\n",
    "        torch.save(encoder.state_dict(), join_path_file(file,self.path/self.model_dir, ext='.pth'))\n",
    "\n",
    "    def load_encoder(self, file, device=None):\n",
    "        \"Load the encoder `name` from the model directory.\"\n",
    "        encoder = get_model(self.model)[0]\n",
    "        if device is None: device = self.dbunch.device\n",
    "        if hasattr(encoder, 'module'): encoder = encoder.module\n",
    "        encoder.load_state_dict(torch.load(join_path_file(file,self.path/self.model_dir, ext='.pth'), map_location=device))\n",
    "        self.freeze()\n",
    "        return self\n",
    "\n",
    "    #TODO: When access is easier, grab new_vocab from self.dbunch\n",
    "    def load_pretrained(self, wgts_fname, vocab_fname, new_vocab, strict=True):\n",
    "        \"Load a pretrained model and adapt it to the data vocabulary.\"\n",
    "        old_vocab = pickle.load(open(vocab_fname, 'rb'))\n",
    "        wgts = torch.load(wgts_fname, map_location = lambda storage,loc: storage)\n",
    "        if 'model' in wgts: wgts = wgts['model'] #Just in case the pretrained model was saved with an optimizer\n",
    "        wgts = match_embeds(wgts, old_vocab, new_vocab)\n",
    "        self.model.load_state_dict(wgts, strict=strict)\n",
    "        self.freeze()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.text.models.core import _model_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: When access is easier, grab vocab from dbunch\n",
    "@delegates(Learner.__init__)\n",
    "def language_model_learner(dbunch, arch, vocab, config=None, drop_mult=1., pretrained=True, pretrained_fnames=None, **kwargs):\n",
    "    \"Create a `Learner` with a language model from `data` and `arch`.\"\n",
    "    model = get_language_model(arch, len(vocab), config=config, drop_mult=drop_mult)\n",
    "    meta = _model_meta[arch]\n",
    "    learn = RNNLearner(dbunch, model, loss_func=CrossEntropyLossFlat(), splitter=meta['split_lm'], **kwargs)\n",
    "    #TODO: add backard\n",
    "    #url = 'url_bwd' if data.backwards else 'url'\n",
    "    if pretrained or pretrained_fnames:\n",
    "        if pretrained_fnames is not None:\n",
    "            fnames = [learn.path/learn.model_dir/f'{fn}.{ext}' for fn,ext in zip(pretrained_fnames, ['pth', 'pkl'])]\n",
    "        else:\n",
    "            if 'url' not in meta:\n",
    "                warn(\"There are no pretrained weights for that architecture yet!\")\n",
    "                return learn\n",
    "            model_path = untar_data(meta['url'] , c_key=ConfigKey.Model)\n",
    "            fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        learn = learn.load_pretrained(*fnames, vocab)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "df = pd.read_csv(path/'texts.csv')\n",
    "df_tok,count = tokenize_df(df, 'text')\n",
    "texts,lengths = df_tok['text'],df_tok['text_lengths'].values.astype(np.int)\n",
    "\n",
    "splits = RandomSplitter()(texts)\n",
    "vocab = make_vocab(count)\n",
    "dsrc = DataSource(texts, [Numericalize(vocab)], filts=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch = LMDataLoader.dbunchify(dsrc, bs=64, seq_len=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def awd_lstm_lm_split1(model):\n",
    "    \"Split a RNN `model` in groups for differential learning rates.\"\n",
    "    groups = [nn.Sequential(rnn, dp) for rnn, dp in zip(model[0].rnns, model[0].hidden_dps)]\n",
    "    groups = L(groups + [nn.Sequential(model[0].encoder, model[0].encoder_dp, model[1])])\n",
    "    print(groups)\n",
    "    return groups.mapped(trainable_params)\n",
    "\n",
    "_model_meta[AWD_LSTM]['split_lm'] = awd_lstm_lm_split1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#4) [Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(400, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      "),Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 1152, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      "),Sequential(\n",
      "  (0): WeightDropout(\n",
      "    (module): LSTM(1152, 400, batch_first=True)\n",
      "  )\n",
      "  (1): RNNDropout()\n",
      "),Sequential(\n",
      "  (0): Embedding(7080, 400, padding_idx=1)\n",
      "  (1): EmbeddingDropout(\n",
      "    (emb): Embedding(7080, 400, padding_idx=1)\n",
      "  )\n",
      "  (2): LinearDecoder(\n",
      "    (decoder): Linear(in_features=400, out_features=7080, bias=True)\n",
      "    (output_dp): RNNDropout()\n",
      "  )\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "learn = language_model_learner(dbunch, AWD_LSTM, vocab, metrics=[accuracy, Perplexity()], path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.780375</td>\n",
       "      <td>4.360741</td>\n",
       "      <td>0.254467</td>\n",
       "      <td>78.315155</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.752711</td>\n",
       "      <td>4.332884</td>\n",
       "      <td>0.256474</td>\n",
       "      <td>76.163612</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.741457</td>\n",
       "      <td>4.321849</td>\n",
       "      <td>0.257541</td>\n",
       "      <td>75.327774</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.719990</td>\n",
       "      <td>4.320048</td>\n",
       "      <td>0.257505</td>\n",
       "      <td>75.192230</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, 1e-2, moms=(0.8,0.7,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: When access is easier, grab vocab from dbunch\n",
    "@delegates(Learner.__init__)\n",
    "def text_classifier_learner(dbunch, arch, vocab, bptt=72, config=None, pretrained=True, drop_mult=1., \n",
    "                            lin_ftrs=None, ps=None, **kwargs):\n",
    "    \"Create a `Learner` with a text classifier from `data` and `arch`.\"\n",
    "    model = get_text_classifier(arch, len(vocab), get_c(dbunch), bptt=bptt, config=config, \n",
    "                                drop_mult=drop_mult, lin_ftrs=lin_ftrs, ps=ps)\n",
    "    meta = _model_meta[arch]\n",
    "    learn = RNNLearner(dbunch, model, loss_func=CrossEntropyLossFlat(), splitter=meta['split_clas'], **kwargs)\n",
    "    if pretrained:\n",
    "        if 'url' not in meta:\n",
    "            warn(\"There are no pretrained weights for that architecture yet!\")\n",
    "            return learn\n",
    "        model_path = untar_data(meta['url'], c_key=ConfigKey.Model)\n",
    "        fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        learn = learn.load_pretrained(*fnames, vocab, strict=False)\n",
    "        learn.freeze()\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = RandomSplitter()(range_of(df_tok))\n",
    "dsrc = DataSource(df_tok, filts=splits, tfms=[\n",
    "    [attrgetter(\"text\"), Numericalize(vocab)],\n",
    "    [attrgetter(\"label\"), Categorize()]])\n",
    "trn_dl = SortedDL(dsrc.train, create_batch=pad_collate, after_batch=[Cuda], shuffle=True, drop_last=True)\n",
    "val_dl = SortedDL(dsrc.valid, create_batch=pad_collate, after_batch=[Cuda])\n",
    "dbunch = DataBunch(trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dbunch, AWD_LSTM, vocab, metrics=[accuracy], path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load_encoder('enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.855475</td>\n",
       "      <td>0.618845</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.784172</td>\n",
       "      <td>0.564299</td>\n",
       "      <td>0.685000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.757014</td>\n",
       "      <td>0.515467</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.761705</td>\n",
       "      <td>0.521394</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(4, moms=(0.8,0.7,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
