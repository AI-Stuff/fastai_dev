{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from local.imports import *\n",
    "from local.core import compose\n",
    "from local.test import *\n",
    "from local.notebook.export import *\n",
    "from local.notebook.showdoc import *\n",
    "import nbformat\n",
    "from notebook import notebookapp\n",
    "from nbconvert.preprocessors import ExecutePreprocessor, Preprocessor\n",
    "from nbconvert import HTMLExporter,MarkdownExporter\n",
    "from nbformat.sign import NotebookNotary\n",
    "from traitlets.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp notebook.export2html\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting notebooks to html\n",
    "\n",
    "> The functions that transform the dev notebooks in the documentation of the library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def remove_widget_state(cell):\n",
    "    \"Remove widgets in the output of `cells`\"\n",
    "    if cell['cell_type'] == 'code' and 'outputs' in cell:\n",
    "        cell['outputs'] = [l for l in cell['outputs']\n",
    "                           if not ('data' in l and 'application/vnd.jupyter.widget-view+json' in l.data)]\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Matches any cell that has a `show_doc` or an `#export` in it\n",
    "_re_cell_to_hide = r's*show_doc\\(|^\\s*#\\s*export\\s+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def hide_cells(cell):\n",
    "    \"Hide `cell` that need to be hidden\"\n",
    "    if check_re(cell, _re_cell_to_hide):  cell['metadata'] = {'hide_input': True}\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in ['show_doc(read_nb)', '# export\\nfrom local.core import *']:\n",
    "    cell = {'cell_type': 'code', 'source': 'show_doc(read_nb)'}\n",
    "    cell1 = hide_cells(cell.copy())\n",
    "    assert 'metadata' in cell1\n",
    "    assert 'hide_input' in cell1['metadata']\n",
    "    assert cell1['metadata']['hide_input']\n",
    "\n",
    "cell = {'cell_type': 'code', 'source': '# exports\\nfrom local.core import *'}\n",
    "test_eq(hide_cells(cell.copy()), cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Matches any line containing an #exports\n",
    "_re_exports = re.compile(r'^#\\s*exports[^\\n]*\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def clean_exports(cell):\n",
    "    \"Remove exports flag from `cell`\"\n",
    "    cell['source'] = _re_exports.sub('', cell['source'])\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = {'cell_type': 'code', 'source': '# exports\\nfrom local.core import *'}\n",
    "test_eq(clean_exports(cell.copy()), {'cell_type': 'code', 'source': 'from local.core import *'})\n",
    "cell = {'cell_type': 'code', 'source': '# exports core\\nfrom local.core import *'}\n",
    "test_eq(clean_exports(cell.copy()), {'cell_type': 'code', 'source': 'from local.core import *'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def treat_backticks(cell):\n",
    "    \"Add links to backticks words in `cell`\"\n",
    "    if cell['cell_type'] == 'markdown': cell['source'] = add_doc_links(cell['source'])\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = {'cell_type': 'markdown', 'source': 'This is a `Tensor`'}\n",
    "test_eq(treat_backticks(cell), {'cell_type': 'markdown',\n",
    "    'source': 'This is a [`Tensor`](https://pytorch.org/docs/stable/tensors.html#torch-tensor)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_nb_link = re.compile(r\"\"\"\n",
    "# Catches any link to a local notebook and keeps the title in group 1, the link without .ipynb in group 2\n",
    "\\[          # Opening [\n",
    "([^\\]]*)    # Catching group for any character except ]\n",
    "\\]\\(        # Closing ], opening (\n",
    "([^http]    # Catching group that must not begin by html (local notebook)\n",
    "[^\\)]*)     # and containing anything but )\n",
    ".ipynb\\)    # .ipynb and closing )\n",
    "\"\"\", re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def convert_links(cell):\n",
    "    \"Convert the .ipynb links to .html\"\n",
    "    if cell['cell_type'] == 'markdown':\n",
    "        cell['source'] = _re_nb_link.sub(r'[\\1](\\2.html)', cell['source'])\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = {'cell_type': 'markdown', 'source': \"This is a link to a [notebook](01_core.ipynb).\"}\n",
    "test_eq(convert_links(cell), {'cell_type': 'markdown', \n",
    "                               'source': \"This is a link to a [notebook](01_core.html).\"})\n",
    "cell = {'cell_type': 'markdown', 'source': \"This is a link to a [page](01_core.html).\"}\n",
    "test_eq(convert_links(cell.copy()), cell)\n",
    "cell = {'cell_type': 'markdown', 'source': \"This is a link to an [external nb](http://01_core.ipynb).\"}\n",
    "test_eq(convert_links(cell.copy()), cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_block_notes = re.compile(r\"\"\"\n",
    "# Catches any pattern > Title: content with title in group 1 and content in group 2\n",
    "^>\\s*      # > followed by any number of whitespace\n",
    "([^:]*)   # Catching group for any character but :\n",
    ":\\s*      # : then any number of whitespace\n",
    "([^\\n]*)  # Catching group for anything but a new line character\n",
    "(?:\\n|$)  # Non-catching group for either a new line or the end of the text\n",
    "\"\"\", re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_jekyll_notes(cell):\n",
    "    \"Convert block quotes to jekyll notes in `cell`\"\n",
    "    t2style = {'Note': 'info', 'Warning': 'danger', 'Important': 'warning'}\n",
    "    def _inner(m):\n",
    "        title,text = m.groups()\n",
    "        style = t2style.get(title, None)\n",
    "        if style is None: return f\"> {m.groups()[0]}: {m.groups()[1]}\"\n",
    "        res = f'<div markdown=\"span\" class=\"alert alert-{style}\" role=\"alert\">'\n",
    "        return res + f'<i class=\"fa fa-{style}-circle\"></i> <b>{title}: </b>{text}</div>'\n",
    "    if cell['cell_type'] == 'markdown':\n",
    "        cell['source'] = _re_block_notes.sub(_inner, cell['source'])\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supported styles are `Warning`, `Note` and `Important`:\n",
    "\n",
    "> Warning: There will be no second warning!\n",
    "\n",
    "> Important: Pay attention! This is important.\n",
    "\n",
    "> Note: Take note of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w,s in zip(['Warning', 'Note', 'Important', 'Bla'], ['danger', 'info', 'warning', 'info']):\n",
    "    cell = {'cell_type': 'markdown', 'source': f\"> {w}: This is my final {w.lower()}!\"}\n",
    "    res = f'<div markdown=\"span\" class=\"alert alert-{s}\" role=\"alert\">'\n",
    "    res += f'<i class=\"fa fa-{s}-circle\"></i> <b>{w}: </b>This is my final {w.lower()}!</div>'\n",
    "    if w != 'Bla': test_eq(add_jekyll_notes(cell), {'cell_type': 'markdown', 'source': res})\n",
    "    else: test_eq(add_jekyll_notes(cell), cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_image = re.compile(r\"\"\"\n",
    "# Catches any image file used, either with `![alt](image_file)` or `<img src=\"image_file\">`\n",
    "^!\\[        #   Beginning of line (since re.MULTILINE is passed) followed by ![\n",
    "[^\\]]*      #   Anything but ]\n",
    "\\]\\(        #   Closing ] and opening (\n",
    "([^\\)]*)    #   Catching block with any character but )\n",
    "\\)          #   Closing )\n",
    "|           # OR\n",
    "<img\\ src=\"  #   <img src=\"\n",
    "([^\"]*)     #   Catching block with any character except \"\n",
    "\"           #   Closing\n",
    "\"\"\", re.MULTILINE | re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def copy_images(cell, fname, dest):\n",
    "    if cell['cell_type'] == 'markdown' and _re_image.search(cell['source']):\n",
    "        grps = _re_image.search(cell['source']).groups()\n",
    "        src = grps[0] or grps[1]\n",
    "        os.makedirs((Path(dest)/src).parent, exist_ok=True)\n",
    "        shutil.copy(Path(fname).parent/src, Path(dest)/src)\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_img = Path('docs')/'images'/'pixelshuffle.png' \n",
    "dest_bak = Path('docs')/'images'/'pixelshuffle.bak'\n",
    "if dest_img.exists(): shutil.move(dest_img, dest_bak)\n",
    "for text in ['Text\\n![Alt](images/pixelshuffle.png)', \n",
    "             'Text\\n<img src=\"images/pixelshuffle.png\" alt=\"Pixelshuffle\" style=\"width: 100%; height: auto;\"/>']:\n",
    "    cell = {'cell_type': 'markdown', 'source': text}\n",
    "    cell1 = copy_images(cell, Path('10_layers.ipynb'), Path('docs'))\n",
    "    #Function doesn't touch cell\n",
    "    test_eq(cell, cell1)\n",
    "    #Image has been copied\n",
    "    assert dest_img.exists()\n",
    "    os.remove(dest_img)\n",
    "if dest_bak.exists(): shutil.move(dest_bak, dest_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#Matches any cell with #hide or #default_exp or #default_cls_lvl\n",
    "_re_cell_to_remove = re.compile(r'^\\s*#\\s*(hide|default_exp|default_cls_lvl)\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def remove_hidden(cells):\n",
    "    \"Remove in `cells` the ones with a flag `#hide` or `#default_exp`\"\n",
    "    return [c for c in cells if _re_cell_to_remove.search(c['source']) is None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = [{'cell_type': 'code', 'source': source} for source in [\n",
    "    '# export\\nfrom local.core import *', \n",
    "    '# hide\\nfrom local.core import *',\n",
    "    '#exports\\nsuper code',\n",
    "    '#default_exp notebook.export',\n",
    "    'show_doc(read_nb)',\n",
    "    '#default_cls_lvl 3']] + [{'cell_type': 'markdown', 'source': source} for source in [\n",
    "    'nice', '#hide\\n\\nto hide']]\n",
    "         \n",
    "cells1 = remove_hidden(cells)\n",
    "test_eq(len(cells1), 4)\n",
    "test_eq(cells1[0], cells[0])\n",
    "test_eq(cells1[1], cells[2])\n",
    "test_eq(cells1[2], cells[4])\n",
    "test_eq(cells1[3], cells[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_default_cls_lvl = re.compile(r\"\"\"\n",
    "^               # Beginning of line (since re.MULTILINE is passed)\n",
    "\\s*\\#\\s*        # Any number of whitespace, #, any number of whitespace\n",
    "default_cls_lvl # default_cls_lvl\n",
    "\\s*             # Any number of whitespace\n",
    "(\\d*)           # Catching group for any number of digits\n",
    "\\s*$            # Any number of whitespace and end of line (since re.MULTILINE is passed)\n",
    "\"\"\", re.IGNORECASE | re.MULTILINE | re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def find_default_level(cells):\n",
    "    \"Find in `cells` the default export module.\"\n",
    "    for cell in cells:\n",
    "        tst = check_re(cell, _re_default_cls_lvl)\n",
    "        if tst: return int(tst.groups()[0])\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_nb = read_nb('91_notebook_export.ipynb')\n",
    "test_eq(find_default_level(tst_nb['cells']), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#Find a cell with #export(s)\n",
    "_re_export = re.compile(r'^\\s*#\\s*exports?\\s*', re.IGNORECASE | re.MULTILINE)\n",
    "_re_show_doc = re.compile(r\"\"\"\n",
    "# First one catches any cell with a #export or #exports, second one catches any show_doc and get the first argument in group 1\n",
    "show_doc     # show_doc\n",
    "\\s*\\(\\s*     # Any number of whitespace, opening (, any number of whitespace\n",
    "([^,\\)\\s]*)  # Catching group for any character but a comma, a closing ) or a whitespace\n",
    "[,\\)\\s]      # A comma, a closing ) or a whitespace\n",
    "\"\"\", re.MULTILINE | re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _show_doc_cell(name, cls_lvl=None):\n",
    "    return {'cell_type': 'code',\n",
    "            'execution_count': None,\n",
    "            'metadata': {},\n",
    "            'outputs': [],\n",
    "            'source': f\"show_doc({name}{'' if cls_lvl is None else f', default_cls_level={cls_lvl}'})\"}\n",
    "\n",
    "def add_show_docs(cells, cls_lvl=None):\n",
    "    \"Add `show_doc` for each exported function or class\"\n",
    "    documented = [_re_show_doc.search(cell['source']).groups()[0] for cell in cells\n",
    "                  if cell['cell_type']=='code' and _re_show_doc.search(cell['source']) is not None]\n",
    "    res = []\n",
    "    for cell in cells:\n",
    "        res.append(cell)\n",
    "        if check_re(cell, _re_export):\n",
    "            names = export_names(cell['source'], func_only=True)\n",
    "            print(cell['source'])\n",
    "            print(names)\n",
    "            for n in names:\n",
    "                if n not in documented: res.append(_show_doc_cell(n, cls_lvl=cls_lvl))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function local.notebook.export.export_names(code, func_only=False)>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,cell in enumerate(tst_nb['cells']):\n",
    "    if cell['source'].startswith('#export\\ndef read_nb'): break\n",
    "tst_cells = [c.copy() for c in tst_nb['cells'][i-1:i+1]]\n",
    "added_cells = add_show_docs(tst_cells, cls_lvl=3)\n",
    "test_eq(len(added_cells), 3)\n",
    "test_eq(added_cells[0], tst_nb['cells'][i-1])\n",
    "test_eq(added_cells[1], tst_nb['cells'][i])\n",
    "test_eq(added_cells[2], _show_doc_cell('read_nb', cls_lvl=3))\n",
    "test_eq(added_cells[2]['source'], 'show_doc(read_nb, default_cls_level=3)')\n",
    "\n",
    "#Check show_doc isn't added if it was already there.\n",
    "tst_cells1 = [{'cell_type':'code', 'source': '#export\\ndef my_func(x):\\n    return x'},\n",
    "              {'cell_type':'code', 'source': 'show_doc(my_func)'}]\n",
    "test_eq(add_show_docs(tst_cells1), tst_cells1)\n",
    "tst_cells1 = [{'cell_type':'code', 'source': '#export\\ndef my_func(x):\\n    return x'},\n",
    "              {'cell_type':'markdown', 'source': 'Some text'},\n",
    "              {'cell_type':'code', 'source': 'show_doc(my_func, title_level=3)'}]\n",
    "test_eq(add_show_docs(tst_cells1), tst_cells1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "_re_fake_header = re.compile(r\"\"\"\n",
    "# Matches any fake header (one that ends with -)\n",
    "\\#+    # One or more #\n",
    "\\s+    # One or more of whitespace\n",
    ".*     # Any char\n",
    "-\\s*   # A dash followed by any number of white space\n",
    "$      # End of text\n",
    "\"\"\", re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def remove_fake_headers(cells):\n",
    "    \"Remove in `cells` the fake header\"\n",
    "    return [c for c in cells if c['cell_type']=='code' or _re_fake_header.search(c['source']) is None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = [{'cell_type': 'markdown',\n",
    "          'metadata': {},\n",
    "          'source': '### Fake-'}] + tst_nb['cells'][:10]\n",
    "cells1 = remove_fake_headers(cells)\n",
    "test_eq(len(cells1), len(cells)-1)\n",
    "test_eq(cells1[0], cells[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def remove_empty(cells):\n",
    "    \"Remove in `cells` the empty cells\"\n",
    "    return [c for c in cells if len(c['source']) >0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing metada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "_re_title_summary = re.compile(r\"\"\"\n",
    "# Catches the title and summary of the notebook, presented as # Title > summary, with title in group 1 and summary in group 2\n",
    "^\\s*       # Beginning of text followe by any number of whitespace\n",
    "\\#\\s+      # # followed by one or more of whitespace\n",
    "([^\\n]*)   # Catching group for any character except a new line\n",
    "\\n+        # One or more new lines\n",
    ">\\s*       # > followed by any number of whitespace\n",
    "([^\\n]*)   # Catching group for any character except a new line\n",
    "\"\"\", re.VERBOSE)\n",
    "\n",
    "_re_properties = re.compile(r\"\"\"\n",
    "^-\\s+      # Beginnig of a line followed by - and at least one space\n",
    "(.*?)      # Any pattern (shortest possible)\n",
    "\\s*:\\s*    # Any number of whitespace, :, any number of whitespace\n",
    "(.*?)$     # Any pattern (shortest possible) then end of line\n",
    "\"\"\", re.MULTILINE | re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_metadata(cells):\n",
    "    \"Find the cell with title and summary in `cells`.\"\n",
    "    for i,cell in enumerate(cells):\n",
    "        if cell['cell_type'] == 'markdown':\n",
    "            match = _re_title_summary.match(cell['source'])\n",
    "            if match:\n",
    "                cells.pop(i)\n",
    "                attrs = {k:v for k,v in _re_properties.findall(cell['source'])}\n",
    "                return {'keywords': 'fastai',\n",
    "                        'summary' : match.groups()[1],\n",
    "                        'title'   : match.groups()[0],\n",
    "                        **attrs}\n",
    "    return {'keywords': 'fastai',\n",
    "            'summary' : 'summary',\n",
    "            'title'   : 'Title'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_nb = read_nb('91_notebook_export.ipynb')\n",
    "test_eq(get_metadata(tst_nb['cells']), {\n",
    "    'keywords': 'fastai',\n",
    "    'summary': 'The functions that transform the dev notebooks in the fastai library',\n",
    "    'title': 'Converting notebooks to modules',\n",
    "    'author': '\"Sylvain Gugger\"'})\n",
    "#The cell with the metada is poped out, so if we do it a second time we get the default.\n",
    "test_eq(get_metadata(tst_nb['cells']), {'keywords': 'fastai',\n",
    "            'summary' : 'summary',\n",
    "            'title'   : 'Title'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing show_doc cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#Catches any cell with a show_doc or an import from local\n",
    "_re_cell_to_execute = re.compile(r\"^\\s*show_doc\\(([^\\)]*)\\)|^from local\\.\", re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ExecuteShowDocPreprocessor(ExecutePreprocessor):\n",
    "    \"An `ExecutePreprocessor` that only executes `show_doc` and `import` cells\"\n",
    "    def preprocess_cell(self, cell, resources, index):\n",
    "        if 'source' in cell and cell['cell_type'] == \"code\":\n",
    "            if _re_cell_to_execute.search(cell['source']):\n",
    "                return super().preprocess_cell(cell, resources, index)\n",
    "        return cell, resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _import_show_doc_cell(mod=None, name=None):\n",
    "    \"Add an import show_doc cell + deal with the _file_ hack if necessary.\"\n",
    "    source = f\"#export\\nfrom local.notebook.showdoc import show_doc\"\n",
    "    if mod:  source += f\"\\nfrom local.{mod} import *\"\n",
    "    if name: source += f\"\\nfrom pathlib import Path\\n_file_ = {name}\"\n",
    "    return {'cell_type': 'code',\n",
    "            'execution_count': None,\n",
    "            'metadata': {'hide_input': True},\n",
    "            'outputs': [],\n",
    "            'source': source}\n",
    "\n",
    "def execute_nb(nb, mod=None, metadata=None, show_doc_only=True, name=None):\n",
    "    \"Execute `nb` (or only the `show_doc` cells) with `metadata`\"\n",
    "    nb['cells'].insert(0, _import_show_doc_cell(mod, name))\n",
    "    ep_cls = ExecuteShowDocPreprocessor if show_doc_only else ExecutePreprocessor\n",
    "    ep = ep_cls(timeout=600, kernel_name='python3')\n",
    "    metadata = metadata or {}\n",
    "    pnb = nbformat.from_dict(nb)\n",
    "    ep.preprocess(pnb, metadata)\n",
    "    return pnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_nb = {k:v for k,v in tst_nb.items() if k != 'cells'}\n",
    "fake_nb['cells'] = [tst_nb['cells'][0].copy()] + added_cells\n",
    "fake_nb = execute_nb(fake_nb, mod='notebook.export')\n",
    "assert len(fake_nb['cells'][-1]['outputs']) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Tricking jupyter notebook to have a __file__ attribute. All _file_ will be replaced by __file__\n",
    "_file_ = Path('local').absolute()/'notebook'/'export.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _exporter(markdown=False):\n",
    "    cfg = Config()\n",
    "    exporter = (HTMLExporter,MarkdownExporter)[markdown](cfg)\n",
    "    exporter.exclude_input_prompt=True\n",
    "    exporter.exclude_output_prompt=True\n",
    "    exporter.template_file = ('jekyll.tpl','jekyll-md.tpl')[markdown]\n",
    "    exporter.template_path.append(str(Path(_file_).parent))\n",
    "    return exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "process_cells = [remove_fake_headers, remove_hidden, remove_empty]\n",
    "process_cell  = [hide_cells, remove_widget_state, add_jekyll_notes, convert_links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_file = re.compile(r\"\"\"\n",
    "^_file_   # _file_ at the beginning of a line (since re.MULTILINE is passed)\n",
    "\\s*=\\s*   # Any number of whitespace, =, any number of whitespace\n",
    "(\\S*)     # Catching group for any non-whitespace characters\n",
    "\\s*$      # Any number of whitespace then the end of line\n",
    "\"\"\", re.MULTILINE | re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _find_file(cells):\n",
    "    \"Find in `cells` if a _file_ is defined.\"\n",
    "    for cell in cells:\n",
    "        if cell['cell_type']=='code' and _re_file.search(cell['source']):\n",
    "            return _re_file.search(cell['source']).groups()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "tst_nb = read_nb('91_notebook_export.ipynb')\n",
    "test_eq(_find_file(tst_nb['cells']), \"Path('local').absolute()/'notebook'/'export.py'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def notebook_path():\n",
    "    \"Returns the absolute path of the Notebook or None if it cannot be determined\"\n",
    "    #NOTE: works only when the security is token-based or there is no password\n",
    "    kernel_id = Path(ipykernel.get_connection_file()).stem.split('-', 1)[1]\n",
    "    for srv in notebookapp.list_running_servers():\n",
    "        try:\n",
    "            sessions = json.load(urlopen(f\"{srv['url']}api/sessions{srv['token']}\"))\n",
    "            return next(Path(srv['notebook_dir'])/sess['notebook']['path']\n",
    "                        for sess in sessions if sess['kernel']['id']==kernel_id)\n",
    "        except: pass  # There may be stale entries in the runtime directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_eq(notebook_path().name, '93_notebook_export2html.ipynb')\n",
    "#test_eq(notebook_path().parent, Path().absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def convert_nb(fname, dest_path='docs'):\n",
    "    \"Convert a notebook `fname` to html file in `dest_path`.\"\n",
    "    fname = Path(fname).absolute()\n",
    "    nb = read_nb(fname)\n",
    "    cls_lvl = find_default_level(nb['cells'])\n",
    "    _name = _find_file(nb['cells'])\n",
    "    mod = find_default_export(nb['cells'])\n",
    "    nb['cells'] = compose(*process_cells,partial(add_show_docs, cls_lvl=cls_lvl))(nb['cells'])\n",
    "    nb['cells'] = [compose(partial(copy_images, fname=fname, dest=dest_path), *process_cell, treat_backticks)(c)\n",
    "                    for c in nb['cells']]\n",
    "    fname = Path(fname).absolute()\n",
    "    dest_name = '.'.join(fname.with_suffix('.html').name.split('_')[1:])\n",
    "    meta_jekyll = get_metadata(nb['cells'])\n",
    "    meta_jekyll['nb_path'] = f'{fname.parent.name}/{fname.name}'\n",
    "    nb = execute_nb(nb, mod=mod, name=_name)\n",
    "    nb['cells'] = [clean_exports(c) for c in nb['cells']]\n",
    "    #print(f'{dest_path}/{dest_name}')\n",
    "    with open(f'{dest_path}/{dest_name}','w') as f:\n",
    "        #res = _exporter().from_notebook_node(nb, resources=meta_jekyll)[0]\n",
    "        #print(res)\n",
    "        f.write(_exporter().from_notebook_node(nb, resources=meta_jekyll)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#export\n",
      "from local.torch_basics import *\n",
      "from local.test import *\n",
      "from local.data.all import *\n",
      "from local.vision.core import *\n",
      "from local.notebook.showdoc import show_doc\n",
      "[]\n",
      "# export\n",
      "from torch import stack, zeros_like as t0, ones_like as t1\n",
      "from torch.distributions.bernoulli import Bernoulli\n",
      "[]\n",
      "# export\n",
      "class RandTransform(Transform):\n",
      "    \"A transform that before_call its state at each `__call__`, only applied on the training set\"\n",
      "    split_idx,do,nm,supports = 0,True,None,[]\n",
      "    def __init__(self, p=1., nm=None, before_call=None, **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.p,self.before_call = p,ifnone(before_call,self.before_call)\n",
      "\n",
      "    def before_call(self, b, split_idx):\n",
      "        \"before_call the state for input `b`\"\n",
      "        self.do = random.random() < self.p\n",
      "\n",
      "    def __call__(self, b, split_idx=None, **kwargs):\n",
      "        self.before_call(b, split_idx=split_idx)\n",
      "        return super().__call__(b, split_idx=split_idx, **kwargs) if self.do else b\n",
      "['RandTransform']\n",
      "# export\n",
      "def _neg_axis(x, axis):\n",
      "    x[...,axis] = -x[...,axis]\n",
      "    return x\n",
      "[]\n",
      "# export\n",
      "@patch\n",
      "def flip_lr(x:Image.Image): return x.transpose(Image.FLIP_LEFT_RIGHT)\n",
      "@patch\n",
      "def flip_lr(x:TensorImage): return x.flip(-1)\n",
      "@patch\n",
      "def flip_lr(x:TensorPoint): return _neg_axis(x, 0)\n",
      "@patch\n",
      "def flip_lr(x:TensorBBox):\n",
      "    bb,lbl = x\n",
      "    bb = _neg_axis(bb.view(-1,2), 0)\n",
      "    return (bb.view(-1,4),lbl)\n",
      "\n",
      "TensorTypes = (TensorImage,TensorMask,TensorPoint, TensorBBox)\n",
      "['Image.Image.flip_lr', 'TensorImage.flip_lr', 'TensorPoint.flip_lr', 'TensorBBox.flip_lr']\n",
      "# export\n",
      "class FlipItem(RandTransform):\n",
      "    \"Randomly flip with probability `p`\"\n",
      "    def __init__(self, p=0.5): super().__init__(p=p)\n",
      "    def encodes(self, x:(Image.Image,*TensorTypes)): return x.flip_lr()\n",
      "['FlipItem']\n",
      "# export\n",
      "@patch\n",
      "def dihedral(x:PILImage, k): return x if k==0 else x.transpose(k-1)\n",
      "@patch\n",
      "def dihedral(x:TensorImage, k):\n",
      "        if k in [1, 3, 4, 7]: x = x.flip(-1)\n",
      "        if k in [2, 4, 5, 7]: x = x.flip(-2)\n",
      "        if k in [3, 5, 6, 7]: x = x.transpose(-1,-2)\n",
      "        return x\n",
      "@patch\n",
      "def dihedral(x:TensorPoint, k):\n",
      "        if k in [1, 3, 4, 7]: x = _neg_axis(x, 0)\n",
      "        if k in [2, 4, 5, 7]: x = _neg_axis(x, 1)\n",
      "        if k in [3, 5, 6, 7]: x = x.flip(1)\n",
      "        return x\n",
      "@patch\n",
      "def dihedral(x:TensorBBox, k):\n",
      "        pnts = TensorPoint.dihedral(x[0].view(-1,2), k).view(-1,2,2)\n",
      "        tl,br = pnts.min(dim=1)[0],pnts.max(dim=1)[0]\n",
      "        return [torch.cat([tl, br], dim=1), x[1]]\n",
      "['PILImage.dihedral', 'TensorImage.dihedral', 'TensorPoint.dihedral', 'TensorBBox.dihedral']\n",
      "# export\n",
      "class DihedralItem(RandTransform):\n",
      "    \"Randomly flip with probability `p`\"\n",
      "    def __init__(self, p=0.5): super().__init__(p=p)\n",
      "\n",
      "    def before_call(self, b, split_idx):\n",
      "        super().before_call(b, split_idx)\n",
      "        self.k = random.randint(0,7)\n",
      "    \n",
      "    def encodes(self, x:(Image.Image,*TensorTypes)): return x.dihedral(self.k)\n",
      "['DihedralItem']\n",
      "# export\n",
      "def clip_remove_empty(bbox, label):\n",
      "    \"Clip bounding boxes with image border and label background the empty ones.\"\n",
      "    bbox = torch.clamp(bbox, -1, 1)\n",
      "    empty = ((bbox[...,2] - bbox[...,0])*(bbox[...,3] - bbox[...,1]) < 0.)\n",
      "    if isinstance(label, torch.Tensor): label[empty] = 0\n",
      "    else: label = [0 if m else l for l,m in zip(label,empty)]\n",
      "    return (bbox, label)\n",
      "['clip_remove_empty']\n",
      "# export\n",
      "from torchvision.transforms.functional import pad as tvpad\n",
      "[]\n",
      "# export\n",
      "mk_class('PadMode', **{o:o.lower() for o in ['Zeros', 'Border', 'Reflection']},\n",
      "         doc=\"All possible padding mode as attributes to get tab-completion and typo-proofing\")\n",
      "[]\n",
      "#export\n",
      "_all_ = ['PadMode']\n",
      "[]\n",
      "# export\n",
      "_pad_modes = {'zeros': 'constant', 'border': 'edge', 'reflection': 'reflect'}\n",
      "\n",
      "@patch\n",
      "def _do_crop_pad(x:Image.Image, sz, tl, orig_sz,\n",
      "                 pad_mode=PadMode.Zeros, resize_mode=Image.BILINEAR, resize_to=None):\n",
      "    if any(tl.gt(0)):\n",
      "        # At least one dim is inside the image, so needs to be cropped\n",
      "        c = tl.max(0)\n",
      "        x = x.crop((*c, *c.add(sz).min(orig_sz)))\n",
      "    if any(tl.lt(0)):\n",
      "        # At least one dim is outside the image, so needs to be padded\n",
      "        p = (-tl).max(0)\n",
      "        f = (sz-orig_sz-p).max(0)\n",
      "        x = tvpad(x, (*p, *f), padding_mode=_pad_modes[pad_mode])\n",
      "    if resize_to is not None: x = x.resize(resize_to, resize_mode)\n",
      "    return x\n",
      "\n",
      "@patch\n",
      "def _do_crop_pad(x:TensorPoint, sz, tl, orig_sz, pad_mode=PadMode.Zeros, **kwargs):\n",
      "    #assert pad_mode==PadMode.Zeros,\"Only zero padding is supported for `TensorPoint` and `TensorBBox`\"\n",
      "    orig_sz,sz,tl = map(FloatTensor, (orig_sz,sz,tl))\n",
      "    return TensorPoint((x+1)*orig_sz/sz - tl*2/sz - 1)\n",
      "\n",
      "@patch\n",
      "def _do_crop_pad(x:TensorBBox, sz, tl, orig_sz, pad_mode=PadMode.Zeros, **kwargs):\n",
      "    bbox,label = x\n",
      "    bbox = TensorPoint._do_crop_pad(bbox.view(-1,2), sz, tl, orig_sz, pad_mode).view(-1,4)\n",
      "    return TensorBBox(clip_remove_empty(bbox, label))\n",
      "    \n",
      "@patch\n",
      "def crop_pad(x:(TensorBBox,TensorPoint,Image.Image),\n",
      "             sz, tl=None, orig_sz=None, pad_mode=PadMode.Zeros, resize_mode=Image.BILINEAR, resize_to=None):\n",
      "    if isinstance(sz,int): sz = (sz,sz)\n",
      "    orig_sz = Tuple(x.size if orig_sz is None else orig_sz)\n",
      "    sz,tl = Tuple(sz),Tuple(((x.size-sz)//2) if tl is None else tl)\n",
      "    return x._do_crop_pad(sz, tl, orig_sz=orig_sz, pad_mode=pad_mode, resize_mode=resize_mode, resize_to=resize_to)\n",
      "['']\n",
      "# export\n",
      "class CropPad(RandTransform):\n",
      "    \"Center crop or pad an image to `size`\"\n",
      "    mode,mode_mask,order,final_size,split_idx = Image.BILINEAR,Image.NEAREST,5,None,None\n",
      "    def __init__(self, size, pad_mode=PadMode.Zeros, **kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        if isinstance(size,int): size=(size,size)\n",
      "        self.size,self.pad_mode = Tuple(size[1],size[0]),pad_mode\n",
      "\n",
      "    def before_call(self, b, split_idx):\n",
      "        self.do = True\n",
      "        self.cp_size = self.size\n",
      "        self.orig_sz = Tuple((b[0] if isinstance(b, tuple) else b).size)\n",
      "        self.tl = (self.orig_sz-self.cp_size)//2\n",
      "\n",
      "    def encodes(self, x:(Image.Image,TensorBBox,TensorPoint)):\n",
      "        return x.crop_pad(self.cp_size, self.tl, orig_sz=self.orig_sz, pad_mode=self.pad_mode,\n",
      "            resize_mode=self.mode_mask if isinstance(x,PILMask) else self.mode, resize_to=self.final_size)\n",
      "['CropPad']\n",
      "# export\n",
      "class RandomCrop(CropPad):\n",
      "    \"Randomly crop an image to `size`\"\n",
      "    def before_call(self, b, split_idx):\n",
      "        super().before_call(b, split_idx)\n",
      "        w,h = self.orig_sz\n",
      "        if not split_idx: self.tl = (random.randint(0,w-self.cp_size[0]), random.randint(0,h-self.cp_size[1]))\n",
      "['RandomCrop']\n",
      "# export\n",
      "mk_class('ResizeMethod', **{o:o.lower() for o in ['Squish', 'Crop', 'Pad']},\n",
      "         doc=\"All possible resize method as attributes to get tab-completion and typo-proofing\")\n",
      "[]\n",
      "#export\n",
      "_all_ = ['ResizeMethod']\n",
      "[]\n",
      "# export\n",
      "class Resize(CropPad):\n",
      "    order=10\n",
      "    \"Resize image to `size` using `method`\"\n",
      "    def __init__(self, size, method=ResizeMethod.Squish, pad_mode=PadMode.Reflection,\n",
      "                 resamples=(Image.BILINEAR, Image.NEAREST), **kwargs):\n",
      "        super().__init__(size, pad_mode=pad_mode, **kwargs)\n",
      "        (self.mode,self.mode_mask),self.method = resamples,method\n",
      "\n",
      "    def before_call(self, b, split_idx):\n",
      "        super().before_call(b, split_idx)\n",
      "        self.final_size = self.size\n",
      "        if self.method==ResizeMethod.Squish:\n",
      "            self.tl,self.cp_size = (0,0),self.orig_sz\n",
      "            return\n",
      "        w,h = self.orig_sz\n",
      "        op = (operator.lt,operator.gt)[self.method==ResizeMethod.Pad]\n",
      "        m = w/self.final_size[0] if op(w/self.final_size[0],h/self.final_size[1]) else h/self.final_size[1]\n",
      "        self.cp_size = (int(m*self.final_size[0]),int(m*self.final_size[1]))\n",
      "        if self.method==ResizeMethod.Pad or split_idx: self.tl = ((w-self.cp_size[0])//2, (h-self.cp_size[1])//2)\n",
      "        else: self.tl = (random.randint(0,w-self.cp_size[0]), random.randint(0,h-self.cp_size[1]))\n",
      "['Resize']\n",
      "# export\n",
      "class RandomResizedCrop(CropPad):\n",
      "    \"Picks a random scaled crop of an image and resize it to `size`\"\n",
      "    def __init__(self, size, min_scale=0.08, ratio=(3/4, 4/3), resamples=(Image.BILINEAR, Image.NEAREST), **kwargs):\n",
      "        super().__init__(size, **kwargs)\n",
      "        self.min_scale,self.ratio = min_scale,ratio\n",
      "        self.mode,self.mode_mask = resamples\n",
      "\n",
      "    def before_call(self, b, split_idx):\n",
      "        super().before_call(b, split_idx)\n",
      "        self.final_size = self.size\n",
      "        w,h = self.orig_sz\n",
      "        for attempt in range(10):\n",
      "            if split_idx: break\n",
      "            area = random.uniform(self.min_scale,1.) * w * h\n",
      "            ratio = math.exp(random.uniform(math.log(self.ratio[0]), math.log(self.ratio[1])))\n",
      "            nw = int(round(math.sqrt(area * ratio)))\n",
      "            nh = int(round(math.sqrt(area / ratio)))\n",
      "            if nw <= w and nh <= h:\n",
      "                self.cp_size = (nw,nh)\n",
      "                self.tl = random.randint(0,w-nw), random.randint(0,h - nh)\n",
      "                return\n",
      "        if   w/h < self.ratio[0]: self.cp_size = (w, int(w/self.ratio[0]))\n",
      "        elif w/h > self.ratio[1]: self.cp_size = (int(h*self.ratio[1]), h)\n",
      "        else:                     self.cp_size = (w, h)\n",
      "        self.tl = ((w-self.cp_size[0])//2, (h-self.cp_size[1])//2)\n",
      "['RandomResizedCrop']\n",
      "#export\n",
      "def _init_mat(x):\n",
      "    mat = torch.eye(3, dtype=x.dtype, device=x.device)\n",
      "    return mat.unsqueeze(0).expand(x.size(0), 3, 3)\n",
      "[]\n",
      "#export\n",
      "@patch\n",
      "def affine_coord(x: TensorImage, mat=None, coord_tfm=None, sz=None, mode='bilinear', pad_mode=PadMode.Reflection):\n",
      "    if mat is None and coord_tfm is None: return x\n",
      "    size = tuple(x.shape[-2:]) if sz is None else (sz,sz) if isinstance(sz,int) else tuple(sz)\n",
      "    if mat is None: mat = _init_mat(x)[:,:2]\n",
      "    coords = F.affine_grid(mat, x.shape[:2] + size)\n",
      "    if coord_tfm is not None: coords = coord_tfm(coords)\n",
      "    return TensorImage(F.grid_sample(x, coords, mode=mode, padding_mode=pad_mode))\n",
      "\n",
      "@patch\n",
      "def affine_coord(x: TensorMask, mat=None, coord_tfm=None, sz=None, mode='nearest', pad_mode=PadMode.Reflection):\n",
      "    add_dim = (x.ndim==3)\n",
      "    if add_dim: x = x[:,None]\n",
      "    res = TensorImage.affine_coord(x.float(), mat, coord_tfm, sz, mode, pad_mode).long()\n",
      "    if add_dim: res = res[:,0]\n",
      "    return TensorMask(res)\n",
      "\n",
      "@patch\n",
      "def affine_coord(x: TensorPoint, mat=None, coord_tfm=None, sz=None, mode='nearest', pad_mode=PadMode.Zeros):\n",
      "    #assert pad_mode==PadMode.Zeros, \"Only zero padding is supported for `TensorPoint` and `TensorBBox`\"\n",
      "    if coord_tfm is not None: x = coord_tfm(x, invert=True)\n",
      "    if mat is not None: x = (x - mat[:,:,2].unsqueeze(1)) @ torch.inverse(mat[:,:,:2].transpose(1,2))\n",
      "    return TensorPoint(x)\n",
      "\n",
      "@patch\n",
      "def affine_coord(x: TensorBBox, mat=None, coord_tfm=None, sz=None, mode='nearest', pad_mode=PadMode.Zeros):\n",
      "    if mat is None and coord_tfm is None: return x\n",
      "    bbox,label = x\n",
      "    bs,n = bbox.shape[:2]\n",
      "    pnts = stack([bbox[...,:2], stack([bbox[...,0],bbox[...,3]],dim=2),\n",
      "                  stack([bbox[...,2],bbox[...,1]],dim=2), bbox[...,2:]], dim=2)\n",
      "    pnts = TensorPoint.affine_coord(pnts.view(bs, 4*n, 2), mat, coord_tfm, sz, mode, pad_mode)\n",
      "    pnts = pnts.view(bs, n, 4, 2)\n",
      "    tl,dr = pnts.min(dim=2)[0],pnts.max(dim=2)[0]\n",
      "    return TensorBBox(clip_remove_empty(torch.cat([tl, dr], dim=2), label))\n",
      "['TensorImage.affine_coord', 'TensorMask.affine_coord', 'TensorPoint.affine_coord', 'TensorBBox.affine_coord']\n",
      "# export\n",
      "class AffineCoordTfm(RandTransform):\n",
      "    \"Combine and apply affine and coord transforms\"\n",
      "    order = 30\n",
      "    def __init__(self, aff_fs=None, coord_fs=None, size=None, mode='bilinear', pad_mode=PadMode.Reflection, mode_mask='nearest'):\n",
      "        self.aff_fs,self.coord_fs = L(aff_fs),L(coord_fs)\n",
      "        store_attr(self, 'size,mode,pad_mode,mode_mask')\n",
      "        self.cp_size = None if size is None else (size,size) if isinstance(size, int) else tuple(size)\n",
      "\n",
      "    def before_call(self, b, split_idx):\n",
      "        if isinstance(b, tuple): b = b[0]\n",
      "        self.do,self.mat = True,self._get_affine_mat(b)[:,:2]\n",
      "        for t in self.coord_fs: t.before_call(b)\n",
      "\n",
      "    def compose(self, tfm):\n",
      "        \"Compose `self` with another `AffineCoordTfm` to only do the interpolation step once\"\n",
      "        self.aff_fs   += tfm.aff_fs\n",
      "        self.coord_fs += tfm.coord_fs\n",
      "\n",
      "    def _get_affine_mat(self, x):\n",
      "        aff_m = _init_mat(x)\n",
      "        ms = [f(x) for f in self.aff_fs]\n",
      "        ms = [m for m in ms if m is not None]\n",
      "        for m in ms: aff_m = aff_m @ m\n",
      "        return aff_m\n",
      "\n",
      "    def _encode(self, x, mode, reverse=False):\n",
      "        coord_func = None if len(self.coord_fs)==0 else partial(compose_tfms, tfms=self.coord_fs, reverse=reverse)\n",
      "        return x.affine_coord(self.mat, coord_func, sz=self.size, mode=mode, pad_mode=self.pad_mode)\n",
      "    \n",
      "    def encodes(self, x:TensorImage): return self._encode(x, self.mode)\n",
      "    def encodes(self, x:TensorMask):  return self._encode(x, self.mode_mask)\n",
      "    def encodes(self, x:(TensorPoint, TensorBBox)): return self._encode(x, self.mode, reverse=True)\n",
      "['AffineCoordTfm']\n",
      "# export\n",
      "def affine_mat(*ms):\n",
      "    \"Restructure length-6 vector `ms` into an affine matrix with 0,0,1 in the last line\"\n",
      "    return stack([stack([ms[0], ms[1], ms[2]], dim=1),\n",
      "                  stack([ms[3], ms[4], ms[5]], dim=1),\n",
      "                  stack([t0(ms[0]), t0(ms[0]), t1(ms[0])], dim=1)], dim=1)\n",
      "['affine_mat']\n",
      "# export\n",
      "def mask_tensor(x, p=0.5, neutral=0.):\n",
      "    \"Mask elements of `x` with `neutral` with probability `1-p`\"\n",
      "    if p==1.: return x\n",
      "    if neutral != 0: x.add_(-neutral)\n",
      "    mask = x.new_empty(*x.size()).bernoulli_(p)\n",
      "    x.mul_(mask)\n",
      "    return x.add_(neutral) if neutral != 0 else x\n",
      "['mask_tensor']\n",
      "# export\n",
      "def flip_mat(x, p=0.5):\n",
      "    \"Return a random flip matrix\"\n",
      "    mask = mask_tensor(-x.new_ones(x.size(0)), p=p, neutral=1.)\n",
      "    return affine_mat(mask,     t0(mask), t0(mask),\n",
      "                      t0(mask), t1(mask), t0(mask))\n",
      "['flip_mat']\n",
      "#export\n",
      "def _get_default(x, mode=None, pad_mode=None):\n",
      "    if mode is None: mode='bilinear' if isinstance(x, TensorMask) else 'bilinear'\n",
      "    if pad_mode is None: pad_mode=PadMode.Zeros if isinstance(x, (TensorPoint, TensorBBox)) else PadMode.Reflection\n",
      "    x0 = x[0] if isinstance(x, tuple) else x\n",
      "    return x0,mode,pad_mode\n",
      "[]\n",
      "# export\n",
      "@patch\n",
      "def flip_batch(x: TensorTypes, p=0.5, size=None, mode=None, pad_mode=None):\n",
      "    x0,mode,pad_mode = _get_default(x, mode, pad_mode)\n",
      "    return x.affine_coord(mat=flip_mat(x0, p=p)[:,:2], sz=size, mode=mode, pad_mode=pad_mode)\n",
      "['TensorTypes.flip_batch']\n",
      "# export\n",
      "def Flip(p=0.5, size=None, mode='bilinear', pad_mode=PadMode.Reflection):\n",
      "    \"Randomly flip a batch of images with a probability `p`\"\n",
      "    return AffineCoordTfm(aff_fs=partial(flip_mat, p=p), size=size, mode=mode, pad_mode=pad_mode)\n",
      "['Flip']\n",
      "# export\n",
      "def _draw_mask(x, def_draw, draw=None, p=0.5, neutral=0.):\n",
      "    if draw is None: draw=def_draw\n",
      "    if callable(draw): return draw(x)\n",
      "    elif is_listy(draw):\n",
      "        test_eq(len(draw), x.size(0))\n",
      "        res = tensor(draw, dtype=x.dtype, device=x.device)\n",
      "    else: res = x.new_zeros(x.size(0)) + draw\n",
      "    return mask_tensor(res, p=p, neutral=neutral)\n",
      "[]\n",
      "# export\n",
      "def dihedral_mat(x, p=0.5, draw=None):\n",
      "    \"Return a random dihedral matrix\"\n",
      "    def _def_draw(x): return torch.randint(0,8, (x.size(0),), device=x.device)\n",
      "    idx = _draw_mask(x, _def_draw, draw=draw, p=p).long()\n",
      "    xs = tensor([1,-1,1,-1,-1,1,1,-1], device=x.device).gather(0, idx)\n",
      "    ys = tensor([1,1,-1,1,-1,-1,1,-1], device=x.device).gather(0, idx)\n",
      "    m0 = tensor([1,1,1,0,1,0,0,0], device=x.device).gather(0, idx)\n",
      "    m1 = tensor([0,0,0,1,0,1,1,1], device=x.device).gather(0, idx)\n",
      "    return affine_mat(xs*m0,  xs*m1,  t0(xs),\n",
      "                      ys*m1,  ys*m0,  t0(xs)).float()\n",
      "    mask = mask_tensor(-x.new_ones(x.size(0)), p=p, neutral=1.)\n",
      "['dihedral_mat']\n",
      "# export\n",
      "@patch\n",
      "def dihedral_batch(x: TensorTypes, p=0.5, draw=None, size=None, mode=None, pad_mode=None):\n",
      "    x0,mode,pad_mode = _get_default(x, mode, pad_mode)\n",
      "    return x.affine_coord(mat=dihedral_mat(x0, p=p, draw=draw)[:,:2], sz=size, mode=mode, pad_mode=pad_mode)\n",
      "['TensorTypes.dihedral_batch']\n",
      "# export\n",
      "def Dihedral(p=0.5, draw=None, size=None, mode='bilinear', pad_mode=PadMode.Reflection):\n",
      "    \"Apply a random dihedral transformation to a batch of images with a probability `p`\"\n",
      "    return AffineCoordTfm(aff_fs=partial(dihedral_mat, p=p, draw=draw), size=size, mode=mode, pad_mode=pad_mode)\n",
      "['Dihedral']\n",
      "# export\n",
      "def rotate_mat(x, max_deg=10, p=0.5, draw=None):\n",
      "    \"Return a random rotation matrix with `max_deg` and `p`\"\n",
      "    def _def_draw(x): return x.new(x.size(0)).uniform_(-max_deg, max_deg)\n",
      "    thetas = _draw_mask(x, _def_draw, draw=draw, p=p) * math.pi/180\n",
      "    return affine_mat(thetas.cos(), thetas.sin(), t0(thetas),\n",
      "                     -thetas.sin(), thetas.cos(), t0(thetas))\n",
      "['rotate_mat']\n",
      "# export\n",
      "@delegates(rotate_mat)\n",
      "@patch\n",
      "def rotate(x: TensorTypes, size=None, mode=None, pad_mode=None, **kwargs):\n",
      "    x0,mode,pad_mode = _get_default(x, mode, pad_mode)\n",
      "    return x.affine_coord(mat=rotate_mat(x0, **kwargs)[:,:2], sz=size, mode=mode, pad_mode=pad_mode)\n",
      "['TensorTypes.rotate']\n",
      "# export\n",
      "def Rotate(max_deg=10, p=0.5, draw=None, size=None, mode='bilinear', pad_mode=PadMode.Reflection):\n",
      "    \"Apply a random rotation of at most `max_deg` with probability `p` to a batch of images\"\n",
      "    return AffineCoordTfm(partial(rotate_mat, max_deg=max_deg, p=p, draw=draw),\n",
      "                          size=size, mode=mode, pad_mode=pad_mode)\n",
      "['Rotate']\n",
      "# export\n",
      "def zoom_mat(x, max_zoom=1.1, p=0.5, draw=None, draw_x=None, draw_y=None):\n",
      "    \"Return a random zoom matrix with `max_zoom` and `p`\"\n",
      "    def _def_draw(x):     return x.new(x.size(0)).uniform_(1, max_zoom)\n",
      "    def _def_draw_ctr(x): return x.new(x.size(0)).uniform_(0,1)\n",
      "    s = 1/_draw_mask(x, _def_draw, draw=draw, p=p, neutral=1.)\n",
      "    col_pct = _draw_mask(x, _def_draw_ctr, draw=draw_x, p=1.)\n",
      "    row_pct = _draw_mask(x, _def_draw_ctr, draw=draw_y, p=1.)\n",
      "    col_c = (1-s) * (2*col_pct - 1)\n",
      "    row_c = (1-s) * (2*row_pct - 1)\n",
      "    return affine_mat(s,     t0(s), col_c,\n",
      "                      t0(s), s,     row_c)\n",
      "['zoom_mat']\n",
      "# export\n",
      "@delegates(zoom_mat)\n",
      "@patch\n",
      "def zoom(x: TensorTypes, size=None, mode='bilinear', pad_mode=PadMode.Reflection, **kwargs):\n",
      "    x0,mode,pad_mode = _get_default(x, mode, pad_mode)\n",
      "    return x.affine_coord(mat=zoom_mat(x0, **kwargs)[:,:2], sz=size, mode=mode, pad_mode=pad_mode)\n",
      "['TensorTypes.zoom']\n",
      "# export\n",
      "def Zoom(max_zoom=1.1, p=0.5, draw=None, draw_x=None, draw_y=None, size=None, mode='bilinear',\n",
      "         pad_mode=PadMode.Reflection):\n",
      "    \"Apply a random zoom of at most `max_zoom` with probability `p` to a batch of images\"\n",
      "    return AffineCoordTfm(partial(zoom_mat, max_zoom=max_zoom, p=p, draw=draw, draw_x=draw_x, draw_y=draw_y),\n",
      "                          size=size, mode=mode, pad_mode=pad_mode)\n",
      "['Zoom']\n",
      "# export\n",
      "def find_coeffs(p1, p2):\n",
      "    \"Find coefficients for warp tfm from `p1` to `p2`\"\n",
      "    m = []\n",
      "    p = p1[:,0,0]\n",
      "    #The equations we'll need to solve.\n",
      "    for i in range(p1.shape[1]):\n",
      "        m.append(stack([p2[:,i,0], p2[:,i,1], t1(p), t0(p), t0(p), t0(p), -p1[:,i,0]*p2[:,i,0], -p1[:,i,0]*p2[:,i,1]]))\n",
      "        m.append(stack([t0(p), t0(p), t0(p), p2[:,i,0], p2[:,i,1], t1(p), -p1[:,i,1]*p2[:,i,0], -p1[:,i,1]*p2[:,i,1]]))\n",
      "    #The 8 scalars we seek are solution of AX = B\n",
      "    A = stack(m).permute(2, 0, 1)\n",
      "    B = p1.view(p1.shape[0], 8, 1)\n",
      "    return torch.solve(B,A)[0]\n",
      "['find_coeffs']\n",
      "# export\n",
      "def apply_perspective(coords, coeffs):\n",
      "    \"Apply perspective tranfom on `coords` with `coeffs`\"\n",
      "    sz = coords.shape\n",
      "    coords = coords.view(sz[0], -1, 2)\n",
      "    coeffs = torch.cat([coeffs, t1(coeffs[:,:1])], dim=1).view(coeffs.shape[0], 3,3)\n",
      "    coords = coords @ coeffs[...,:2].transpose(1,2) + coeffs[...,2].unsqueeze(1)\n",
      "    coords.div_(coords[...,2].unsqueeze(-1))\n",
      "    return coords[...,:2].view(*sz)\n",
      "['apply_perspective']\n",
      "# export\n",
      "class _WarpCoord():\n",
      "    def __init__(self, magnitude=0.2, p=0.5, draw_x=None, draw_y=None):\n",
      "        self.coeffs,self.magnitude,self.p,self.draw_x,self.draw_y = None,magnitude,p,draw_x,draw_y\n",
      "\n",
      "    def _def_draw(self, x): return x.new(x.size(0)).uniform_(-self.magnitude, self.magnitude)\n",
      "    def before_call(self, x):\n",
      "        x_t = _draw_mask(x, self._def_draw, self.draw_x, p=self.p)\n",
      "        y_t = _draw_mask(x, self._def_draw, self.draw_y, p=self.p)\n",
      "        orig_pts = torch.tensor([[-1,-1], [-1,1], [1,-1], [1,1]], dtype=x.dtype, device=x.device)\n",
      "        self.orig_pts = orig_pts.unsqueeze(0).expand(x.size(0),4,2)\n",
      "        targ_pts = stack([stack([-1-y_t, -1-x_t]), stack([-1+y_t, 1+x_t]),\n",
      "                          stack([ 1+y_t, -1+x_t]), stack([ 1-y_t, 1-x_t])])\n",
      "        self.targ_pts = targ_pts.permute(2,0,1)\n",
      "\n",
      "    def __call__(self, x, invert=False):\n",
      "        coeffs = find_coeffs(self.targ_pts, self.orig_pts) if invert else find_coeffs(self.orig_pts, self.targ_pts)\n",
      "        return apply_perspective(x, coeffs)\n",
      "[]\n",
      "# export\n",
      "@delegates(_WarpCoord.__init__)\n",
      "@patch\n",
      "def warp(x: TensorTypes, size=None, mode='bilinear', pad_mode=PadMode.Reflection, **kwargs):\n",
      "    x0,mode,pad_mode = _get_default(x, mode, pad_mode)\n",
      "    coord_tfm = _WarpCoord(**kwargs)\n",
      "    coord_tfm.before_call(x0)\n",
      "    return x.affine_coord(coord_tfm=coord_tfm, sz=size, mode=mode, pad_mode=pad_mode)\n",
      "['TensorTypes.warp']\n",
      "# export\n",
      "def Warp(magnitude=0.2, p=0.5, draw_x=None, draw_y=None,size=None, mode='bilinear', pad_mode=PadMode.Reflection):\n",
      "    \"Apply perspective warping with `magnitude` and `p` on a batch of matrices\"\n",
      "    return AffineCoordTfm(coord_fs=_WarpCoord(magnitude=magnitude, p=p, draw_x=draw_x, draw_y=draw_y),\n",
      "                          size=size, mode=mode, pad_mode=pad_mode)\n",
      "['Warp']\n",
      "# export\n",
      "def logit(x):\n",
      "    \"Logit of `x`, clamped to avoid inf.\"\n",
      "    x = x.clamp(1e-7, 1-1e-7)\n",
      "    return -(1/x-1).log()\n",
      "['logit']\n",
      "# export\n",
      "@patch\n",
      "def lighting(x: TensorImage, func):\n",
      "    return TensorImage(torch.sigmoid(func(logit(x))))\n",
      "['TensorImage.lighting']\n",
      "# export\n",
      "class LightingTfm(RandTransform):\n",
      "    \"Apply `fs` to the logits\"\n",
      "    order = 40\n",
      "    def __init__(self, fs): self.fs=L(fs)\n",
      "    def before_call(self, b, split_idx):\n",
      "        self.do = True\n",
      "        if isinstance(b, tuple): b = b[0]\n",
      "        for t in self.fs: t.before_call(b)\n",
      "\n",
      "    def compose(self, tfm):\n",
      "        \"Compose `self` with another `LightingTransform`\"\n",
      "        self.fs += tfm.fs\n",
      "\n",
      "    def encodes(self,x:TensorImage): return x.lighting(partial(compose_tfms, tfms=self.fs))\n",
      "['LightingTfm']\n",
      "# export\n",
      "class _BrightnessLogit():\n",
      "    def __init__(self, max_lighting=0.2, p=0.75, draw=None):\n",
      "        self.max_lighting,self.p,self.draw = max_lighting,p,draw\n",
      "\n",
      "    def _def_draw(self, x): return x.new(x.size(0)).uniform_(0.5*(1-self.max_lighting), 0.5*(1+self.max_lighting))\n",
      "\n",
      "    def before_call(self, x):\n",
      "        self.change = _draw_mask(x, self._def_draw, draw=self.draw, p=self.p, neutral=0.5)\n",
      "\n",
      "    def __call__(self, x): return x.add_(logit(self.change[:,None,None,None]))\n",
      "[]\n",
      "# export\n",
      "@delegates(_BrightnessLogit.__init__)\n",
      "@patch\n",
      "def brightness(x: TensorImage, **kwargs):\n",
      "    func = _BrightnessLogit(**kwargs)\n",
      "    func.before_call(x)\n",
      "    return x.lighting(func)\n",
      "['TensorImage.brightness']\n",
      "# export\n",
      "def Brightness(max_lighting=0.2, p=0.75, draw=None):\n",
      "    \"Apply change in brightness of `max_lighting` to batch of images with probability `p`.\"\n",
      "    return LightingTfm(_BrightnessLogit(max_lighting, p, draw))\n",
      "['Brightness']\n",
      "# export\n",
      "class _ContrastLogit():\n",
      "    def __init__(self, max_lighting=0.2, p=0.75, draw=None):\n",
      "        self.max_lighting,self.p,self.draw = max_lighting,p,draw\n",
      "\n",
      "    def _def_draw(self, x):\n",
      "        return torch.exp(x.new(x.size(0)).uniform_(math.log(1-self.max_lighting), -math.log(1-self.max_lighting)))\n",
      "\n",
      "    def before_call(self, x):\n",
      "        self.change = _draw_mask(x, self._def_draw, draw=self.draw, p=self.p, neutral=1.)\n",
      "\n",
      "    def __call__(self, x): return x.mul_(self.change[:,None,None,None])\n",
      "[]\n",
      "# export\n",
      "@delegates(_ContrastLogit.__init__)\n",
      "@patch\n",
      "def contrast(x: TensorImage, **kwargs):\n",
      "    func = _ContrastLogit(**kwargs)\n",
      "    func.before_call(x)\n",
      "    return x.lighting(func)\n",
      "['TensorImage.contrast']\n",
      "# export\n",
      "def Contrast(max_lighting=0.2, p=0.75, draw=None):\n",
      "    \"Apply change in contrast of `max_lighting` to batch of images with probability `p`.\"\n",
      "    return LightingTfm(_ContrastLogit(max_lighting, p, draw))\n",
      "['Contrast']\n",
      "# export\n",
      "def _compose_same_tfms(tfms):\n",
      "    tfms = L(tfms)\n",
      "    if len(tfms) == 0: return None\n",
      "    res = tfms[0]\n",
      "    for tfm in tfms[1:]: res.compose(tfm)\n",
      "    return res\n",
      "[]\n",
      "# export\n",
      "def setup_aug_tfms(tfms):\n",
      "    \"Go through `tfms` and combines together affine/coord or lighting transforms\"\n",
      "    aff_tfms = [tfm for tfm in tfms if isinstance(tfm, AffineCoordTfm)]\n",
      "    lig_tfms = [tfm for tfm in tfms if isinstance(tfm, LightingTfm)]\n",
      "    others = [tfm for tfm in tfms if tfm not in aff_tfms+lig_tfms]\n",
      "    aff_tfm,lig_tfm =  _compose_same_tfms(aff_tfms),_compose_same_tfms(lig_tfms)\n",
      "    res = [aff_tfm] if aff_tfm is not None else []\n",
      "    if lig_tfm is not None: res.append(lig_tfm)\n",
      "    return res + others\n",
      "['setup_aug_tfms']\n",
      "# export\n",
      "def aug_transforms(do_flip=True, flip_vert=False, max_rotate=10., max_zoom=1.1, max_lighting=0.2,\n",
      "                   max_warp=0.2, p_affine=0.75, p_lighting=0.75, xtra_tfms=None,\n",
      "                   size=None, mode='bilinear', pad_mode=PadMode.Reflection):\n",
      "    \"Utility func to easily create a list of flip, rotate, zoom, warp, lighting transforms.\"\n",
      "    res,tkw = [],dict(size=size, mode=mode, pad_mode=pad_mode)\n",
      "    if do_flip:    res.append(Dihedral(p=0.5, **tkw) if flip_vert else Flip(p=0.5, **tkw))\n",
      "    if max_warp:   res.append(Warp(magnitude=max_warp, p=p_affine, **tkw))\n",
      "    if max_rotate: res.append(Rotate(max_deg=max_rotate, p=p_affine, **tkw))\n",
      "    if max_zoom>1: res.append(Zoom(max_zoom=max_zoom, p=p_affine, **tkw))\n",
      "    if max_lighting:\n",
      "        res.append(Brightness(max_lighting=max_lighting, p=p_lighting))\n",
      "        res.append(Contrast(max_lighting=max_lighting, p=p_lighting))\n",
      "    return setup_aug_tfms(res + L(xtra_tfms))\n",
      "['aug_transforms']\n"
     ]
    },
    {
     "ename": "CellExecutionError",
     "evalue": "An error occurred while executing the following cell:\n------------------\nshow_doc(, default_cls_level=3)\n------------------\n\n\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-92495c01d190>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    show_doc(, default_cls_level=3)\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n\nSyntaxError: invalid syntax (<ipython-input-17-92495c01d190>, line 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCellExecutionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-931e40731d6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconvert_nb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'09_vision_augment.ipynb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../docs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-f7a414992e16>\u001b[0m in \u001b[0;36mconvert_nb\u001b[0;34m(fname, dest_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmeta_jekyll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cells'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmeta_jekyll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nb_path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{fname.parent.name}/{fname.name}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecute_nb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mnb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cells'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean_exports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cells'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#print(f'{dest_path}/{dest_name}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-f37815134ff7>\u001b[0m in \u001b[0;36mexecute_nb\u001b[0;34m(nb, mod, metadata, show_doc_only, name)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mpnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpnb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nbconvert/preprocessors/execute.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, nb, resources, km)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_preprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Executing notebook with kernel: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             \u001b[0mnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExecutePreprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m             \u001b[0minfo_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'language_info'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo_msg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'language_info'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nbconvert/preprocessors/base.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, nb, resources)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \"\"\"\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcells\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcells\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-0b36676ccb9c>\u001b[0m in \u001b[0;36mpreprocess_cell\u001b[0;34m(self, cell, resources, index)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'source'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cell_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"code\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_re_cell_to_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nbconvert/preprocessors/execute.py\u001b[0m in \u001b[0;36mpreprocess_cell\u001b[0;34m(self, cell, resources, cell_index, store_history)\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'error'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mCellExecutionError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_cell_and_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'status'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'error'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCellExecutionError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_cell_and_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCellExecutionError\u001b[0m: An error occurred while executing the following cell:\n------------------\nshow_doc(, default_cls_level=3)\n------------------\n\n\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-92495c01d190>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    show_doc(, default_cls_level=3)\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n\nSyntaxError: invalid syntax (<ipython-input-17-92495c01d190>, line 1)\n"
     ]
    }
   ],
   "source": [
    "convert_nb('09_vision_augment.ipynb', '../docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def convert_all(path='.', dest_path='../docs', force_all=False):\n",
    "    \"Convert all notebooks in `path` to html files in `dest_path`.\"\n",
    "    path = Path(path)\n",
    "    changed_cnt = 0\n",
    "    for fname in path.glob(\"[0-9]*.ipynb\"):\n",
    "        # only rebuild modified files\n",
    "        if fname.name.startswith('_'): continue\n",
    "        fname_out = Path(dest_path)/'.'.join(fname.with_suffix('.html').name.split('_')[1:])\n",
    "        if not force_all and fname_out.exists() and os.path.getmtime(fname) < os.path.getmtime(fname_out):\n",
    "            continue\n",
    "        print(f\"converting: {fname} => {fname_out}\")\n",
    "        changed_cnt += 1\n",
    "        try: convert_nb(fname, dest_path=dest_path)\n",
    "        except Exception as e: print(e)\n",
    "    if changed_cnt==0: print(\"No notebooks were modified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# convert_all(force_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def convert_post(fname, dest_path='posts'):\n",
    "    \"Convert a notebook `fname` to blog post markdown in `dest_path`.\"\n",
    "    fname = Path(fname).absolute()\n",
    "    nb = read_nb(fname)\n",
    "    meta_jekyll = get_metadata(nb['cells'])\n",
    "    nb['cells'] = compose(*process_cells)(nb['cells'])\n",
    "    nb['cells'] = [compose(*process_cell)(c) for c in nb['cells']]\n",
    "    fname = Path(fname).absolute()\n",
    "    dest_name = fname.with_suffix('.md').name\n",
    "    exp = _exporter(markdown=True)\n",
    "    with (Path(dest_path)/dest_name).open('w') as f:\n",
    "        f.write(exp.from_notebook_node(nb, resources=meta_jekyll)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_post('posts/2019-08-06-delegation.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_test.ipynb.\n",
      "Converted 01_core.ipynb.\n",
      "Converted 01a_torch_core.ipynb.\n",
      "Converted 02_script.ipynb.\n",
      "Converted 03_dataloader.ipynb.\n",
      "Converted 04_transform.ipynb.\n",
      "Converted 05_data_core.ipynb.\n",
      "Converted 06_data_transforms.ipynb.\n",
      "Converted 07_vision_core.ipynb.\n",
      "Converted 08_pets_tutorial.ipynb.\n",
      "Converted 09_vision_augment.ipynb.\n",
      "Converted 11_layers.ipynb.\n",
      "Converted 11a_vision_models_xresnet.ipynb.\n",
      "Converted 12_optimizer.ipynb.\n",
      "Converted 13_learner.ipynb.\n",
      "Converted 14_callback_schedule.ipynb.\n",
      "Converted 15_callback_hook.ipynb.\n",
      "Converted 16_callback_progress.ipynb.\n",
      "Converted 17_callback_tracker.ipynb.\n",
      "Converted 18_callback_fp16.ipynb.\n",
      "Converted 19_callback_mixup.ipynb.\n",
      "Converted 20_metrics.ipynb.\n",
      "Converted 21_tutorial_imagenette.ipynb.\n",
      "Converted 22_vision_learner.ipynb.\n",
      "Converted 23_tutorial_transfer_learning.ipynb.\n",
      "Converted 30_text_core.ipynb.\n",
      "Converted 31_text_data.ipynb.\n",
      "Converted 32_text_models_awdlstm.ipynb.\n",
      "Converted 33_text_models_core.ipynb.\n",
      "Converted 34_callback_rnn.ipynb.\n",
      "Converted 35_tutorial_wikitext.ipynb.\n",
      "Converted 36_text_models_qrnn.ipynb.\n",
      "Converted 37_text_learner.ipynb.\n",
      "Converted 38_tutorial_ulmfit.ipynb.\n",
      "Converted 40_tabular_core.ipynb.\n",
      "Converted 41_tabular_model.ipynb.\n",
      "Converted 42_tabular_rapids.ipynb.\n",
      "Converted 50_data_block.ipynb.\n",
      "Converted 90_notebook_core.ipynb.\n",
      "Converted 91_notebook_export.ipynb.\n",
      "Converted 92_notebook_showdoc.ipynb.\n",
      "Converted 93_notebook_export2html.ipynb.\n",
      "Converted 94_index.ipynb.\n",
      "Converted 95_utils_test.ipynb.\n",
      "Converted 96_data_external.ipynb.\n",
      "Converted notebook2jekyll.ipynb.\n",
      "Converted tmp.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "notebook2script(all_fs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def debug_nb(fname, dest=None):\n",
    "    fname = Path(fname).absolute()\n",
    "    nb = read_nb(fname)\n",
    "    cls_lvl = find_default_level(nb['cells'])\n",
    "    _name = _find_file(nb['cells'])\n",
    "    nb['cells'] = compose(*process_cells, partial(add_show_docs, cls_lvl=cls_lvl))(nb['cells'])\n",
    "    nb['cells'] = [compose(*process_cell)(c) for c in nb['cells']]\n",
    "    fname = Path(fname).absolute()\n",
    "    nb = execute_nb(nb, name=_name)\n",
    "    dest = dest or fname.with_suffix('.dbg.ipynb')\n",
    "    nbformat.write(nbformat.from_dict(nb), open(dest, 'w'), version=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# debug_nb('93_notebook_export2html.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
