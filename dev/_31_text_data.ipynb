{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.imports import *\n",
    "from local.test import *\n",
    "from local.core import *\n",
    "from local.data.transform import *\n",
    "from local.data.core import *\n",
    "from local.data.source import *\n",
    "from local.data.external import *\n",
    "from local.data.pipeline import *\n",
    "from local.text.core import *\n",
    "from local.notebook.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp text.data\n",
    "#default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text data\n",
    "\n",
    "> Functions and transforms to help gather text data in a `DataSource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(count, min_freq=3, max_vocab=60000):\n",
    "    \"Create a vocab of `max_vocab` size from `Counter` `count` with items present more than `min_freq`\"\n",
    "    vocab = [o for o,c in count.most_common(max_vocab) if c >= min_freq]\n",
    "    for o in reversed(defaults.text_spec_tok): #Make sure all special tokens are in the vocab\n",
    "        if o in vocab: vocab.remove(o)\n",
    "        vocab.insert(0, o)\n",
    "    vocab = vocab[:max_vocab]\n",
    "    if len(vocab) < max_vocab and len(vocab)%8 != 0: \n",
    "        #Make sure vocab size is a multiple of 8 for fast mixed precision training\n",
    "        vocab += ['xxfake' for _ in range(0, 8-len(vocab)%8)]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(['a', 'a', 'a', 'a', 'b', 'b', 'c', 'c', 'd'])\n",
    "test_eq(set(make_vocab(count)), set(defaults.text_spec_tok + 'a xxfake'.split()))\n",
    "test_eq(len(make_vocab(count))%8, 0)\n",
    "test_eq(set(make_vocab(count, min_freq=1)), set(defaults.text_spec_tok + 'a b c d xxfake'.split()))\n",
    "test_eq(set(make_vocab(count,max_vocab=12, min_freq=1)), set(defaults.text_spec_tok + 'a b c'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Numericalize(ItemTransform):\n",
    "    \"Reversible transform of tokenized texts to numericalized ids\"\n",
    "    def __init__(self, vocab=None, min_freq=3, max_vocab=60000, sep=None):\n",
    "        self.sep = sep or defaults.text_token_sep\n",
    "        self.vocab,self.min_freq,self.max_vocab = vocab,min_freq,max_vocab\n",
    "        self.o2i = None if vocab is None else defaultdict(int, {v:k for k,v in enumerate(vocab)})\n",
    "    \n",
    "    def setup(self, dsrc):\n",
    "        if dsrc is None: return\n",
    "        if self.vocab is None:\n",
    "            dsrc = getattr(dsrc,'train',dsrc)\n",
    "            count = Counter(p for o in dsrc for p in o.split(self.sep))\n",
    "            self.vocab = make_vocab(count, min_freq=self.min_freq, max_vocab=self.max_vocab)\n",
    "            self.o2i = defaultdict(int, {v:k for k,v in enumerate(self.vocab) if v != 'xxfake'})\n",
    "\n",
    "    def encodes(self, o):      return [self.o2i[o_] for o_ in o.split(self.sep)]\n",
    "    def decodes(self, o)->Str: return self.sep.join([self.vocab[o_] for o_ in o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = Numericalize(min_freq=1, sep=' ')\n",
    "num.setup(L('This is an example of text', 'this is another text'))\n",
    "test_eq(set(num.vocab), set(defaults.text_spec_tok + 'This is an example of text this another xxfake'.split()))\n",
    "test_eq(len(num.vocab)%8, 0)\n",
    "start = 'This is an example of text'\n",
    "t = num(start)\n",
    "test_eq(t, [11, 9, 12, 13, 14, 10])\n",
    "test_eq(num.decode(t), start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = Numericalize(min_freq=2, sep=' ')\n",
    "num.setup(L('This is an example of text', 'this is another text'))\n",
    "test_eq(set(num.vocab), set(defaults.text_spec_tok + 'is text xxfake'.split()))\n",
    "test_eq(len(num.vocab)%8, 0)\n",
    "t = num(start)\n",
    "test_eq(t, [0, 9, 0, 0, 0, 10])\n",
    "test_eq(num.decode(t), f'{UNK} is {UNK} {UNK} {UNK} text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMPreloader -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMCollate():\n",
    "    def __init__(self, bs=64, seq_len=72): self.bs,self.seq_len,self.offset = bs,seq_len,None\n",
    "    def __call__(self, samples):\n",
    "        #Samples has more than bs elements if more than one text is needed to make one of the batch\n",
    "        i,res,s_len,s_txt = 0,[],0,[]\n",
    "        for s in samples:\n",
    "            s = tensor(s).long()\n",
    "            l = self.seq_len-s_len\n",
    "            s_txt.append(s[0][self.offset[i]:self.offset[i]+l+1])\n",
    "            s_len += len(s_txt[-1])\n",
    "            self.offset[i] = self.offset[i]+l if self.offset[i]+l < len(s[0]) else 0\n",
    "            if s_len >= self.seq_len+1:\n",
    "                i += 1\n",
    "                res.append(torch.cat(s_txt))\n",
    "                s_len,s_txt = 0,[]\n",
    "        res = torch.stack(res, dim=0)\n",
    "        return res[:,:-1],res[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [(range(21),), (range(32),), (range(10),), (range(16),), (range(26),)]\n",
    "cumlen = tensor([len(t) for t in items]).cumsum(0)\n",
    "tst = LMCollate(bs=5, seq_len=10)\n",
    "tst.offset = [0,0,21,0,5]\n",
    "res = tst([items[0], items[1], items[1], items[3], items[4]])\n",
    "\n",
    "for i in [0,1,3]: \n",
    "    test_eq(res[0][i], tensor(range(10)))\n",
    "    test_eq(res[1][i], tensor(range(1,11)))\n",
    "test_eq(res[0][2], tensor(range(21,31)))\n",
    "test_eq(res[1][2], tensor(range(22,32)))\n",
    "test_eq(res[0][4], tensor(range(5,15)))\n",
    "test_eq(res[1][4], tensor(range(6,16)))\n",
    "test_eq(tst.offset, [10, 10, 31, 10, 15])\n",
    "\n",
    "res = tst([items[0], items[1], items[1], items[2], items[3], items[4], items[4]])\n",
    "for i in [0,1]: \n",
    "    test_eq(res[0][i], tensor(range(10,20)))\n",
    "    test_eq(res[1][i], tensor(range(11,21)))\n",
    "test_eq(res[0][2], torch.cat([tensor([31]), tensor(range(9))]))\n",
    "test_eq(res[1][2], tensor(range(10)))\n",
    "test_eq(res[0][3], torch.cat([tensor(range(10,16)), tensor(range(4))]))\n",
    "test_eq(res[1][3], torch.cat([tensor(range(11,16)), tensor(range(5))]))\n",
    "test_eq(res[0][4], tensor(range(15,25)))\n",
    "test_eq(res[1][4], tensor(range(16,26)))\n",
    "test_eq(tst.offset, [20, 20, 9, 4, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMSampler(BatchSampler):\n",
    "    def __init__(self, ds, cf, lengths=None, bs=64, seq_len=72, shuffle=False):\n",
    "        self.ds,self.cf,self.bs,self.seq_len,self.shuffle = ds,cf,bs,seq_len,shuffle\n",
    "        self.lengths = [len(o[0]) for o in ds] if lengths is None else lengths\n",
    "        self.n_batch = sum(self.lengths) // bs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.batchify()\n",
    "        for i in range(0, self.n_batch-1, self.seq_len):\n",
    "            idx = tensor(range(self.bs)) * self.n_batch + i + self.seq_len\n",
    "            end_idx = len(self.cumlen) - (self.cumlen[:,None] > idx[None]).sum(0)\n",
    "            s_idx = [list(range(i1,i2+1)) for (i1,i2) in zip(self.start_idx, end_idx)]\n",
    "            yield [self.idxs[i] for s in s_idx for i in s]\n",
    "            self.start_idx = end_idx\n",
    "        \n",
    "    def batchify(self):\n",
    "        self.idxs = torch.randperm(len(self.ds)) if self.shuffle else tensor(range(len(self.ds)))\n",
    "        self.cumlen = (tensor(self.lengths)[self.idxs] if self.shuffle else tensor(self.lengths)).cumsum(0)\n",
    "        idx = tensor(range(self.bs)) * self.n_batch\n",
    "        self.start_idx = len(self.cumlen) - (self.cumlen[:,None] > idx[None]).sum(0)\n",
    "        self.cf.offset = idx - torch.cat([tensor([0]), self.cumlen])[self.start_idx]\n",
    "        \n",
    "    def __len__(self): return (self.n_batch-1) // self.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [(range(21),), (range(32),), (range(10),), (range(16),), (range(26),)]\n",
    "cf = LMCollate(bs=5, seq_len=10)\n",
    "s = LMSampler(items, cf, bs=5, seq_len=10)\n",
    "s.batchify()\n",
    "test_eq(cf.offset, tensor([0,0,21,0,5]))\n",
    "itr = iter(s)\n",
    "\n",
    "b1 = next(itr)\n",
    "test_eq(b1, [0, 1, 1, 3, 4])\n",
    "\n",
    "b2 = next(itr)\n",
    "test_eq(b2, [0, 1, 1, 2, 3, 4, 4])\n",
    "\n",
    "test_fail(lambda: next(itr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: make better\n",
    "cf = LMCollate(bs=5, seq_len=10)\n",
    "s = LMSampler(items, cf, bs=5, seq_len=10, shuffle=True)\n",
    "itr = iter(s)\n",
    "b1 = next(itr)\n",
    "b2 = next(itr)\n",
    "test_fail(lambda: next(itr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "class TensorTextBase(TensorBase):\n",
    "    \n",
    "    def get_ctxs(self, max_samples=10, **kwargs):\n",
    "        n_samples = min(self.shape[0], max_samples)\n",
    "        df = pd.DataFrame({'index': range(n_samples)})\n",
    "        return [df.iloc[i] for i in range(n_samples)]\n",
    "    \n",
    "    def display(self, ctxs):\n",
    "        df = pd.DataFrame(ctxs)\n",
    "        with pd.option_context('display.max_colwidth', -1): \n",
    "            display(HTML(df.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other approach\n",
    "class LM_PreLoader(GetAttr):\n",
    "    \"An intermediate between a dataset with texts and a DataLoader\"\n",
    "    _xtra = ['show', 'decode', 'show_at', 'decode_at', 'decode_batch']\n",
    "    def __init__(self, ds, lengths=None, bs=64, seq_len=70, shuffle=False):\n",
    "        self.ds,self.bs,self.seq_len,self.shuffle = ds,bs,seq_len,shuffle\n",
    "        self.lengths = [len(o[0]) for o in ds] if lengths is None else lengths\n",
    "        self.n_batch = sum(self.lengths) // bs\n",
    "        self.batchify()\n",
    "        self.default = self.ds\n",
    "    \n",
    "    def __len__(self): return ((self.n_batch-1) // self.seq_len) * self.bs\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        k = (i % self.bs) * self.n_batch + (i // self.bs) * self.seq_len\n",
    "        item_idx = (self.cumlen > k).nonzero().min().item()\n",
    "        offset = k if item_idx==0 else k-self.cumlen[item_idx-1]\n",
    "        text = self.ds[self.idxs[item_idx]][0][offset:]\n",
    "        while len(text) <= self.seq_len:\n",
    "            item_idx += 1\n",
    "            text += self.ds[self.idxs[item_idx]][0]\n",
    "        return TensorTextBase(tensor(text[:self.seq_len])),TensorTextBase(tensor(text[1:self.seq_len+1]))\n",
    "    \n",
    "    def batchify(self):\n",
    "        self.idxs = torch.randperm(len(ds)) if self.shuffle else tensor(range(len(self.ds)))\n",
    "        self.cumlen = (tensor(self.lengths)[self.idxs] if self.shuffle else tensor(self.lengths)).cumsum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [10,7,19,23,5,42]\n",
    "ds = LM_PreLoader([(list(range(l)), 0) for l in lengths], lengths=lengths, bs=5, seq_len=4)\n",
    "x,y = ds[0]\n",
    "test_eq(x[1:], y[:-1])\n",
    "test_eq(x+1, y)\n",
    "#Going on the seq dimension reads the text in order\n",
    "test_eq(torch.cat([ds[5*i][0] for i in range(5)]), \n",
    "        tensor(list(range(10))+list(range(7))+list(range(3))))\n",
    "#3 is skipped for the next sample in the natch since it's the last target\n",
    "test_eq(torch.cat([ds[5*i+1][0] for i in range(5)]),\n",
    "        tensor(list(range(4,19))+list(range(5))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "df = pd.read_csv(path/'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie wi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most u...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  is_valid\n",
       "0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
       "1  positive  This is a extremely well-made film. The acting...     False\n",
       "2  negative  Every once in a long while a movie will come a...     False\n",
       "3  positive  Name just says it all. I watched this movie wi...     False\n",
       "4  negative  This movie succeeds at being one of the most u...     False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tok,count = tokenize_df(df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>is_valid</th>\n",
       "      <th>text</th>\n",
       "      <th>text_lengths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>xxbos▁xxmaj▁un▁-▁bleeping▁-▁believable▁!▁xxmaj...</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>False</td>\n",
       "      <td>xxbos▁xxmaj▁this▁is▁a▁extremely▁well▁-▁made▁fi...</td>\n",
       "      <td>462.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>xxbos▁xxmaj▁every▁once▁in▁a▁long▁while▁a▁movie...</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>False</td>\n",
       "      <td>xxbos▁xxmaj▁name▁just▁says▁it▁all▁.▁i▁watched▁...</td>\n",
       "      <td>184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>False</td>\n",
       "      <td>xxbos▁xxmaj▁this▁movie▁succeeds▁at▁being▁one▁o...</td>\n",
       "      <td>398.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  is_valid                                               text  \\\n",
       "0  negative     False  xxbos▁xxmaj▁un▁-▁bleeping▁-▁believable▁!▁xxmaj...   \n",
       "1  positive     False  xxbos▁xxmaj▁this▁is▁a▁extremely▁well▁-▁made▁fi...   \n",
       "2  negative     False  xxbos▁xxmaj▁every▁once▁in▁a▁long▁while▁a▁movie...   \n",
       "3  positive     False  xxbos▁xxmaj▁name▁just▁says▁it▁all▁.▁i▁watched▁...   \n",
       "4  negative     False  xxbos▁xxmaj▁this▁movie▁succeeds▁at▁being▁one▁o...   \n",
       "\n",
       "   text_lengths  \n",
       "0         103.0  \n",
       "1         462.0  \n",
       "2         220.0  \n",
       "3         184.0  \n",
       "4         398.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tok.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts,lengths = df_tok['text'].values,df_tok['text_lengths'].map(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = RandomSplitter()(L(t for t in texts))\n",
    "dsrc = DataSource(L(t for t in texts), type_tfms=[Numericalize(make_vocab(count))], filts=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"xxbos▁xxmaj▁un▁-▁xxunk▁-▁believable▁!▁xxmaj▁meg▁xxmaj▁ryan▁does▁n't▁even▁look▁her▁usual▁xxunk▁lovable▁self▁in▁this▁,▁which▁normally▁makes▁me▁forgive▁her▁shallow▁xxunk▁acting▁xxunk▁.▁xxmaj▁hard▁to▁believe▁she▁was▁the▁producer▁on▁this▁dog▁.▁xxmaj▁plus▁xxmaj▁kevin▁xxmaj▁kline▁:▁what▁kind▁of▁suicide▁trip▁has▁his▁career▁been▁on▁?▁xxmaj▁xxunk▁...▁xxmaj▁xxunk▁!▁!▁!▁xxmaj▁finally▁this▁was▁directed▁by▁the▁guy▁who▁did▁xxmaj▁big▁xxmaj▁xxunk▁?▁xxmaj▁must▁be▁a▁replay▁of▁xxmaj▁jonestown▁-▁hollywood▁style▁.▁xxmaj▁xxunk▁!\",)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsrc.decode_at(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "ds = LM_PreLoader(dsrc.train, lengths=lengths[splits[0]], bs=bs)\n",
    "dl = TfmdDL(ds, bs=bs, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = dl.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10) [('xxbos▁i▁think▁xxmaj▁james▁xxmaj▁cameron▁might▁be▁becoming▁my▁favorite▁director▁because▁this▁is▁my▁second▁review▁of▁his▁movies▁.▁xxmaj▁anyway▁,▁everyone▁xxunk▁the▁xxup▁xxunk▁xxmaj▁titanic▁.▁xxmaj▁it▁was▁big▁,▁fast▁,▁and▁\"▁xxunk▁\"▁...▁until▁xxmaj▁april▁xxunk▁.▁xxmaj▁it▁was▁all▁over▁the▁news▁and▁one▁of▁the▁biggest▁tragedies▁ever▁.▁xxmaj▁well▁xxmaj▁james',),(',▁and▁xxunk▁knock▁it▁out▁of▁the▁number▁one▁spot▁.▁xxmaj▁every▁time▁i▁hear▁someone▁declare▁\"▁titanic▁\"▁is▁the▁greatest▁film▁they▁\\'ve▁ever▁seen▁,▁i▁think▁to▁myself▁,▁\"▁you▁do▁n\\'t▁see▁a▁lot▁of▁movies▁,▁do▁you▁?▁\"▁xxmaj▁what▁a▁travesty▁.▁xxmaj▁you▁could▁make▁50▁good▁films▁that▁are▁a▁lot▁better▁than▁\"▁titanic',),('every▁season▁,▁there▁are▁bound▁to▁be▁some▁weak▁ones▁.▁xxmaj▁however▁,▁most▁of▁the▁time▁each▁episode▁had▁an▁interesting▁story▁,▁some▁kind▁of▁conflict▁,▁and▁a▁resolution▁that▁usually▁did▁not▁include▁violence▁.▁xxmaj▁while▁xxmaj▁bonanza▁was▁a▁western▁,▁the▁xxunk▁was▁never▁featured▁as▁the▁main▁attraction▁.▁xxmaj▁while▁i▁am▁a▁fan▁of▁xxmaj▁the▁xxmaj▁xxunk',),('enters▁a▁small▁town▁trying▁to▁find▁work▁.▁xxmaj▁dennis▁xxmaj▁hopper▁is▁the▁bad▁guy▁and▁no▁one▁plays▁them▁better▁.▁xxmaj▁look▁for▁a▁brief▁appearance▁by▁country▁singing▁star▁xxmaj▁xxunk▁xxmaj▁xxunk▁.▁xxmaj▁this▁is▁a▁serious▁drama▁most▁of▁the▁time▁but▁there▁are▁some▁xxunk▁moments▁.▁xxmaj▁what▁matters▁is▁that▁you▁will▁enjoy▁this▁low▁budget▁but▁high▁quality',),(\"but▁(▁again▁)▁predictable▁and▁not▁even▁remotely▁scary▁.▁\\n\\n▁xxmaj▁it▁ends▁very▁stupidly▁.▁\\n\\n▁xxmaj▁all▁in▁all▁,▁the▁first▁one▁is▁worth▁watching▁,▁but▁that▁'s▁it▁.▁xxmaj▁tune▁in▁for▁that▁one▁then▁turn▁it▁off▁.▁xxbos▁xxmaj▁in▁this▁movie▁,▁xxmaj▁chávez▁xxunk▁(▁either▁venezuelan▁and▁not▁-▁venezuelan▁)▁just▁lie▁about▁a▁dramatic▁situation\",),('minute▁long▁nude▁xxunk▁scene▁involving▁xxmaj▁jane▁that▁is▁full▁frontal▁in▁its▁nudity▁,▁it▁was▁only▁recently▁restored▁)▁.▁xxmaj▁its▁clear▁watching▁the▁restored▁version▁why▁this▁film▁was▁reduced▁by▁20▁minutes▁in▁its▁run▁time▁for▁xxup▁tv▁.▁xxmaj▁as▁it▁stands▁in▁its▁restored▁version▁this▁is▁a▁very▁adult▁film▁that▁is▁romantic▁,▁touching▁,▁action▁filled▁and▁everything',),('.▁xxmaj▁katsu▁plays▁up▁his▁character▁\\'▁simple▁minded▁xxunk▁to▁a▁manipulative▁politician▁all▁in▁the▁name▁of▁patriotic▁pride▁.▁xxmaj▁anybody▁who▁questions▁the▁politician▁is▁labeled▁a▁\"▁xxunk▁\"▁and▁becomes▁an▁assassination▁target▁.▁\\n\\n▁xxmaj▁one▁of▁the▁best▁photographed▁films▁ever▁,▁many▁shots▁are▁incredible▁xxunk▁of▁form▁,▁color▁and▁light▁.▁xxmaj▁the▁fight▁scenes▁are▁frequent▁and',),(\"\\n\\n▁-▁xxmaj▁the▁xxunk▁focus▁of▁the▁movie▁was▁a▁repeating▁,▁compound▁xxunk▁with▁exploding▁xxunk▁.▁xxmaj▁it▁never▁needed▁to▁be▁loaded▁and▁even▁had▁a▁xxunk▁when▁fired▁.▁xxmaj▁it▁managed▁to▁shred▁the▁laws▁of▁physics▁,▁the▁integrity▁of▁the▁original▁legend▁,▁historical▁fact▁and▁plot▁suspense▁all▁by▁itself▁.▁\\n\\n▁-▁xxmaj▁xxunk▁'s▁palace▁,▁xxmaj▁xxunk▁,\",),('as▁well▁.▁i▁felt▁it▁was▁almost▁as▁entertaining▁as▁the▁film▁it▁was▁made▁for▁.▁xxmaj▁there▁are▁some▁great▁interviews▁and▁behind▁the▁scenes▁footage▁,▁mixed▁with▁news▁stories▁about▁the▁film▁from▁some▁major▁places▁like▁xxup▁xxunk▁,▁xxup▁fox▁and▁xxup▁mtv▁.▁xxmaj▁over▁all▁,▁a▁fun▁little▁film▁that▁is▁xxup▁very▁rough▁around▁the▁edges▁,▁but▁still',),(\"way▁,▁to▁help▁out▁.▁xxmaj▁madison▁figures▁the▁best▁way▁to▁help▁xxmaj▁ungar▁is▁to▁let▁him▁move▁in▁with▁him▁until▁his▁xxunk▁xxunk▁wear▁off▁.▁\\n\\n▁xxmaj▁unfortunately▁for▁xxmaj▁madison▁,▁he▁does▁n't▁know▁what▁he▁'s▁getting▁himself▁into▁.▁xxmaj▁madison▁is▁a▁xxunk▁happy▁-▁go▁-▁lucky▁if▁rather▁irresponsible▁xxunk▁who▁'s▁refrigerator▁was▁last▁cleaned▁probably\",)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.decode_batch((x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xxbos▁i▁think▁xxmaj▁james▁xxmaj▁cameron▁might▁be▁becoming▁my▁favorite▁director▁because▁this▁is▁my▁second▁review▁of▁his▁movies▁.▁xxmaj▁anyway▁,▁everyone▁xxunk▁the▁xxup▁xxunk▁xxmaj▁titanic▁.▁xxmaj▁it▁was▁big▁,▁fast▁,▁and▁\"▁xxunk▁\"▁...▁until▁xxmaj▁april▁xxunk▁.▁xxmaj▁it▁was▁all▁over▁the▁news▁and▁one▁of▁the▁biggest▁tragedies▁ever▁.▁xxmaj▁well▁xxmaj▁james',)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.decode((x[0],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ToTensor\n",
    "def encodes(x: Str) -> TensorTextBase: return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_title1(o, ax=None, ctx=None):\n",
    "    \"Set title of `ax` to `o`, or print `o` if `ax` is `None`\"\n",
    "    ax = ifnone(ax,ctx)\n",
    "    if ax is None: print(o)\n",
    "    elif isinstance(ax, pd.Series): ax = ax.append(pd.Series({'text': o}))\n",
    "    else: ax.set_title(o)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _show(self, ctx=None, **kwargs): return show_title1(str(self), ctx=ctx)\n",
    "Str.show = _show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsrc = DataSource(L(t for t in texts), type_tfms=[Numericalize(make_vocab(count))], filts=splits)\n",
    "bs = 16\n",
    "ds = LM_PreLoader(dsrc.train, lengths=lengths[splits[0]], bs=bs)\n",
    "dl = TfmdDL(ds, bs=bs, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>xxbos▁i▁think▁xxmaj▁james▁xxmaj▁cameron▁might▁be▁becoming▁my▁favorite▁director▁because▁this▁is▁my▁second▁review▁of▁his▁movies▁.▁xxmaj▁anyway▁,▁everyone▁xxunk▁the▁xxup▁xxunk▁xxmaj▁titanic▁.▁xxmaj▁it▁was▁big▁,▁fast▁,▁and▁\"▁xxunk▁\"▁...▁until▁xxmaj▁april▁xxunk▁.▁xxmaj▁it▁was▁all▁over▁the▁news▁and▁one▁of▁the▁biggest▁tragedies▁ever▁.▁xxmaj▁well▁xxmaj▁james</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>,▁and▁xxunk▁knock▁it▁out▁of▁the▁number▁one▁spot▁.▁xxmaj▁every▁time▁i▁hear▁someone▁declare▁\"▁titanic▁\"▁is▁the▁greatest▁film▁they▁'ve▁ever▁seen▁,▁i▁think▁to▁myself▁,▁\"▁you▁do▁n't▁see▁a▁lot▁of▁movies▁,▁do▁you▁?▁\"▁xxmaj▁what▁a▁travesty▁.▁xxmaj▁you▁could▁make▁50▁good▁films▁that▁are▁a▁lot▁better▁than▁\"▁titanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>every▁season▁,▁there▁are▁bound▁to▁be▁some▁weak▁ones▁.▁xxmaj▁however▁,▁most▁of▁the▁time▁each▁episode▁had▁an▁interesting▁story▁,▁some▁kind▁of▁conflict▁,▁and▁a▁resolution▁that▁usually▁did▁not▁include▁violence▁.▁xxmaj▁while▁xxmaj▁bonanza▁was▁a▁western▁,▁the▁xxunk▁was▁never▁featured▁as▁the▁main▁attraction▁.▁xxmaj▁while▁i▁am▁a▁fan▁of▁xxmaj▁the▁xxmaj▁xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>enters▁a▁small▁town▁trying▁to▁find▁work▁.▁xxmaj▁dennis▁xxmaj▁hopper▁is▁the▁bad▁guy▁and▁no▁one▁plays▁them▁better▁.▁xxmaj▁look▁for▁a▁brief▁appearance▁by▁country▁singing▁star▁xxmaj▁xxunk▁xxmaj▁xxunk▁.▁xxmaj▁this▁is▁a▁serious▁drama▁most▁of▁the▁time▁but▁there▁are▁some▁xxunk▁moments▁.▁xxmaj▁what▁matters▁is▁that▁you▁will▁enjoy▁this▁low▁budget▁but▁high▁quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>but▁(▁again▁)▁predictable▁and▁not▁even▁remotely▁scary▁.▁\\n\\n▁xxmaj▁it▁ends▁very▁stupidly▁.▁\\n\\n▁xxmaj▁all▁in▁all▁,▁the▁first▁one▁is▁worth▁watching▁,▁but▁that▁'s▁it▁.▁xxmaj▁tune▁in▁for▁that▁one▁then▁turn▁it▁off▁.▁xxbos▁xxmaj▁in▁this▁movie▁,▁xxmaj▁chávez▁xxunk▁(▁either▁venezuelan▁and▁not▁-▁venezuelan▁)▁just▁lie▁about▁a▁dramatic▁situation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>minute▁long▁nude▁xxunk▁scene▁involving▁xxmaj▁jane▁that▁is▁full▁frontal▁in▁its▁nudity▁,▁it▁was▁only▁recently▁restored▁)▁.▁xxmaj▁its▁clear▁watching▁the▁restored▁version▁why▁this▁film▁was▁reduced▁by▁20▁minutes▁in▁its▁run▁time▁for▁xxup▁tv▁.▁xxmaj▁as▁it▁stands▁in▁its▁restored▁version▁this▁is▁a▁very▁adult▁film▁that▁is▁romantic▁,▁touching▁,▁action▁filled▁and▁everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>.▁xxmaj▁katsu▁plays▁up▁his▁character▁'▁simple▁minded▁xxunk▁to▁a▁manipulative▁politician▁all▁in▁the▁name▁of▁patriotic▁pride▁.▁xxmaj▁anybody▁who▁questions▁the▁politician▁is▁labeled▁a▁\"▁xxunk▁\"▁and▁becomes▁an▁assassination▁target▁.▁\\n\\n▁xxmaj▁one▁of▁the▁best▁photographed▁films▁ever▁,▁many▁shots▁are▁incredible▁xxunk▁of▁form▁,▁color▁and▁light▁.▁xxmaj▁the▁fight▁scenes▁are▁frequent▁and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>\\n\\n▁-▁xxmaj▁the▁xxunk▁focus▁of▁the▁movie▁was▁a▁repeating▁,▁compound▁xxunk▁with▁exploding▁xxunk▁.▁xxmaj▁it▁never▁needed▁to▁be▁loaded▁and▁even▁had▁a▁xxunk▁when▁fired▁.▁xxmaj▁it▁managed▁to▁shred▁the▁laws▁of▁physics▁,▁the▁integrity▁of▁the▁original▁legend▁,▁historical▁fact▁and▁plot▁suspense▁all▁by▁itself▁.▁\\n\\n▁-▁xxmaj▁xxunk▁'s▁palace▁,▁xxmaj▁xxunk▁,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>as▁well▁.▁i▁felt▁it▁was▁almost▁as▁entertaining▁as▁the▁film▁it▁was▁made▁for▁.▁xxmaj▁there▁are▁some▁great▁interviews▁and▁behind▁the▁scenes▁footage▁,▁mixed▁with▁news▁stories▁about▁the▁film▁from▁some▁major▁places▁like▁xxup▁xxunk▁,▁xxup▁fox▁and▁xxup▁mtv▁.▁xxmaj▁over▁all▁,▁a▁fun▁little▁film▁that▁is▁xxup▁very▁rough▁around▁the▁edges▁,▁but▁still</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>way▁,▁to▁help▁out▁.▁xxmaj▁madison▁figures▁the▁best▁way▁to▁help▁xxmaj▁ungar▁is▁to▁let▁him▁move▁in▁with▁him▁until▁his▁xxunk▁xxunk▁wear▁off▁.▁\\n\\n▁xxmaj▁unfortunately▁for▁xxmaj▁madison▁,▁he▁does▁n't▁know▁what▁he▁'s▁getting▁himself▁into▁.▁xxmaj▁madison▁is▁a▁xxunk▁happy▁-▁go▁-▁lucky▁if▁rather▁irresponsible▁xxunk▁who▁'s▁refrigerator▁was▁last▁cleaned▁probably</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dl.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
