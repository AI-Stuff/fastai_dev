{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.imports import *\n",
    "from local.test import *\n",
    "from local.core import *\n",
    "from local.notebook.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "> Define the general fastai optimizer and the variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Optimizer():\n",
    "    \"Base optimizer class for the fastai library, updating `params` with `steppers`\"\n",
    "    def __init__(self, params, steppers, **defaults):\n",
    "        steppers,params = L(steppers),L(params)\n",
    "        for step in steppers: defaults = {**getattr(step, 'defaults', {}), **defaults}\n",
    "        self.param_groups = params if isinstance(params[0], (L,list)) else L([params])\n",
    "        self.step_func = compose(*steppers)\n",
    "        self.hypers = L({**defaults} for p in self.param_groups)\n",
    "\n",
    "    def grad_params(self):\n",
    "        \"Helper function to loop over param groups then params that have a grad\"\n",
    "        return [(p,hyper) for pg,hyper in zip(self.param_groups,self.hypers)\n",
    "            for p in pg if p.grad is not None]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero all the grad attributes of the parameters\"\n",
    "        for p,hyper in self.grad_params():\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        \"Execute steppers on all parameters that have a grad\"\n",
    "        for p,hyper in self.grad_params(): self.step_func(p, **hyper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing an Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`params` will be used to create the `param_groups` of the optimizer. If it's a colection (or a generator) of parameters, it will be a `L` containing one `L` with all the parameters. To define multiple parameter groups `params` should be passed as a collection (or a generator) of `L`s.\n",
    "\n",
    "> Note: In PyTorch, `model.parameters()` returns a generator with all the parameters, that you can directly pass to `Optimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer([1,2,3], noop)\n",
    "test_eq(opt.param_groups, [[1,2,3]])\n",
    "opt = Optimizer(range(3), noop)\n",
    "test_eq(opt.param_groups, [[0,1,2]])\n",
    "opt = Optimizer([[1,2],[3]], noop)\n",
    "test_eq(opt.param_groups, [[1,2],[3]])\n",
    "opt = Optimizer(([o,o+1] for o in range(0,4,2)), noop)\n",
    "test_eq(opt.param_groups, [[0,1],[2,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`steppers` is a list of functions that will be composed when applying the step. For instance, you can compose a function making the SGD step, with one applying weight decay. Additionaly, each `stepper` can have a `defaults` attribute that contains hyper-parameters and their default value. Those are all gathered at initialization, and new value can be passed to overrided those defaults with the `defaults` kwargs.\n",
    "\n",
    "Once the defaults have all been pulled off, they are copied as many times as there are `param_groups` and stored in `hypers`. To apply different hyper-parameters to different groups (differential learning rates, or no weight decay for certain layers for instance), you will need to adjsut those values after the init. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tst_arg(p, lr=0, **kwargs): return p\n",
    "tst_arg.defaults = dict(lr=1e-2)\n",
    "\n",
    "opt = Optimizer([1,2,3], tst_arg)\n",
    "test_eq(opt.hypers, [{'lr': 1e-2}])\n",
    "opt = Optimizer([1,2,3], tst_arg, lr=0.1)\n",
    "test_eq(opt.hypers, [{'lr': 0.1}])\n",
    "opt = Optimizer([[1,2],[3]], tst_arg)\n",
    "test_eq(opt.hypers, [{'lr': 1e-2}, {'lr': 1e-2}])\n",
    "opt = Optimizer([[1,2],[3]], tst_arg, lr=0.1)\n",
    "test_eq(opt.hypers, [{'lr': 0.1}, {'lr': 0.1}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic steppers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to give examples of optimizer steps, we will need some steppers, like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sgd_step(p, lr, **kwargs):\n",
    "    p.data.add_(-lr, p.grad.data)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tst_param(val, grad):\n",
    "    \"Create a tensor with `val` and a gradient of `grad` for testing\"\n",
    "    res = tensor([val]).float()\n",
    "    res.grad = tensor([grad]).float()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tst_param(1., 0.1)\n",
    "p = sgd_step(p, 1.)\n",
    "test_eq(p, tensor([0.9]))\n",
    "test_eq(p.grad, tensor([0.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def weight_decay(p, lr, wd, **kwargs):\n",
    "    \"Weight decay as decaying `p` with `lr*wd`\"\n",
    "    p.data.mul_(1 - lr*wd)\n",
    "    return p\n",
    "weight_decay.defaults = dict(wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tst_param(1., 0.1)\n",
    "p = weight_decay(p, 1., 0.1)\n",
    "test_eq(p, tensor([0.9]))\n",
    "test_eq(p.grad, tensor([0.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def l2_reg(p, lr, wd, **kwargs):\n",
    "    \"L2 regularization as adding `wd*p` to `p.grad`\"\n",
    "    p.grad.data.add_(wd, p.data)\n",
    "    return p\n",
    "l2_reg.defaults = dict(wd=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tst_param(1., 0.1)\n",
    "p = l2_reg(p, 1., 0.1)\n",
    "test_eq(p, tensor([1.]))\n",
    "test_eq(p.grad, tensor([0.2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: Weight decay and L2 regularization is the same thing for basic SGD, but for more complex optimizers, they are very different. See [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Optimizer.step\" class=\"doc_header\"><code>Optimizer.step</code><a href=\"https://github.com/fastai/fastai_docs/tree/master/dev/__main__.py#L22\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Optimizer.step</code>()\n",
       "\n",
       "Execute steppers on all parameters that have a grad"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Optimizer.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method will loop over all param groups, then all parameters for which `grad` is not None and call each function in `stepper`, passing it the parameter `p` with the hyper-parameters in the corresponding dict in `hypers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test basic step\n",
    "def tst_params(): return [tst_param(i, 0.1*i) for i in range(4)]\n",
    "\n",
    "params = tst_params()\n",
    "opt = Optimizer(params, sgd_step, lr=0.1)\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [i*0.99 for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test two steps\n",
    "params = tst_params()\n",
    "opt = Optimizer(params, [weight_decay, sgd_step], lr=0.1, wd=0.1)\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [i*0.98 for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test None gradients are ignored\n",
    "params = tst_params()\n",
    "opt = Optimizer(params, sgd_step, lr=0.1)\n",
    "params[-1].grad = None\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [0., 0.99, 1.98, 3.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test discriminative lrs\n",
    "params = tst_params()\n",
    "opt = Optimizer([params[:2], params[2:]], sgd_step, lr=0.1)\n",
    "opt.hypers[0]['lr'] = 0.01\n",
    "opt.step()\n",
    "test_close([p.item() for p in params], [0., 0.999, 1.98, 2.97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Optimizer.zero_grad\" class=\"doc_header\"><code>Optimizer.zero_grad</code><a href=\"https://github.com/fastai/fastai_docs/tree/master/dev/__main__.py#L16\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Optimizer.zero_grad</code>()\n",
       "\n",
       "Zero all the grad attributes of the parameters"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Optimizer.zero_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tst_params()\n",
    "opt = Optimizer(params, [weight_decay, sgd_step], lr=0.1, wd=0.1)\n",
    "opt.zero_grad()\n",
    "[test_eq(p.grad, tensor([0.])) for p in params];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Optimizer.grad_params\" class=\"doc_header\"><code>Optimizer.grad_params</code><a href=\"https://github.com/fastai/fastai_docs/tree/master/dev/__main__.py#L11\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Optimizer.grad_params</code>()\n",
       "\n",
       "Helper function to loop over param groups then params that have a grad"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Optimizer.grad_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used by `Optimizer.step` and `Optimizer.zero_grad` to loop first over the `param_groups` then over all the parameters that have a gradient, and return the tuples `(p,hyper)` where `hyper` is the dictionary of hyper-parameters associated to the parameter groups `p` is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tst_params()\n",
    "opt = Optimizer([params[:2], params[2:]], sgd_step, lr=0.1)\n",
    "opt.hypers[0]['lr'] = 0.01\n",
    "test_eq(opt.grad_params(), [(tensor([0.]), {'lr': 0.01}),\n",
    "                            (tensor([1.]), {'lr': 0.01}),\n",
    "                            (tensor([2.]), {'lr': 0.1}),\n",
    "                            (tensor([3.]), {'lr': 0.1})])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StatefulOptimizer -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class StatefulOptimizer(Optimizer):\n",
    "    \"`Optimizer` that can have state through `stats`\"\n",
    "    def __init__(self, params, steppers, stats=None, **defaults): \n",
    "        self.stats = L(stats)\n",
    "        for stat in self.stats: defaults = {**getattr(stat, 'defaults', {}), **defaults}\n",
    "        super().__init__(params, steppers, **defaults)\n",
    "        self.state = {}\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update the stats and execute the steppers in on all parameters that have a grad\"\n",
    "        for p,hyper in self.grad_params():\n",
    "            state = self.state.get(p, {})\n",
    "            for stat in self.stats: state = stat(state, p, **hyper)\n",
    "            self.step_func(p, **state, **hyper)\n",
    "            self.state[p] = state\n",
    "    \n",
    "    def _init_state(self, p):\n",
    "        \"Create a state for p and call all the statistics to initialize it\"\n",
    "        state = {}\n",
    "        for stat in self.stats: state = {**getattr(stat, \"init_state\", lambda:{})(p), **state}\n",
    "        self.state[p] = state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between a `StatefulOptimizer` and an `Optimizer` is that a `StatefulOptimzier` keeps a state for things like moving averages of gradients. It does so with `stats` which basically are functions taking the state associated to a parameter, that parameter, plus the optimizer hyper-parameters and updates the state. That state can then be used by any stepper. It is initiliazed to an empty dictionary the first time we try to access it, then the `stat` function will have to properly initiliaze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tst_stat(state, p, **kwargs): \n",
    "    state['sum'] = state.get('sum', torch.zeros_like(p)) + p.data\n",
    "    return state\n",
    "tst_stat.defaults = {'mom': 0.9}\n",
    "\n",
    "#Test StatefulOptimizer init\n",
    "opt = StatefulOptimizer([1,2,3], noop, stats=tst_stat)\n",
    "test_eq(opt.hypers, [{'mom': 0.9}])\n",
    "opt = StatefulOptimizer([1,2,3], noop, stats=tst_stat, mom=0.99)\n",
    "test_eq(opt.hypers, [{'mom': 0.99}])\n",
    "\n",
    "#Test stat\n",
    "x = torch.randn(4,5)\n",
    "state = tst_stat({}, x)\n",
    "assert 'sum' in state\n",
    "test_eq(state['sum'], x)\n",
    "state = tst_stat(state, x)\n",
    "test_eq(state['sum'], 2*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def average_grad(self, p, state, mom, dampening=False, **kwargs):\n",
    "    if 'grad_avg' not in state: state['grad_avg'] = torch.zeros_like(p.grad.data)\n",
    "    damp = 1-mom if dampening else 1.\n",
    "    state['grad_avg'].mul_(mom).add_(damp, p.grad.data)\n",
    "average_grad.defaults = dict(mom=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def momentum_step(p, lr, grad_avg, **kwargs):\n",
    "    p.data.add_(-lr, grad_avg)\n",
    "    return p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
