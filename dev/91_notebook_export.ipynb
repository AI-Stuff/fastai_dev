{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# export\n",
    "from local.imports import *\n",
    "from local.notebook.core import *\n",
    "import nbformat,inspect\n",
    "from nbformat.sign import NotebookNotary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp notebook.export\n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting notebooks to modules\n",
    "\n",
    "> The functions that transform the dev notebooks in the fastai library\n",
    "\n",
    "- author: \"Sylvain Gugger\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's a notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A jupyter notebook is a json file behind the scenes. We can just read it with the json module, which will return a nested dictionary of dictionaries/lists of dictionaries, but there are some small differences between reading the json and using the tools from `nbformat` so we'll use this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_nb(fname):\n",
    "    \"Read the notebook in `fname`.\"\n",
    "    with open(Path(fname),'r') as f: return nbformat.reads(f.read(), as_version=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fname` can be a string or a pathlib object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nb = read_nb('91_notebook_export.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The root has four keys: `cells` contains the cells of the notebook, `metadata` some stuff around the version of python used to execute the notebook, `nbformat` and `nbformat_minor` the version of nbformat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cells', 'metadata', 'nbformat', 'nbformat_minor'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nb.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernelspec': {'display_name': 'Python 3',\n",
       "  'language': 'python',\n",
       "  'name': 'python3'},\n",
       " 'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "  'file_extension': '.py',\n",
       "  'mimetype': 'text/x-python',\n",
       "  'name': 'python',\n",
       "  'nbconvert_exporter': 'python',\n",
       "  'pygments_lexer': 'ipython3',\n",
       "  'version': '3.7.3'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nb['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.2'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{test_nb['nbformat']}.{test_nb['nbformat_minor']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells key then contains a list of cells. Each one is a new dictionary that contains entries like the type (code or markdown), the source (what is written in the cell) and the output (for code cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cell_type': 'code',\n",
       " 'execution_count': 1,\n",
       " 'metadata': {'hide_input': False},\n",
       " 'outputs': [],\n",
       " 'source': '# export\\nfrom local.imports import *\\nfrom local.notebook.core import *\\nimport nbformat,inspect\\nfrom nbformat.sign import NotebookNotary'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nb['cells'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def check_re(cell, pat, code_only=True):\n",
    "    \"Check if `cell` contains a line with regex `pat`\"\n",
    "    if code_only and cell['cell_type'] != 'code': return\n",
    "    if isinstance(pat, str): pat = re.compile(pat, re.IGNORECASE | re.MULTILINE)\n",
    "    return pat.search(cell['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pat` can be a string or a compiled regex, if `code_only=True`, ignores markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = test_nb['cells'][0].copy()\n",
    "assert check_re(cell, '# export') is not None\n",
    "assert check_re(cell, re.compile('# export')) is not None\n",
    "assert check_re(cell, '# bla') is None\n",
    "cell['cell_type'] = 'markdown'\n",
    "assert check_re(cell, '# export') is None\n",
    "assert check_re(cell, '# export', code_only=False) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "_re_blank_export = re.compile(r\"\"\"\n",
    "# Matches any line with #export or #exports without any module name:\n",
    "^         # beginning of line (since re.MULTILINE is passed)\n",
    "\\s*       # any number of whitespace\n",
    "\\#\\s*     # # then any number of whitespace\n",
    "exports?  # export or exports\n",
    "\\s*       # any number of whitespace\n",
    "$         # end of line (since re.MULTILINE is passed)\n",
    "\"\"\", re.IGNORECASE | re.MULTILINE | re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "_re_mod_export = re.compile(r\"\"\"\n",
    "# Matches any line with #export or #exports with a module name and catches it in group 1:\n",
    "^         # beginning of line (since re.MULTILINE is passed)\n",
    "\\s*       # any number of whitespace\n",
    "\\#\\s*     # # then any number of whitespace\n",
    "exports?  # export or exports\n",
    "\\s*       # any number of whitespace\n",
    "(\\S+)     # catch a group with any non-whitespace chars\n",
    "\\s*       # any number of whitespace\n",
    "$         # end of line (since re.MULTILINE is passed)\n",
    "\"\"\", re.IGNORECASE | re.MULTILINE | re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def is_export(cell, default):\n",
    "    \"Check if `cell` is to be exported and returns the name of the module.\"\n",
    "    if check_re(cell, _re_blank_export):\n",
    "        if default is None:\n",
    "            print(f\"This cell doesn't have an export destination and was ignored:\\n{cell['source'][1]}\")\n",
    "        return default\n",
    "    tst = check_re(cell, _re_mod_export)\n",
    "    return os.path.sep.join(tst.groups()[0].split('.')) if tst else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells to export are marked with an `#export` or `#exports` code, potentially with a module name where we want it exported. The default is given in a cell of the form `#default_exp bla` inside the notebook (usually at the top), though in this function, it needs the be passed (the final script will read the whole notebook to find it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = test_nb['cells'][0].copy()\n",
    "assert is_export(cell, 'export') == 'export'\n",
    "cell['source'] = \"# exports\" \n",
    "assert is_export(cell, 'export') == 'export'\n",
    "cell['source'] = \"# export mod\" \n",
    "assert is_export(cell, 'export') == 'mod'\n",
    "cell['source'] = \"# export mod.file\" \n",
    "assert is_export(cell, 'export') == 'mod/file'\n",
    "cell['source'] = \"# expt mod.file\"\n",
    "assert is_export(cell, 'export') is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "_re_default_exp = re.compile(r\"\"\"\n",
    "# Matches any line with #default_exp with a module name and catches it in group 1:\n",
    "^            # beginning of line (since re.MULTILINE is passed)\n",
    "\\s*          # any number of whitespace\n",
    "\\#\\s*        # # then any number of whitespace\n",
    "default_exp  # export or exports\n",
    "\\s*          # any number of whitespace\n",
    "(\\S+)        # catch a group with any non-whitespace chars\n",
    "\\s*          # any number of whitespace\n",
    "$            # end of line (since re.MULTILINE is passed)\n",
    "\"\"\", re.IGNORECASE | re.MULTILINE | re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def find_default_export(cells):\n",
    "    \"Find in `cells` the default export module.\"\n",
    "    for cell in cells:\n",
    "        tst = check_re(cell, _re_default_exp)\n",
    "        if tst: return tst.groups()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stops at the first cell containing a `#default_exp` code and return the value behind. Returns `None` if there are no cell with that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert find_default_export(test_nb['cells']) == 'notebook.export'\n",
    "assert find_default_export(test_nb['cells'][2:]) is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to export notebooks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _create_mod_file(fname, nb_path):\n",
    "    \"Create a module file for `fname`.\"\n",
    "    fname.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write(f\"#AUTOGENERATED! DO NOT EDIT! File to edit: dev/{nb_path.name} (unless otherwise specified).\")\n",
    "        f.write('\\n\\n__all__ = []')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_patch_func = re.compile(r\"\"\"\n",
    "# Catches any function decorated with @patch, its name in group 1 and the patched class in group 2\n",
    "@patch       # At any place in the cell, something that begins with @patch\n",
    "\\s*def       # Any number of whitespace (including a new line probably) followed by def\n",
    "\\s+          # One whitespace or more\n",
    "([^\\(\\s]*)   # Catch a group composed of anything but whitespace or an opening parenthesis (name of the function)\n",
    "\\s*\\(        # Any number of whitespace followed by an opening parenthesis\n",
    "[^:]*        # Any number of character different of : (the name of the first arg that is type-annotated)\n",
    ":\\s*         # A column followed by any number of whitespace\n",
    "([^,\\)\\s]*)  # Catch a group composed of anything but a comma, a closing parenthesis or whitespace (name of the class)\n",
    "\\s*          # Any number of whitespace\n",
    "(?:,|\\))     # Non-catching group with either a comma or a closing parenthesis\n",
    "\"\"\", re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "tst = _re_patch_func.search(\"\"\"\n",
    "@patch\n",
    "def func(obj:Class)\"\"\")\n",
    "assert tst.groups() == (\"func\", \"Class\")\n",
    "tst = _re_patch_func.search(\"\"\"\n",
    "@patch\n",
    "def func (obj:Class, a)\"\"\")\n",
    "assert tst.groups() == (\"func\", \"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_class_func_def = re.compile(r\"\"\"\n",
    "# Catches any 0-indented function or class definition with its name in group 1\n",
    "^              # Beginning of a line (since re.MULTILINE is passed)\n",
    "(?:def|class)  # Non-catching group for def or class\n",
    "\\s+            # One whitespace or more\n",
    "([^\\(\\s]*)     # Catching group with any character except an opening parenthesis or a whitespace (name)\n",
    "\\s*            # Any number of whitespace\n",
    "(?:\\(|:)       # Non-catching group with either an opening parenthesis or a : (classes don't need ())\n",
    "\"\"\", re.MULTILINE | re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert _re_class_func_def.search(\"class Class:\").groups() == ('Class',)\n",
    "assert _re_class_func_def.search(\"def func(a, b):\").groups() == ('func',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_obj_def = re.compile(r\"\"\"\n",
    "# Catches any 0-indented object definition (bla = thing) with its name in group 1\n",
    "^          # Beginning of a line (since re.MULTILINE is passed)\n",
    "([^=\\s]*)  # Catching group with any character except a whitespace or an equal sign\n",
    "\\s*=       # Any number of whitespace followed by an =\n",
    "\"\"\", re.MULTILINE | re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert _re_obj_def.search(\"a = 1\").groups() == ('a',)\n",
    "assert _re_obj_def.search(\"a=1\").groups() == ('a',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _not_private(n):\n",
    "    for t in n.split('.'):\n",
    "        if t.startswith('_'): return False\n",
    "    return '\\\\' not in t and '^' not in t and '[' not in t\n",
    "\n",
    "def export_names(code, func_only=False):\n",
    "    \"Find the names of the objects, functions or classes defined in `code` that are exported.\"\n",
    "    #Format monkey-patches with @patch\n",
    "    code = _re_patch_func.sub(r'def \\2.\\1() = ', code)\n",
    "    names = _re_class_func_def.findall(code)\n",
    "    if not func_only: names += _re_obj_def.findall(code)\n",
    "    return [n for n in names if _not_private(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function only picks the zero-indented objects, functions or classes (we don't want the class methods for instance) and excludes private names (that begin with `_`). It only returns func and class names when `func_only=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert export_names(\"def my_func(x):\\n  pass\\nclass MyClass():\") == [\"my_func\", \"MyClass\"]\n",
    "#Indented funcs are ignored (funcs inside a class)\n",
    "assert export_names(\"  def my_func(x):\\n  pass\\nclass MyClass():\") == [\"MyClass\"]\n",
    "#Private funcs are ignored\n",
    "assert export_names(\"def _my_func():\\n  pass\\nclass MyClass():\") == [\"MyClass\"]\n",
    "#trailing spaces\n",
    "assert export_names(\"def my_func ():\\n  pass\\nclass MyClass():\") == [\"my_func\", \"MyClass\"]\n",
    "#class without parenthesis\n",
    "assert export_names(\"def my_func ():\\n  pass\\nclass MyClass:\") == [\"my_func\", \"MyClass\"]\n",
    "#object and funcs\n",
    "assert export_names(\"def my_func ():\\n  pass\\ndefault_bla=[]:\") == [\"my_func\", \"default_bla\"]\n",
    "assert export_names(\"def my_func ():\\n  pass\\ndefault_bla=[]:\", func_only=True) == [\"my_func\"]\n",
    "#Private objects are ignored\n",
    "assert export_names(\"def my_func ():\\n  pass\\n_default_bla = []:\") == [\"my_func\"]\n",
    "#Objects with dots are privates if one part is private\n",
    "assert export_names(\"def my_func ():\\n  pass\\ndefault.bla = []:\") == [\"my_func\", \"default.bla\"]\n",
    "assert export_names(\"def my_func ():\\n  pass\\ndefault._bla = []:\") == [\"my_func\"]\n",
    "#Monkey-path with @patch are properly renamed\n",
    "assert export_names(\"@patch\\ndef my_func(x:Class):\\n  pass\") == [\"Class.my_func\"]\n",
    "assert export_names(\"@patch\\ndef my_func(x:Class):\\n  pass\", func_only=True) == [\"Class.my_func\"]\n",
    "assert export_names(\"some code\\n@patch\\ndef my_func(x:Class, y):\\n  pass\") == [\"Class.my_func\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_all_def   = re.compile(r\"\"\"\n",
    "# Catches a cell with defines \\_all\\_ = [\\*\\*] and get that \\*\\* in group 1\n",
    "^_all_   #  Beginning of line (since re.MULTILINE is passed)\n",
    "\\s*=\\s*  #  Any number of whitespace, =, any number of whitespace\n",
    "\\[       #  Opening [\n",
    "([^\\n\\]]*) #  Catching group with anything except a ] or newline\n",
    "\\]       #  Closing ]\n",
    "\"\"\", re.MULTILINE | re.VERBOSE)\n",
    "\n",
    "#Same with __all__\n",
    "_re__all__def = re.compile(r'^__all__\\s*=\\s*\\[([^\\]]*)\\]', re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def extra_add(code):\n",
    "    \"Catch adds to `__all__` required by a cell with `_all_=`\"\n",
    "    if _re_all_def.search(code):\n",
    "        names = _re_all_def.search(code).groups()[0]\n",
    "        names = re.sub('\\s*,\\s*', ',', names)\n",
    "        names = names.replace('\"', \"'\")\n",
    "        code = _re_all_def.sub('', code)\n",
    "        code = re.sub(r'([^\\n]|^)\\n*$', r'\\1', code)\n",
    "        return names.split(','),code\n",
    "    return [],code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert extra_add('_all_ = [\"func\", \"func1\", \"func2\"]') == ([\"'func'\", \"'func1'\", \"'func2'\"],'')\n",
    "assert extra_add('_all_ = [\"func\",   \"func1\" , \"func2\"]') ==  ([\"'func'\", \"'func1'\", \"'func2'\"],'')\n",
    "assert extra_add(\"_all_ = ['func','func1', 'func2']\\n\") ==  ([\"'func'\", \"'func1'\", \"'func2'\"],'')\n",
    "assert extra_add('code\\n\\n_all_ = [\"func\", \"func1\", \"func2\"]') == ([\"'func'\", \"'func1'\", \"'func2'\"],'code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _add2add(fname, names, line_width=120):\n",
    "    if len(names) == 0: return\n",
    "    with open(fname, 'r') as f: text = f.read()\n",
    "    tw = TextWrapper(width=120, initial_indent='', subsequent_indent=' '*11, break_long_words=False)\n",
    "    re_all = _re__all__def.search(text)\n",
    "    start,end = re_all.start(),re_all.end()\n",
    "    text_all = tw.wrap(f\"{text[start:end-1]}{'' if text[end-2]=='[' else ', '}{', '.join(names)}]\")\n",
    "    with open(fname, 'w') as f: f.write(text[:start] + '\\n'.join(text_all) + text[end:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'test_add.txt'\n",
    "with open(fname, 'w') as f: f.write(\"Bla\\n__all__ = [my_file, MyClas]\\nBli\")\n",
    "_add2add(fname, ['new_function'])\n",
    "with open(fname, 'r') as f: \n",
    "    assert f.read() == \"Bla\\n__all__ = [my_file, MyClas, new_function]\\nBli\"\n",
    "_add2add(fname, [f'new_function{i}' for i in range(10)])\n",
    "with open(fname, 'r') as f: \n",
    "    assert f.read() == \"\"\"Bla\n",
    "__all__ = [my_file, MyClas, new_function, new_function0, new_function1, new_function2, new_function3, new_function4,\n",
    "           new_function5, new_function6, new_function7, new_function8, new_function9]\n",
    "Bli\"\"\"\n",
    "os.remove(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _relative_import(name, fname):\n",
    "    mods = name.split('.')\n",
    "    splits = str(fname).split(os.path.sep)\n",
    "    if mods[0] not in splits: return name\n",
    "    splits = splits[splits.index(mods[0]):]\n",
    "    while splits[0] == mods[0]: splits,mods = splits[1:],mods[1:]\n",
    "    return '.' * (len(splits)) + '.'.join(mods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert _relative_import('local.core', Path('local')/'data.py') == '.core'\n",
    "assert _relative_import('local.core', Path('local')/'vision'/'data.py') == '..core'\n",
    "assert _relative_import('local.vision.transform', Path('local')/'vision'/'data.py') == '.transform'\n",
    "assert _relative_import('local.notebook.core', Path('local')/'data'/'external.py') == '..notebook.core'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#Catches any from local.bla import something and catches local.bla in group 1, the imported thing(s) in group 2.\n",
    "_re_import = re.compile(r'^\\s*from (local.\\S*) import (.*)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _deal_import(code_lines, fname):\n",
    "    pat = re.compile(r'from (local.\\S*) import (\\S*)$')\n",
    "    lines = []\n",
    "    def _replace(m):\n",
    "        mod,obj = m.groups()\n",
    "        return f\"from {_relative_import(mod, fname)} import {obj}\"\n",
    "    for line in code_lines:\n",
    "        line = re.sub('_'+'file_', '__'+'file__', line) #Need to break _file_ or that line will be treated\n",
    "        lines.append(_re_import.sub(_replace,line))\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "lines = [\"from local.core import *\", \"nothing to see\", \"from local.vision import bla1, bla2\"]\n",
    "assert _deal_import(lines, Path('local')/'data.py') == [\n",
    "    \"from .core import *\", \"nothing to see\", \"from .vision import bla1, bla2\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#Tricking jupyter notebook to have a __file__ attribute. All _file_ will be replaced by __file__\n",
    "_file_ = Path('local').absolute()/'notebook'/'export.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_index():\n",
    "    if not (Path(_file_).parent/'index.txt').exists(): return {}\n",
    "    return json.load(open(Path(_file_).parent/'index.txt', 'r'))\n",
    "\n",
    "def _save_index(index): \n",
    "    fname = Path(_file_).parent/'index.txt'\n",
    "    fname.parent.mkdir(parents=True, exist_ok=True)\n",
    "    json.dump(index, open(fname, 'w'), indent=2)\n",
    "    \n",
    "def _reset_index():\n",
    "    if (Path(_file_).parent/'index.txt').exists():\n",
    "        os.remove(Path(_file_).parent/'index.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "ind,ind_bak = Path(_file_).parent/'index.txt',Path(_file_).parent/'index.bak'\n",
    "if ind.exists(): shutil.move(ind, ind_bak)\n",
    "assert _get_index() == {}\n",
    "_save_index({'foo':'bar'})\n",
    "assert _get_index() == {'foo':'bar'}\n",
    "if ind_bak.exists(): shutil.move(ind_bak, ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_pkl(fname, cell):\n",
    "    dic = pickle.load(open((Path.cwd()/'lib.pkl'), 'rb')) if (Path.cwd()/'lib.pkl').exists() else collections.defaultdict(list)\n",
    "    dic[fname].append(cell)\n",
    "    pickle.dump(dic, open((Path.cwd()/'lib.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def _notebook2script(fname, silent=False, to_pkl=False):\n",
    "    \"Finds cells starting with `#export` and puts them into a new module\"\n",
    "    if os.environ.get('IN_TEST',0): return  # don't export if running tests\n",
    "    fname = Path(fname)\n",
    "    nb = read_nb(fname)\n",
    "    default = find_default_export(nb['cells'])\n",
    "    if default is not None:\n",
    "        default = os.path.sep.join(default.split('.'))\n",
    "        if not to_pkl: _create_mod_file(Path.cwd()/'local'/f'{default}.py', fname)\n",
    "    index = _get_index()\n",
    "    exports = [is_export(c, default) for c in nb['cells']]\n",
    "    cells = [(i,c,e) for i,(c,e) in enumerate(zip(nb['cells'],exports)) if e is not None]\n",
    "    for (i,c,e) in cells:\n",
    "        fname_out = Path.cwd()/'local'/f'{e}.py'\n",
    "        orig = ('#C' if e==default else f'#Comes from {fname.name}, c') + 'ell\\n'\n",
    "        code = '\\n\\n' + orig + '\\n'.join(_deal_import(c['source'].split('\\n')[1:], fname_out))\n",
    "        # remove trailing spaces\n",
    "        names = export_names(code)\n",
    "        extra,code = extra_add(code)\n",
    "        if not to_pkl: _add2add(fname_out, [f\"'{f}'\" for f in names if '.' not in f] + extra)\n",
    "        index.update({f: fname.name for f in names})\n",
    "        code = re.sub(r' +$', '', code, flags=re.MULTILINE)\n",
    "        if code != '\\n\\n' + orig[:-1]:\n",
    "            if to_pkl: _update_pkl(fname_out, (i, fname, code))\n",
    "            else:\n",
    "                with open(fname_out, 'a') as f: f.write(code)\n",
    "    _save_index(index)\n",
    "    if not silent: print(f\"Converted {fname}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 92_notebook_showdoc.ipynb.\n"
     ]
    }
   ],
   "source": [
    "_notebook2script('92_notebook_showdoc.ipynb', to_pkl=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def _get_sorted_files(all_fs: Union[bool,str], up_to=None):\n",
    "    \"Return the list of files corresponding to `g` in the current dir.\"\n",
    "    if (all_fs==True): ret = glob.glob('*.ipynb') # Checks both that is bool type and that is True\n",
    "    else: ret = glob.glob(all_fs) if isinstance(g,str) else []\n",
    "    if len(ret)==0: print('WARNING: No files found')\n",
    "    ret = [f for f in ret if not f.startswith('_')]\n",
    "    if up_to is not None: ret = [f for f in ret if str(f)<=str(up_to)]\n",
    "    return sorted(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def notebook2script(fname=None, all_fs=None, up_to=None, silent=False, to_pkl=False):\n",
    "    \"Convert `fname` or all the notebook satisfying `all_fs`.\"\n",
    "    # initial checks\n",
    "    if os.environ.get('IN_TEST',0): return  # don't export if running tests\n",
    "    assert fname or all_fs\n",
    "    if all_fs: _reset_index()\n",
    "    if (all_fs is None) and (up_to is not None): all_fs=True # Enable allFiles if upTo is present\n",
    "    fnames = _get_sorted_files(all_fs, up_to=up_to) if all_fs else [fname]\n",
    "    [_notebook2script(f, silent=silent, to_pkl=to_pkl) for f in fnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finds cells starting with `#export` and puts them into the appropriate module.\n",
    "* `fname`: the filename of one notebook to convert\n",
    "* `all_fs`: `True` if you want to convert all notebook files in the folder or a glob expression\n",
    "* `up_to`: converts all notebooks respecting the previous arg up to a certain number\n",
    "\n",
    "Examples of use in console:\n",
    "```\n",
    "notebook2script                                 # Parse all files\n",
    "notebook2script --fname 00_export.ipynb         # Parse 00_export.ipynb\n",
    "notebook2script --all_fs=nb*                    # Parse all files starting with nb*\n",
    "notebook2script --up_to=10                      # Parse all files with (name<='10')\n",
    "notebook2script --all_fs=*_*.ipynb --up_to=10   # Parse all files with an '_' and (name<='10')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the way back to notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_name(obj):\n",
    "    \"Get the name of `obj`\"\n",
    "    if hasattr(obj, '__name__'):       return obj.__name__\n",
    "    elif getattr(obj, '_name', False): return obj._name\n",
    "    elif hasattr(obj,'__origin__'):    return str(obj.__origin__).split('.')[-1] #for types\n",
    "    else:                              return str(obj).split('.')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_name(in_ipython) == 'in_ipython'\n",
    "assert get_name(DocsTestClass.test) == 'test'\n",
    "# assert get_name(Union[Tensor, float]) == 'Union'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def qual_name(obj):\n",
    "    \"Get the qualified name of `obj`\"\n",
    "    if hasattr(obj,'__qualname__'): return obj.__qualname__\n",
    "    if inspect.ismethod(obj):       return f\"{get_name(obj.__self__)}.{get_name(fn)}\"\n",
    "    return get_name(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert qual_name(DocsTestClass) == 'DocsTestClass'\n",
    "assert qual_name(DocsTestClass.test) == 'DocsTestClass.test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def source_nb(func, is_name=None, return_all=False):\n",
    "    \"Return the name of the notebook where `func` was defined\"\n",
    "    is_name = is_name or isinstance(func, str)\n",
    "    index = _get_index()\n",
    "    name = func if is_name else qual_name(func)\n",
    "    while len(name) > 0:\n",
    "        if name in index: return (name,index[name]) if return_all else index[name]\n",
    "        name = '.'.join(name.split('.')[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either pass an object or its name (by default `is_name` will look if `func` is a string or not, but you can override if there is some inconsistent behavior). \n",
    "\n",
    "If passed a method of a class, the function will return the notebook in which the largest part of the function was defined in case there is a monkey-matching that defines `class.method` in a different notebook than `class`. If `return_all=True`, the function will return a tuple with the name by which the function was found and the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.data.pipeline import Transform\n",
    "from local.test import test_fail\n",
    "\n",
    "assert source_nb(test_fail) == '00_test.ipynb'\n",
    "assert source_nb(Transform) == '02_data_transforms.ipynb'\n",
    "assert source_nb(Transform.decode) == '02_data_transforms.ipynb'\n",
    "#opt_call is in the core module but defined in 02\n",
    "# from local.core import opt_call\n",
    "# assert source_nb(opt_call) == '02_data_pipeline.ipynb' # TODO: find something else\n",
    "assert source_nb(int) is None\n",
    "#Added through a monkey-patch\n",
    "assert source_nb('Path.ls') == '01_core.ipynb'\n",
    "\n",
    "#Test with name\n",
    "assert source_nb('DocsTestClass') == '90_notebook_core.ipynb'\n",
    "assert source_nb('DocsTestClass.test') == '90_notebook_core.ipynb'\n",
    "\n",
    "#Test return_all\n",
    "assert source_nb(DocsTestClass, return_all=True) == ('DocsTestClass','90_notebook_core.ipynb')\n",
    "assert source_nb(DocsTestClass.test, return_all=True) == ('DocsTestClass','90_notebook_core.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If someone decides to change a module instead of the notebooks, the following functions help update the notebooks accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "_re_default_nb = re.compile(r'File to edit: dev/(\\S+)\\s+')\n",
    "_re_cell = re.compile(r'^#Cell|^#Comes from\\s+(\\S+), cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path.cwd()/'local'/'core.py') as f: code = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('01_core.ipynb',\n",
       "  'from .test import *\\nfrom .imports import *\\nfrom .notebook.showdoc import show_doc'),\n",
       " ('01_core.ipynb', 'defaults = SimpleNamespace()'),\n",
       " ('01_core.ipynb',\n",
       "  'class PrePostInitMeta(type):\\n    \"A metaclass that calls optional `__pre_init__` and `__post_init__` methods\"\\n    def __new__(cls, name, bases, dct):\\n        x = super().__new__(cls, name, bases, dct)\\n        def _pass(self, *args,**kwargs): pass\\n        for o in (\\'__init__\\', \\'__pre_init__\\', \\'__post_init__\\'):\\n            if not hasattr(x,o): setattr(x,o,_pass)\\n        old_init = x.__init__\\n\\n        @functools.wraps(old_init)\\n        def _init(self,*args,**kwargs):\\n            self.__pre_init__()\\n            old_init(self, *args,**kwargs)\\n            self.__post_init__()\\n        setattr(x, \\'__init__\\', _init)\\n        return x'),\n",
       " ('01_core.ipynb',\n",
       "  'class BaseObj(metaclass=PrePostInitMeta):\\n    \"Base class that provides `PrePostInitMeta` metaclass to subclasses\"\\n    pass'),\n",
       " ('01_core.ipynb',\n",
       "  'class NewChkMeta(PrePostInitMeta):\\n    \"Metaclass to avoid recreating object passed to constructor (plus all `PrePostInitMeta` functionality)\"\\n    def __new__(cls, name, bases, dct):\\n        x = super().__new__(cls, name, bases, dct)\\n        old_init,old_new = x.__init__,x.__new__\\n\\n        @functools.wraps(old_init)\\n        def _new(cls, x=None, *args, **kwargs):\\n            if x is not None and isinstance(x,cls):\\n                x._newchk = 1\\n                return x\\n            res = old_new(cls)\\n            res._newchk = 0\\n            return res\\n\\n        @functools.wraps(old_init)\\n        def _init(self,*args,**kwargs):\\n            if self._newchk: return\\n            old_init(self, *args, **kwargs)\\n\\n        x.__init__,x.__new__ = _init,_new\\n        return x'),\n",
       " ('01_core.ipynb',\n",
       "  'class BypassNewMeta(type):\\n    \"Metaclass: casts `x` to this class, initializing with `_new_meta` if available\"\\n    def __call__(cls, x, *args, **kwargs):\\n        if hasattr(cls, \\'_new_meta\\'): x = cls._new_meta(x, *args, **kwargs)\\n        if cls!=x.__class__: x.__class__ = cls\\n        return x'),\n",
       " ('01_core.ipynb',\n",
       "  'def patch_to(cls, as_prop=False):\\n    \"Decorator: add `f` to `cls`\"\\n    def _inner(f):\\n        nf = copy(f)\\n        # `functools.update_wrapper` when passing patched function to `Pipeline`, so we do it manually\\n        for o in functools.WRAPPER_ASSIGNMENTS: setattr(nf, o, getattr(f,o))\\n        nf.__qualname__ = f\"{cls.__name__}.{f.__name__}\"\\n        setattr(cls, f.__name__, property(nf) if as_prop else nf)\\n        return f\\n    return _inner'),\n",
       " ('01_core.ipynb',\n",
       "  'def patch(f):\\n    \"Decorator: add `f` to the first parameter\\'s class (based on f\\'s type annotations)\"\\n    cls = next(iter(f.__annotations__.values()))\\n    return patch_to(cls)(f)'),\n",
       " ('01_core.ipynb',\n",
       "  'def patch_property(f):\\n    \"Decorator: add `f` as a property to the first parameter\\'s class (based on f\\'s type annotations)\"\\n    cls = next(iter(f.__annotations__.values()))\\n    return patch_to(cls, as_prop=True)(f)'),\n",
       " ('01_core.ipynb',\n",
       "  'def _mk_param(n,d=None): return inspect.Parameter(n, inspect.Parameter.KEYWORD_ONLY, default=d)'),\n",
       " ('01_core.ipynb',\n",
       "  'def use_kwargs(names, keep=False):\\n    \"Decorator: replace `**kwargs` in signature with `names` params\"\\n    def _f(f):\\n        sig = inspect.signature(f)\\n        sigd = dict(sig.parameters)\\n        k = sigd.pop(\\'kwargs\\')\\n        s2 = {n:_mk_param(n) for n in names if n not in sigd}\\n        sigd.update(s2)\\n        if keep: sigd[\\'kwargs\\'] = k\\n        f.__signature__ = sig.replace(parameters=sigd.values())\\n        return f\\n    return _f'),\n",
       " ('01_core.ipynb',\n",
       "  'def delegates(to=None, keep=False):\\n    \"Decorator: replace `**kwargs` in signature with params from `to`\"\\n    def _f(f):\\n        if to is None: to_f,from_f = f.__base__.__init__,f.__init__\\n        else:          to_f,from_f = to,f\\n        sig = inspect.signature(from_f)\\n        sigd = dict(sig.parameters)\\n        k = sigd.pop(\\'kwargs\\')\\n        s2 = {k:v for k,v in inspect.signature(to_f).parameters.items()\\n              if v.default != inspect.Parameter.empty and k not in sigd}\\n        sigd.update(s2)\\n        if keep: sigd[\\'kwargs\\'] = k\\n        from_f.__signature__ = sig.replace(parameters=sigd.values())\\n        return f\\n    return _f'),\n",
       " ('01_core.ipynb',\n",
       "  'def funcs_kwargs(cls):\\n    \"Replace methods in `self._methods` with those from `kwargs`\"\\n    old_init = cls.__init__\\n    def _init(self, *args, **kwargs):\\n        for k in cls._methods:\\n            arg = kwargs.pop(k,None)\\n            if arg is not None:\\n                if isinstance(arg,types.MethodType): arg = types.MethodType(arg.__func__, self)\\n                setattr(self, k, arg)\\n        old_init(self, *args, **kwargs)\\n    functools.update_wrapper(_init, old_init)\\n    cls.__init__ = use_kwargs(cls._methods)(_init)\\n    return cls'),\n",
       " ('01_core.ipynb',\n",
       "  'def method(f):\\n    \"Mark `f` as a method\"\\n    # `1` is a dummy instance since Py3 doesn\\'t allow `None` any more\\n    return types.MethodType(f, 1)'),\n",
       " ('01_core.ipynb',\n",
       "  \"#NB: Please don't move this to a different line or module, since it's used in testing `get_source_link`\\ndef chk(f): return typechecked(always=True)(f)\"),\n",
       " ('01_core.ipynb',\n",
       "  'def add_docs(cls, cls_doc=None, **docs):\\n    \"Copy values from `docs` to `cls` docstrings, and confirm all public methods are documented\"\\n    if cls_doc is not None: cls.__doc__ = cls_doc\\n    for k,v in docs.items():\\n        f = getattr(cls,k)\\n        if hasattr(f,\\'__func__\\'): f = f.__func__ # required for class methods\\n        f.__doc__ = v\\n    # List of public callables without docstring\\n    nodoc = [c for n,c in vars(cls).items() if isinstance(c,Callable)\\n             and not n.startswith(\\'_\\') and c.__doc__ is None]\\n    assert not nodoc, f\"Missing docs: {nodoc}\"\\n    assert cls.__doc__ is not None, f\"Missing class docs: {cls}\"'),\n",
       " ('01_core.ipynb',\n",
       "  'def docs(cls):\\n    \"Decorator version of `add_docs`, using `_docs` dict\"\\n    add_docs(cls, **cls._docs)\\n    return cls'),\n",
       " ('01_core.ipynb',\n",
       "  'def custom_dir(c, add:List):\\n    \"Implement custom `__dir__`, adding `add` to `cls`\"\\n    return dir(type(c)) + list(c.__dict__.keys()) + add'),\n",
       " ('01_core.ipynb',\n",
       "  'class GetAttr(BaseObj):\\n    \"Inherit from this to have all attr accesses in `self._xtra` passed down to `self.default`\"\\n    @property\\n    def _xtra(self): return [o for o in dir(self.default) if not o.startswith(\\'_\\')]\\n    def __getattr__(self,k):\\n        if k not in (\\'_xtra\\',\\'default\\') and k in self._xtra: return getattr(self.default, k)\\n        raise AttributeError(k)\\n    def __dir__(self): return custom_dir(self, self._xtra)\\n    def __setstate__(self,data): self.__dict__.update(data)'),\n",
       " ('01_core.ipynb',\n",
       "  'def delegate_attr(self, k, to):\\n    \"Use in `__getattr__` to delegate to attr `to` without inheriting from `GetAttr`\"\\n    if k.startswith(\\'_\\') or k==to: raise AttributeError(k)\\n    try: return getattr(getattr(self,to), k)\\n    except AttributeError: raise AttributeError(k) from None'),\n",
       " ('01_core.ipynb',\n",
       "  'def coll_repr(c, max_n=10):\\n    \"String repr of up to `max_n` items of (possibly lazy) collection `c`\"\\n    return f\\'(#{len(c)}) [\\' + \\',\\'.join(itertools.islice(map(str,c), max_n)) + (\\n        \\'...\\' if len(c)>10 else \\'\\') + \\']\\''),\n",
       " ('01_core.ipynb',\n",
       "  'def mask2idxs(mask):\\n    \"Convert bool mask or index list to index `L`\"\\n    mask = list(mask)\\n    if len(mask)==0: return []\\n    if isinstance(mask[0],bool): return [i for i,m in enumerate(mask) if m]\\n    return [int(i) for i in mask]'),\n",
       " ('01_core.ipynb',\n",
       "  \"def _listify(o):\\n    if o is None: return []\\n    if isinstance(o, list): return o\\n    if isinstance(o, str) or hasattr(o,'__array__'): return [o]\\n    if is_iter(o): return list(o)\\n    return [o]\"),\n",
       " ('01_core.ipynb',\n",
       "  'class CollBase:\\n    \"Base class for composing a list of `items`\"\\n    _xtra =  [o for o in dir([]) if not o.startswith(\\'_\\')]\\n\\n    def __init__(self, items): self.items = items\\n    def __len__(self): return len(self.items)\\n    def __getitem__(self, k): return self.items[k]\\n    def __setitem__(self, k, v): self.items[list(k) if isinstance(k,CollBase) else k] = v\\n    def __delitem__(self, i): del(self.items[i])\\n    def __repr__(self): return self.items.__repr__()\\n    def __iter__(self): return self.items.__iter__()\\n    def _new(self, items, *args, **kwargs): return self.__class__(items, *args, **kwargs)'),\n",
       " ('01_core.ipynb',\n",
       "  'def cycle(o):\\n    \"Like `itertools.cycle` except creates list of `None`s if `o` is empty\"\\n    return itertools.cycle(o) if o is not None and len(o) > 0 else itertools.cycle([None])'),\n",
       " ('01_core.ipynb',\n",
       "  'def zip_cycle(x, *args):\\n    \"Like `itertools.zip_longest` but `cycle`s through elements of all but first argument\"\\n    return zip(x, *map(cycle,args))'),\n",
       " ('01_core.ipynb',\n",
       "  'class L(CollBase, GetAttr, metaclass=NewChkMeta):\\n    \"Behaves like a list of `items` but can also index with list of indices or masks\"\\n    def __init__(self, items=None, *rest, use_list=False, match=None):\\n        if rest: items = (items,)+rest\\n        if items is None: items = []\\n        if (use_list is not None) or not hasattr(items,\\'__array__\\'):\\n            items = list(items) if use_list else _listify(items)\\n        if match is not None:\\n            if len(items)==1: items = items*len(match)\\n            else: assert len(items)==len(match), \\'Match length mismatch\\'\\n        super().__init__(items)\\n\\n    def __getitem__(self, idx): return L(self._gets(idx), use_list=None) if is_iter(idx) else self._get(idx)\\n    def _get(self, i): return getattr(self.items,\\'iloc\\',self.items)[i]\\n    def _gets(self, i):\\n        i = mask2idxs(i)\\n        return (self.items.iloc[list(i)] if hasattr(self.items,\\'iloc\\')\\n                else self.items.__array__()[(i,)] if hasattr(self.items,\\'__array__\\')\\n                else [self.items[i_] for i_ in i])\\n\\n    def __setitem__(self, idx, o):\\n        \"Set `idx` (can be list of indices, or mask, or int) items to `o` (which is broadcast if not iterable)\"\\n        idx = idx if isinstance(idx,L) else _listify(idx)\\n        if not is_iter(o): o = [o]*len(idx)\\n        for i,o_ in zip(idx,o): self.items[i] = o_\\n\\n    @property\\n    def default(self): return self.items\\n    def __iter__(self): return (self[i] for i in range(len(self)))\\n    def __repr__(self): return coll_repr(self)\\n    def __eq__(self,b): return all_equal(b,self)\\n    def __invert__(self): return self._new(not i for i in self)\\n    def __mul__ (a,b): return a._new(a.items*b)\\n    def __add__ (a,b): return a._new(a.items+_listify(b))\\n    def __radd__(a,b): return a._new(b)+a\\n    def __addi__(a,b):\\n        a.items += list(b)\\n        return a\\n\\n    def sorted(self, key=None, reverse=False):\\n        \"New `L` sorted by `key`. If key is str then use `attrgetter`. If key is int then use `itemgetter`.\"\\n        if isinstance(key,str):   k=lambda o:getattr(o,key,0)\\n        elif isinstance(key,int): k=itemgetter(key)\\n        else: k=key\\n        return self._new(sorted(self.items, key=k, reverse=reverse))\\n\\n    @classmethod\\n    def range(cls, a, b=None, step=None):\\n        \"Same as builtin `range`, but returns an `L`. Can pass a collection for `a`, to use `len(a)`\"\\n        if is_coll(a): a = len(a)\\n        return cls(range(a,b,step) if step is not None else range(a,b) if b is not None else range(a))\\n\\n    def unique(self): return L(dict.fromkeys(self).keys())\\n    def val2idx(self): return {v:k for k,v in enumerate(self)}\\n    def itemgot(self, idx): return self.mapped(itemgetter(idx))\\n    def attrgot(self, k, default=None): return self.mapped(lambda o:getattr(o,k,default))\\n    def cycle(self): return cycle(self)\\n    def filtered(self, f, *args, **kwargs): return self._new(filter(partial(f,*args,**kwargs), self))\\n    def mapped(self, f, *args, **kwargs): return self._new(map(partial(f,*args,**kwargs), self))\\n    def mapped_dict(self, f, *args, **kwargs): return {k:f(k, *args,**kwargs) for k in self}\\n    def starmapped(self, f, *args, **kwargs): return self._new(itertools.starmap(partial(f,*args,**kwargs), self))\\n    def zipped(self, cycled=False): return self._new((zip_cycle if cycled else zip)(*self))\\n    def zipwith(self, *rest, cycled=False): return self._new([self, *rest]).zipped(cycled=cycled)\\n    def mapped_zip(self, f, cycled=False): return self.zipped(cycled=cycled).starmapped(f)\\n    def mapped_zipwith(self, f, *rest, cycled=False): return self.zipwith(*rest, cycled=cycled).starmapped(f)\\n    def shuffled(self):\\n        it = copy(self.items)\\n        random.shuffle(it)\\n        return self._new(it)'),\n",
       " ('01_core.ipynb',\n",
       "  'add_docs(L,\\n         __getitem__=\"Retrieve `idx` (can be list of indices, or mask, or int) items\",\\n         unique=\"Unique items, in stable order\",\\n         val2idx=\"Dict from value to index\",\\n         filtered=\"Create new `L` filtered by predicate `f`, passing `args` and `kwargs` to `f`\",\\n         mapped=\"Create new `L` with `f` applied to all `items`, passing `args` and `kwargs` to `f`\",\\n         mapped_dict=\"Like `mapped`, but creates a dict from `items` to function results\",\\n         starmapped=\"Like `mapped`, but use `itertools.starmap`\",\\n         itemgot=\"Create new `L` with item `idx` of all `items`\",\\n         attrgot=\"Create new `L` with attr `k` of all `items`\",\\n         cycle=\"Same as `itertools.cycle`\",\\n         zipped=\"Create new `L` with `zip(*items)`\",\\n         zipwith=\"Create new `L` with `self` zipped with each of `*rest`\",\\n         mapped_zip=\"Combine `zipped` and `starmapped`\",\\n         mapped_zipwith=\"Combine `zipwith` and `starmapped`\",\\n         shuffled=\"Same as `random.shuffle`, but not inplace\")'),\n",
       " ('01_core.ipynb',\n",
       "  'def ifnone(a, b):\\n    \"`b` if `a` is None else `a`\"\\n    return b if a is None else a'),\n",
       " ('01_core.ipynb',\n",
       "  'def get_class(nm, *fld_names, sup=None, doc=None, funcs=None, **flds):\\n    \"Dynamically create a class, optionally inheriting from `sup`, containing `fld_names`\"\\n    attrs = {}\\n    for f in fld_names: attrs[f] = None\\n    for f in L(funcs): attrs[f.__name__] = f\\n    for k,v in flds.items(): attrs[k] = v\\n    sup = ifnone(sup, ())\\n    if not isinstance(sup, tuple): sup=(sup,)\\n\\n    def _init(self, *args, **kwargs):\\n        for i,v in enumerate(args): setattr(self, list(attrs.keys())[i], v)\\n        for k,v in kwargs.items(): setattr(self,k,v)\\n\\n    def _repr(self):\\n        return \\'\\\\n\\'.join(f\\'{o}: {getattr(self,o)}\\' for o in set(dir(self))\\n                         if not o.startswith(\\'_\\') and not isinstance(getattr(self,o), types.MethodType))\\n\\n    if not sup: attrs[\\'__repr__\\'] = _repr\\n    attrs[\\'__init__\\'] = _init\\n    res = type(nm, sup, attrs)\\n    if doc is not None: res.__doc__ = doc\\n    return res'),\n",
       " ('01_core.ipynb',\n",
       "  'def mk_class(nm, *fld_names, sup=None, doc=None, funcs=None, mod=None, **flds):\\n    \"Create a class using `get_class` and add to the caller\\'s module\"\\n    if mod is None: mod = inspect.currentframe().f_back.f_locals\\n    res = get_class(nm, *fld_names, sup=sup, doc=doc, funcs=funcs, **flds)\\n    mod[nm] = res'),\n",
       " ('01_core.ipynb',\n",
       "  'def wrap_class(nm, *fld_names, sup=None, doc=None, funcs=None, **flds):\\n    \"Decorator: makes function a method of a new class `nm` passing parameters to `mk_class`\"\\n    def _inner(f):\\n        mk_class(nm, *fld_names, sup=sup, doc=doc, funcs=L(funcs)+f, mod=f.__globals__, **flds)\\n        return f\\n    return _inner'),\n",
       " ('01_core.ipynb',\n",
       "  'def store_attr(self, nms):\\n    \"Store params named in comma-separated `nms` from calling context into attrs in `self`\"\\n    mod = inspect.currentframe().f_back.f_locals\\n    for n in re.split(\\', *\\', nms): setattr(self,n,mod[n])'),\n",
       " ('01_core.ipynb',\n",
       "  'def tuplify(o, use_list=False, match=None):\\n    \"Make `o` a tuple\"\\n    return tuple(L(o, use_list=use_list, match=match))'),\n",
       " ('01_core.ipynb',\n",
       "  'def replicate(item,match):\\n    \"Create tuple of `item` copied `len(match)` times\"\\n    return (item,)*len(match)'),\n",
       " ('01_core.ipynb',\n",
       "  'def uniqueify(x, sort=False, bidir=False, start=None):\\n    \"Return the unique elements in `x`, optionally `sort`-ed, optionally return the reverse correspondance.\"\\n    res = L(x).unique()\\n    if start is not None: res = start+res\\n    if sort: res.sort()\\n    if bidir: return res, res.val2idx()\\n    return res'),\n",
       " ('01_core.ipynb',\n",
       "  'def setify(o): return o if isinstance(o,set) else set(L(o))'),\n",
       " ('01_core.ipynb',\n",
       "  'def is_listy(x):\\n    \"`isinstance(x, (tuple,list,L))`\"\\n    return isinstance(x, (tuple,list,L,slice,Generator))'),\n",
       " ('01_core.ipynb',\n",
       "  'def range_of(x):\\n    \"All indices of collection `x` (i.e. `list(range(len(x)))`)\"\\n    return list(range(len(x)))'),\n",
       " ('01_core.ipynb',\n",
       "  'def groupby(x, key):\\n    \"Like `itertools.groupby` but doesn\\'t need to be sorted, and isn\\'t lazy\"\\n    res = {}\\n    for o in x: res.setdefault(key(o), []).append(o)\\n    return res'),\n",
       " ('01_core.ipynb',\n",
       "  'def merge(*ds):\\n    \"Merge all dictionaries in `ds`\"\\n    return {k:v for d in ds for k,v in d.items()}'),\n",
       " ('01_core.ipynb',\n",
       "  'def shufflish(x, pct=0.04):\\n    \"Randomly relocate items of `x` up to `pct` of `len(x)` from their starting location\"\\n    n = len(x)\\n    return L(x[i] for i in sorted(range_of(x), key=lambda o: o+n*(1+random.random()*pct)))'),\n",
       " ('01_core.ipynb',\n",
       "  'class IterLen:\\n    \"Base class to add iteration to anything supporting `len` and `__getitem__`\"\\n    def __iter__(self): return (self[i] for i in range_of(self))'),\n",
       " ('01_core.ipynb',\n",
       "  '@docs\\nclass ReindexCollection(GetAttr, IterLen):\\n    \"Reindexes collection `coll` with indices `idxs` and optional LRU cache of size `cache`\"\\n    def __init__(self, coll, idxs=None, cache=None):\\n        self.default,self.coll,self.idxs,self.cache = coll,coll,ifnone(idxs,L.range(coll)),cache\\n        def _get(self, i): return self.coll[i]\\n        self._get = types.MethodType(_get,self)\\n        if cache is not None: self._get = functools.lru_cache(maxsize=cache)(self._get)\\n\\n    def __getitem__(self, i): return self._get(self.idxs[i])\\n    def __len__(self): return len(self.coll)\\n    def reindex(self, idxs): self.idxs = idxs\\n    def shuffle(self): random.shuffle(self.idxs)\\n    def cache_clear(self): self._get.cache_clear()\\n\\n    _docs = dict(reindex=\"Replace `self.idxs` with idxs\",\\n                shuffle=\"Randomly shuffle indices\",\\n                cache_clear=\"Clear LRU cache\")'),\n",
       " ('01_core.ipynb',\n",
       "  'def _oper(op,a,b=None): return (lambda o:op(o,a)) if b is None else op(a,b)\\n\\ndef _mk_op(nm, mod=None):\\n    \"Create an operator using `oper` and add to the caller\\'s module\"\\n    if mod is None: mod = inspect.currentframe().f_back.f_locals\\n    op = getattr(operator,nm)\\n    def _inner(a,b=None): return _oper(op, a,b)\\n    _inner.__name__ = _inner.__qualname__ = nm\\n    _inner.__doc__ = f\\'Same as `operator.{nm}`, or returns partial if 1 arg\\'\\n    mod[nm] = _inner'),\n",
       " ('01_core.ipynb',\n",
       "  \"for op in 'lt gt le ge eq ne add sub mul truediv'.split(): _mk_op(op)\"),\n",
       " ('01_core.ipynb',\n",
       "  'class _InfMeta(type):\\n    @property\\n    def count(self): return itertools.count()\\n    @property\\n    def zeros(self): return itertools.cycle([0])\\n    @property\\n    def ones(self):  return itertools.cycle([1])\\n    @property\\n    def nones(self): return itertools.cycle([None])'),\n",
       " ('01_core.ipynb',\n",
       "  'class Inf(metaclass=_InfMeta):\\n    \"Infinite lists\"\\n    pass'),\n",
       " ('01_core.ipynb',\n",
       "  'def true(*args, **kwargs):\\n    \"Predicate: always `True`\"\\n    return True'),\n",
       " ('01_core.ipynb',\n",
       "  'def stop(e=StopIteration):\\n    \"Raises exception `e` (by default `StopException`) even if in an expression\"\\n    raise e'),\n",
       " ('01_core.ipynb',\n",
       "  'def gen(func, seq, cond=true):\\n    \"Like `(func(o) for o in seq if cond(func(o)))` but handles `StopIteration`\"\\n    return itertools.takewhile(cond, map(func,seq))'),\n",
       " ('01_core.ipynb',\n",
       "  'def chunked(it, cs, drop_last=False):\\n    if not isinstance(it, Iterator): it = iter(it)\\n    while True:\\n        res = list(itertools.islice(it, cs))\\n        if res and (len(res)==cs or not drop_last): yield res\\n        if len(res)<cs: return'),\n",
       " ('01_core.ipynb',\n",
       "  'def retain_type(new, old=None, typ=None):\\n    \"Cast `new` to type of `old` if it\\'s a superclass\"\\n    # e.g. old is TensorImage, new is Tensor - if not subclass then do nothing\\n    if new is None: return new\\n    assert old is not None or typ is not None\\n    if typ is None:\\n        if not isinstance(old, type(new)): return new\\n        typ = old if isinstance(old,type) else type(old)\\n    # Do nothing the new type is already an instance of requested type (i.e. same type)\\n    return typ(new) if typ!=NoneType and not isinstance(new, typ) else new'),\n",
       " ('01_core.ipynb',\n",
       "  'def retain_types(new, old=None, typs=None):\\n    \"Cast each item of `new` to type of matching item in `old` if it\\'s a superclass\"\\n    if not is_listy(new): return retain_type(new, old, typs)\\n    return tuple(L(new, old, typs).mapped_zip(retain_type, cycled=True))'),\n",
       " ('01_core.ipynb',\n",
       "  'def show_title(o, ax=None, ctx=None, label=None, **kwargs):\\n    \"Set title of `ax` to `o`, or print `o` if `ax` is `None`\"\\n    ax = ifnone(ax,ctx)\\n    if ax is None: print(o)\\n    elif hasattr(ax, \\'set_title\\'): ax.set_title(o)\\n    elif isinstance(ax, pd.Series):\\n        while label in ax: label += \\'_\\'\\n        ax = ax.append(pd.Series({label: o}))\\n    return ax'),\n",
       " ('01_core.ipynb',\n",
       "  'class ShowTitle:\\n    \"Base class that adds a simple `show`\"\\n    _show_args = {\\'label\\': \\'text\\'}\\n    def show(self, ctx=None, **kwargs): return show_title(str(self), ctx=ctx, **merge(self._show_args, kwargs))\\n\\nclass Int(int, ShowTitle): pass\\nclass Float(float, ShowTitle): pass\\nclass Str(str, ShowTitle): pass\\nadd_docs(Int, \"An `int` with `show`\"); add_docs(Str, \"An `str` with `show`\"); add_docs(Float, \"An `float` with `show`\")'),\n",
       " ('01_core.ipynb',\n",
       "  'class TupleBase(tuple, ShowTitle):\\n    \"A `tuple` with `show` and `__neg__`\"\\n    def __neg__(self): return tuple(map(operator.neg,self))'),\n",
       " ('01_core.ipynb',\n",
       "  'def trace(f):\\n    \"Add `set_trace` to an existing function `f`\"\\n    def _inner(*args,**kwargs):\\n        set_trace()\\n        return f(*args,**kwargs)\\n    return _inner'),\n",
       " ('01_core.ipynb',\n",
       "  'def compose(*funcs, order=None):\\n    \"Create a function that composes all functions in `funcs`, passing along remaining `*args` and `**kwargs` to all\"\\n    funcs = L(funcs)\\n    if order is not None: funcs = funcs.sorted(order)\\n    def _inner(x, *args, **kwargs):\\n        for f in L(funcs): x = f(x, *args, **kwargs)\\n        return x\\n    return _inner'),\n",
       " ('01_core.ipynb',\n",
       "  'def maps(*args, retain=noop):\\n    \"Like `map`, except funcs are composed first\"\\n    f = compose(*args[:-1])\\n    def _f(b): return retain(f(b), b)\\n    return map(_f, args[-1])'),\n",
       " ('01_core.ipynb',\n",
       "  'def partialler(f, *args, order=None, **kwargs):\\n    \"Like `functools.partial` but also copies over docstring\"\\n    fnew = partial(f,*args,**kwargs)\\n    fnew.__doc__ = f.__doc__\\n    if order is not None: fnew.order=order\\n    elif hasattr(f,\\'order\\'): fnew.order=f.order\\n    return fnew'),\n",
       " ('01_core.ipynb',\n",
       "  'def instantiate(t):\\n    \"Instantiate `t` if it\\'s a type, otherwise do nothing\"\\n    return t() if isinstance(t, type) else t'),\n",
       " ('01_core.ipynb',\n",
       "  \"mk_class('_Arg', 'i')\\n_0,_1,_2,_3,_4 = _Arg(0),_Arg(1),_Arg(2),_Arg(3),_Arg(4)\"),\n",
       " ('01_core.ipynb',\n",
       "  'class bind:\\n    \"Same as `partial`, except you can use `_0` `_1` etc param placeholders\"\\n    def __init__(self, fn, *pargs, **pkwargs):\\n        store_attr(self, \\'fn,pargs,pkwargs\\')\\n        self.maxi = max((x.i for x in pargs if isinstance(x, _Arg)), default=-1)\\n\\n    def __call__(self, *args, **kwargs):\\n        fargs = L(args[x.i] if isinstance(x, _Arg) else x for x in self.pargs) + args[self.maxi+1:]\\n        return self.fn(*fargs, **{**self.pkwargs, **kwargs})'),\n",
       " ('01_core.ipynb',\n",
       "  'class _Self:\\n    \"An alternative to `lambda` for calling methods on passed object.\"\\n    def __init__(self): self.nms,self.args,self.kwargs,self.ready = [],[],[],True\\n    def __repr__(self): return f\\'self: {self.nms}({self.args}, {self.kwargs})\\'\\n\\n    def __call__(self, *args, **kwargs):\\n        if self.ready:\\n            x = args[0]\\n            for n,a,k in zip(self.nms,self.args,self.kwargs):\\n                x = getattr(x,n)\\n                if a is not None: x = x(*a, **k)\\n            return x\\n        else:\\n            self.args.append(args)\\n            self.kwargs.append(kwargs)\\n            self.ready = True\\n            return self\\n\\n    def __getattr__(self,k):\\n        if not self.ready:\\n            self.args.append(None)\\n            self.kwargs.append(None)\\n        self.nms.append(k)\\n        self.ready = False\\n        return self\\n\\nclass _SelfCls:\\n    def __getattr__(self,k): return getattr(_Self(),k)\\n\\nSelf = _SelfCls()'),\n",
       " ('01_core.ipynb',\n",
       "  '#NB: Please don\\'t move this to a different line or module, since it\\'s used in testing `get_source_link`\\n@patch\\ndef ls(self:Path, file_type=None, file_exts=None):\\n    \"Contents of path as a list\"\\n    extns=L(file_exts)\\n    if file_type: extns += L(k for k,v in mimetypes.types_map.items() if v.startswith(file_type+\\'/\\'))\\n    return L(self.iterdir()).filtered(lambda x: len(extns)==0 or x.suffix in extns)'),\n",
       " ('01_core.ipynb',\n",
       "  'def _is_instance(f, gs):\\n    tst = [g if type(g) in [type, \\'function\\'] else g.__class__ for g in gs]\\n    for g in tst:\\n        if isinstance(f, g) or f==g: return True\\n    return False\\n\\ndef _is_first(f, gs):\\n    for o in L(getattr(f, \\'run_after\\', None)):\\n        if _is_instance(o, gs): return False\\n    for g in gs:\\n        if _is_instance(f, L(getattr(g, \\'run_before\\', None))): return False\\n    return True\\n\\ndef sort_by_run(fs):\\n    end = L(getattr(f, \\'toward_end\\', False) for f in fs)\\n    inp,res = L(fs)[~end] + L(fs)[end], []\\n    while len(inp) > 0:\\n        for i,o in enumerate(inp):\\n            if _is_first(o, inp):\\n                res.append(inp.pop(i))\\n                break\\n        else: raise Exception(\"Impossible to sort\")\\n    return res'),\n",
       " ('01_core.ipynb',\n",
       "  'def display_df(df):\\n    \"Display `df` in a notebook or defaults to print\"\\n    try: from IPython.display import display, HTML\\n    except: return print(df)\\n    display(HTML(df.to_html()))'),\n",
       " ('01_core.ipynb',\n",
       "  'def round_multiple(x, mult, round_down=False):\\n    \"Round `x` to nearest multiple of `mult`\"\\n    def _f(x_): return (int if round_down else round)(x_/mult)*mult\\n    res = L(x).mapped(_f)\\n    return res if is_listy(x) else res[0]'),\n",
       " ('01_core.ipynb',\n",
       "  'def num_cpus():\\n    \"Get number of cpus\"\\n    try:                   return len(os.sched_getaffinity(0))\\n    except AttributeError: return os.cpu_count()\\n\\ndefaults.cpus = num_cpus()'),\n",
       " ('01_core.ipynb',\n",
       "  'def add_props(f, n=2):\\n    \"Create properties passing each of `range(n)` to f\"\\n    return (property(partial(f,i)) for i in range(n))'),\n",
       " ('06_data_source.ipynb',\n",
       "  'def all_union(sets):\\n    \"Set of union of all `sets` (each `setified` if needed)\"\\n    return set().union(*(map(setify,sets)))'),\n",
       " ('06_data_source.ipynb',\n",
       "  'def all_disjoint(sets):\\n    \"`True` iif no element appears in more than one item of `sets`\"\\n    return sum(map(len,sets))==len(all_union(sets))'),\n",
       " ('13_learner.ipynb',\n",
       "  \"_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\\n_camel_re2 = re.compile('([a-z0-9])([A-Z])')\\n\\ndef camel2snake(name):\\n    s1   = re.sub(_camel_re1, r'\\\\1_\\\\2', name)\\n    return re.sub(_camel_re2, r'\\\\1_\\\\2', s1).lower()\"),\n",
       " ('15_callback_hook.ipynb',\n",
       "  'class PrettyString(str):\\n    \"Little hack to get strings to show properly in Jupyter.\"\\n    def __repr__(self): return self'),\n",
       " ('32_text_models_awdlstm.ipynb',\n",
       "  'def one_param(m): return next(iter(m.parameters()))')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_split(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _split(code):\n",
    "    lines = code.split('\\n')\n",
    "    default_nb = _re_default_nb.search(lines[0]).groups()[0]\n",
    "    s,res = 1,[]\n",
    "    while _re_cell.search(lines[s]) is None: s += 1\n",
    "    e = s+1\n",
    "    while e < len(lines):\n",
    "        while e < len(lines) and _re_cell.search(lines[e]) is None: e += 1\n",
    "        grps = _re_cell.search(lines[s]).groups()\n",
    "        nb = grps[0] or default_nb\n",
    "        content = lines[s+1:e]\n",
    "        while len(content) > 1 and content[-1] == '': content = content[:-1]\n",
    "        res.append((nb, '\\n'.join(content)))\n",
    "        s,e = e,e+1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _relimport2name(name, mod_name):\n",
    "    if mod_name.endswith('.py'): mod_name = mod_name[:-3]\n",
    "    mods = mod_name.split(os.path.sep)\n",
    "    mods = mods[mods.index('local'):]\n",
    "    i = 0\n",
    "    while name[i] == '.': i += 1\n",
    "    return '.'.join(mods[:-i] + [name[i:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert _relimport2name('.core', 'local/data.py') == 'local.core'\n",
    "assert _relimport2name('.core', 'home/sgugger/fastai_dev/dev/local/data.py') == 'local.core'\n",
    "assert _relimport2name('..core', 'local/vision/data.py') == 'local.core'\n",
    "assert _relimport2name('.transform', 'local/vision/data.py') == 'local.vision.transform'\n",
    "assert _relimport2name('..notebook.core', 'local/data/external.py') == 'local.notebook.core'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#Catches any from .bla import something and catches local.bla in group 1, the imported thing(s) in group 2.\n",
    "_re_loc_import = re.compile(r'^\\s*from (\\.\\S*) import (.*)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _deal_loc_import(code, fname):\n",
    "    lines = []\n",
    "    def _replace(m):\n",
    "        mod,obj = m.groups()\n",
    "        return f\"from {_relimport2name(mod, fname)} import {obj}\"\n",
    "    for line in code.split('\\n'):\n",
    "        line = re.sub('__'+'file__', '_'+'file_', line) #Need to break _file_ or that line will be treated\n",
    "        lines.append(_re_loc_import.sub(_replace,line))\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "code = \"from .core import *\\nnothing to see\\nfrom .vision import bla1, bla2\"\n",
    "assert _deal_loc_import(code, 'local/data.py') == \"from local.core import *\\nnothing to see\\nfrom local.vision import bla1, bla2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _script2notebook(fname, dic, silent=False):\n",
    "    \"Put the content of `fname` back in the notebooks it came from.\"\n",
    "    if os.environ.get('IN_TEST',0): return  # don't export if running tests\n",
    "    fname = Path(fname)\n",
    "    with open(fname) as f: code = f.read()\n",
    "    splits = _split(code)\n",
    "    assert len(splits) == len(dic[fname]), f\"Exported file from notebooks should have {len(dic[fname])} cells but has {len(splits)}.\"\n",
    "    assert np.all([c1[0]==c2[1]] for c1,c2 in zip(splits, dic[fname]))\n",
    "    splits = [(c2[0],c1[0],c1[1]) for c1,c2 in zip(splits, dic[fname])]\n",
    "    nb_fnames = {s[1] for s in splits}\n",
    "    for nb_fname in nb_fnames:\n",
    "        nb = read_nb(nb_fname)\n",
    "        for i,f,c in splits:\n",
    "            c = _deal_loc_import(c, str(fname))\n",
    "            if f == nb_fname:\n",
    "                l = nb['cells'][i]['source'].split('\\n')[0]\n",
    "                nb['cells'][i]['source'] = l + '\\n' + c\n",
    "        NotebookNotary().sign(nb)\n",
    "        nbformat.write(nb, nb_fname, version=4)\n",
    "    if not silent: print(f\"Converted {fname}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (Path.cwd()/'lib.pkl').exists(): os.remove(Path.cwd()/'lib.pkl')\n",
    "notebook2script(all_fs=True, silent=True, to_pkl=True)\n",
    "dic = pickle.load(open(Path.cwd()/'lib.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted /home/sgugger/git/fastai_dev/dev/local/tabular/core.py.\n"
     ]
    }
   ],
   "source": [
    "_script2notebook(Path().cwd()/'local/tabular/core.py', dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_manual_mods = ['__init__.py', 'imports.py', 'torch_imports.py', 'all.py', 'torch_basics.py', 'fp16_utils.py']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def script2notebook(folder='local', silent=False):\n",
    "    if (Path.cwd()/'lib.pkl').exists(): os.remove(Path.cwd()/'lib.pkl')\n",
    "    notebook2script(all_fs=True, silent=True, to_pkl=True)\n",
    "    dic = pickle.load(open(Path.cwd()/'lib.pkl', 'rb'))\n",
    "    os.remove(Path.cwd()/'lib.pkl')\n",
    "    if os.environ.get('IN_TEST',0): return  # don't export if running tests\n",
    "    for f in (Path.cwd()/folder).glob('**/*.py'):\n",
    "        if f.name not in _manual_mods: _script2notebook(f, dic, silent=silent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted /home/sgugger/git/fastai_dev/dev/local/test.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/metrics.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/optimizer.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/core.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/layers.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/script.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/torch_core.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/learner.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/tabular/core.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/tabular/model.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/callback/fp16.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/callback/tracker.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/callback/schedule.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/callback/mixup.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/callback/rnn.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/callback/progress.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/callback/hook.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/vision/augment.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/vision/core.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/vision/models/xresnet.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/utils/test.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/text/core.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/text/data.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/text/models/awdlstm.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/text/models/core.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/text/models/qrnn.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/notebook/showdoc.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/notebook/core.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/notebook/export.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/notebook/export2html.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/data/block.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/data/load.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/data/core.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/data/transform.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/data/pipeline.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/data/source.py.\n",
      "Converted /home/sgugger/git/fastai_dev/dev/local/data/external.py.\n"
     ]
    }
   ],
   "source": [
    "script2notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diff notebook - library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _print_diff(code1, code2, fname):\n",
    "    diff = difflib.ndiff(code1, code2)\n",
    "    sys.stdout.writelines(diff)\n",
    "    #for l in difflib.context_diff(code1, code2): print(l)\n",
    "    #_print_diff_py(code1, code2, fname) if fname.endswith('.py') else _print_diff_txt(code1, code2, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def diff_nb_script(lib_folder='local'):\n",
    "    tmp_path1,tmp_path2 = Path.cwd()/'tmp_lib',Path.cwd()/'tmp_lib1'\n",
    "    shutil.copytree(Path.cwd()/lib_folder, tmp_path1)\n",
    "    try:\n",
    "        notebook2script(all_fs=True, silent=True)\n",
    "        shutil.copytree(Path.cwd()/lib_folder, tmp_path2)\n",
    "        shutil.rmtree(Path.cwd()/lib_folder)\n",
    "        shutil.copytree(tmp_path1, Path.cwd()/lib_folder)\n",
    "        res = subprocess.run(['diff', '-ru', 'tmp_lib1', lib_folder], stdout=subprocess.PIPE)\n",
    "        print(res.stdout.decode('utf-8'))\n",
    "    finally: \n",
    "        shutil.rmtree(tmp_path1)\n",
    "        shutil.rmtree(tmp_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff -ru tmp_lib1/callback/fp16.py local/callback/fp16.py\n",
      "--- tmp_lib1/callback/fp16.py\t2019-09-09 10:28:32.006440259 -0700\n",
      "+++ local/callback/fp16.py\t2019-09-09 10:26:52.902513593 -0700\n",
      "@@ -1,6 +1,7 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/18_callback_fp16.ipynb (unless otherwise specified).\n",
      " \n",
      "-__all__ = ['get_master', 'to_master_grads', 'to_model_params', 'test_overflow', 'grad_overflow', 'MixedPrecision']\n",
      "+__all__ = ['get_master', 'to_master_grads', 'to_model_params', 'test_overflow', 'grad_overflow', 'MixedPrecision',\n",
      "+           'get_master', 'to_master_grads', 'to_model_params', 'test_overflow', 'grad_overflow', 'MixedPrecision']\n",
      " \n",
      " #Cell\n",
      " from ..torch_basics import *\n",
      "diff -ru tmp_lib1/callback/hook.py local/callback/hook.py\n",
      "--- tmp_lib1/callback/hook.py\t2019-09-09 10:28:31.902440384 -0700\n",
      "+++ local/callback/hook.py\t2019-09-09 10:26:52.666513650 -0700\n",
      "@@ -1,7 +1,8 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/15_callback_hook.ipynb (unless otherwise specified).\n",
      " \n",
      " __all__ = ['Hook', 'hook_output', 'Hooks', 'hook_outputs', 'has_params', 'HookCallback', 'ActivationStats',\n",
      "-           'total_params', 'layer_info']\n",
      "+           'total_params', 'layer_info', 'Hook', 'hook_output', 'Hooks', 'hook_outputs', 'has_params', 'HookCallback',\n",
      "+           'ActivationStats', 'total_params', 'layer_info']\n",
      " \n",
      " #Cell\n",
      " from ..torch_basics import *\n",
      "diff -ru tmp_lib1/callback/mixup.py local/callback/mixup.py\n",
      "--- tmp_lib1/callback/mixup.py\t2019-09-09 10:28:32.022440240 -0700\n",
      "+++ local/callback/mixup.py\t2019-09-09 10:26:52.938513585 -0700\n",
      "@@ -1,6 +1,6 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/19_callback_mixup.ipynb (unless otherwise specified).\n",
      " \n",
      "-__all__ = ['reduce_loss', 'MixUp']\n",
      "+__all__ = ['reduce_loss', 'MixUp', 'reduce_loss', 'MixUp']\n",
      " \n",
      " #Cell\n",
      " from ..torch_basics import *\n",
      "diff -ru tmp_lib1/callback/progress.py local/callback/progress.py\n",
      "--- tmp_lib1/callback/progress.py\t2019-09-09 10:28:31.942440336 -0700\n",
      "+++ local/callback/progress.py\t2019-09-09 10:26:52.750513630 -0700\n",
      "@@ -1,6 +1,6 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/16_callback_progress.ipynb (unless otherwise specified).\n",
      " \n",
      "-__all__ = ['ProgressCallback', 'ShowGraphCallback', 'CSVLogger']\n",
      "+__all__ = ['ProgressCallback', 'ShowGraphCallback', 'CSVLogger', 'ProgressCallback', 'ShowGraphCallback', 'CSVLogger']\n",
      " \n",
      " #Cell\n",
      " from ..torch_basics import *\n",
      "diff -ru tmp_lib1/callback/rnn.py local/callback/rnn.py\n",
      "--- tmp_lib1/callback/rnn.py\t2019-09-09 10:28:32.342439853 -0700\n",
      "+++ local/callback/rnn.py\t2019-09-09 10:26:54.326513234 -0700\n",
      "@@ -1,6 +1,6 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/34_callback_rnn.ipynb (unless otherwise specified).\n",
      " \n",
      "-__all__ = ['RNNTrainer']\n",
      "+__all__ = ['RNNTrainer', 'RNNTrainer']\n",
      " \n",
      " #Cell\n",
      " from ..imports import *\n",
      "diff -ru tmp_lib1/callback/schedule.py local/callback/schedule.py\n",
      "--- tmp_lib1/callback/schedule.py\t2019-09-09 10:28:31.834440465 -0700\n",
      "+++ local/callback/schedule.py\t2019-09-09 10:26:52.530513683 -0700\n",
      "@@ -1,7 +1,8 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/14_callback_schedule.ipynb (unless otherwise specified).\n",
      " \n",
      " __all__ = ['annealer', 'SchedLin', 'SchedCos', 'SchedNo', 'SchedExp', 'SchedPoly', 'combine_scheds', 'combined_cos',\n",
      "-           'ParamScheduler', 'LRFinder']\n",
      "+           'ParamScheduler', 'LRFinder', 'annealer', 'SchedLin', 'SchedCos', 'SchedNo', 'SchedExp', 'SchedPoly',\n",
      "+           'combine_scheds', 'combined_cos', 'ParamScheduler', 'LRFinder']\n",
      " \n",
      " #Cell\n",
      " from ..torch_basics import *\n",
      "diff -ru tmp_lib1/callback/tracker.py local/callback/tracker.py\n",
      "--- tmp_lib1/callback/tracker.py\t2019-09-09 10:28:31.970440302 -0700\n",
      "+++ local/callback/tracker.py\t2019-09-09 10:26:52.814513614 -0700\n",
      "@@ -1,7 +1,8 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/17_callback_tracker.ipynb (unless otherwise specified).\n",
      " \n",
      " __all__ = ['TerminateOnNaNCallback', 'TrackerCallback', 'EarlyStoppingCallback', 'SaveModelCallback',\n",
      "-           'ReduceLROnPlateau']\n",
      "+           'ReduceLROnPlateau', 'TerminateOnNaNCallback', 'TrackerCallback', 'EarlyStoppingCallback',\n",
      "+           'SaveModelCallback', 'ReduceLROnPlateau']\n",
      " \n",
      " #Cell\n",
      " from ..torch_basics import *\n",
      "diff -ru tmp_lib1/core.py local/core.py\n",
      "--- tmp_lib1/core.py\t2019-09-09 10:28:32.310439891 -0700\n",
      "+++ local/core.py\t2019-09-09 10:26:53.994513321 -0700\n",
      "@@ -9,7 +9,16 @@\n",
      "            'retain_types', 'show_title', 'ShowTitle', 'Int', 'Float', 'Str', 'TupleBase', 'trace', 'compose', 'maps',\n",
      "            'partialler', 'instantiate', '_0', '_1', '_2', '_3', '_4', 'bind', 'Self', 'Self', 'sort_by_run',\n",
      "            'display_df', 'round_multiple', 'num_cpus', 'add_props', 'all_union', 'all_disjoint', 'camel2snake',\n",
      "-           'PrettyString', 'one_param']\n",
      "+           'PrettyString', 'one_param', 'defaults', 'PrePostInitMeta', 'BaseObj', 'NewChkMeta', 'BypassNewMeta',\n",
      "+           'patch_to', 'patch', 'patch_property', 'use_kwargs', 'delegates', 'funcs_kwargs', 'method', 'chk',\n",
      "+           'add_docs', 'docs', 'custom_dir', 'GetAttr', 'delegate_attr', 'coll_repr', 'mask2idxs', 'CollBase', 'cycle',\n",
      "+           'zip_cycle', 'L', 'ifnone', 'get_class', 'mk_class', 'wrap_class', 'store_attr', 'tuplify', 'replicate',\n",
      "+           'uniqueify', 'setify', 'is_listy', 'range_of', 'groupby', 'merge', 'shufflish', 'IterLen',\n",
      "+           'ReindexCollection', 'lt', 'gt', 'le', 'ge', 'eq', 'ne', 'add', 'sub', 'mul', 'truediv', 'Inf', 'true',\n",
      "+           'stop', 'gen', 'chunked', 'retain_type', 'retain_types', 'show_title', 'ShowTitle', 'Int', 'Float', 'Str',\n",
      "+           'TupleBase', 'trace', 'compose', 'maps', 'partialler', 'instantiate', '_0', '_1', '_2', '_3', '_4', 'bind',\n",
      "+           'Self', 'Self', 'sort_by_run', 'display_df', 'round_multiple', 'num_cpus', 'add_props', 'all_union',\n",
      "+           'all_disjoint', 'camel2snake', 'PrettyString', 'one_param']\n",
      " \n",
      " #Cell\n",
      " from .test import *\n",
      "diff -ru tmp_lib1/data/block.py local/data/block.py\n",
      "--- tmp_lib1/data/block.py\t2019-09-09 10:28:32.582439563 -0700\n",
      "+++ local/data/block.py\t2019-09-09 10:26:54.770513118 -0700\n",
      "@@ -1,6 +1,6 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/50_data_block.ipynb (unless otherwise specified).\n",
      " \n",
      "-__all__ = ['DataBlock']\n",
      "+__all__ = ['DataBlock', 'DataBlock']\n",
      " \n",
      " #Cell\n",
      " from ..torch_basics import *\n",
      "diff -ru tmp_lib1/data/core.py local/data/core.py\n",
      "--- tmp_lib1/data/core.py\t2019-09-09 10:28:30.926441554 -0700\n",
      "+++ local/data/core.py\t2019-09-09 10:26:50.910514059 -0700\n",
      "@@ -3,7 +3,10 @@\n",
      " __all__ = ['get_files', 'FileGetter', 'image_extensions', 'get_image_files', 'ImageGetter', 'RandomSplitter',\n",
      "            'GrandparentSplitter', 'parent_label', 'RegexLabeller', 'CategoryMap', 'Category', 'Categorize',\n",
      "            'MultiCategory', 'MultiCategorize', 'OneHotEncode', 'ToTensor', 'TfmdDL', 'Cuda', 'ByteToFloatTensor',\n",
      "-           'Normalize', 'broadcast_vec', 'DataBunch']\n",
      "+           'Normalize', 'broadcast_vec', 'DataBunch', 'get_files', 'FileGetter', 'image_extensions', 'get_image_files',\n",
      "+           'ImageGetter', 'RandomSplitter', 'GrandparentSplitter', 'parent_label', 'RegexLabeller', 'CategoryMap',\n",
      "+           'Category', 'Categorize', 'MultiCategory', 'MultiCategorize', 'OneHotEncode', 'ToTensor', 'TfmdDL', 'Cuda',\n",
      "+           'ByteToFloatTensor', 'Normalize', 'broadcast_vec', 'DataBunch']\n",
      " \n",
      " #Cell\n",
      " from ..torch_basics import *\n",
      "diff -ru tmp_lib1/data/external.py local/data/external.py\n",
      "--- tmp_lib1/data/external.py\t2019-09-09 10:28:32.870439214 -0700\n",
      "+++ local/data/external.py\t2019-09-09 10:26:56.134512748 -0700\n",
      "@@ -1,6 +1,7 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/96_data_external.ipynb (unless otherwise specified).\n",
      " \n",
      "-__all__ = ['download_url', 'URLs', 'get_path', 'ConfigKey', 'download_data', 'tar_extract', 'untar_data']\n",
      "+__all__ = ['download_url', 'URLs', 'get_path', 'ConfigKey', 'download_data', 'tar_extract', 'untar_data',\n",
      "+           'download_url', 'URLs', 'get_path', 'ConfigKey', 'download_data', 'tar_extract', 'untar_data']\n",
      " \n",
      " #Cell\n",
      " from ..torch_basics import *\n",
      "diff -ru tmp_lib1/data/load.py local/data/load.py\n",
      "--- tmp_lib1/data/load.py\t2019-09-09 10:28:30.682441845 -0700\n",
      "+++ local/data/load.py\t2019-09-09 10:26:50.510514146 -0700\n",
      "@@ -1,6 +1,6 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/01c_dataloader.ipynb (unless otherwise specified).\n",
      " \n",
      "-__all__ = ['fa_collate', 'fa_convert', 'DataLoader']\n",
      "+__all__ = ['fa_collate', 'fa_convert', 'DataLoader', 'fa_collate', 'fa_convert', 'DataLoader']\n",
      " \n",
      " #Cell\n",
      " from ..torch_basics import *\n",
      "diff -ru tmp_lib1/data/pipeline.py local/data/pipeline.py\n",
      "--- tmp_lib1/data/pipeline.py\t2019-09-09 10:28:30.818441683 -0700\n",
      "+++ local/data/pipeline.py\t2019-09-09 10:26:50.714514102 -0700\n",
      "@@ -1,7 +1,8 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/03_data_pipeline.ipynb (unless otherwise specified).\n",
      " \n",
      " __all__ = ['get_func', 'Func', 'Sig', 'compose_tfms', 'batch_to_samples', 'mk_transform', 'Pipeline', 'TfmdBase',\n",
      "-           'TfmdList', 'TfmdDS']\n",
      "+           'TfmdList', 'TfmdDS', 'get_func', 'Func', 'Sig', 'compose_tfms', 'batch_to_samples', 'mk_transform',\n",
      "+           'Pipeline', 'TfmdBase', 'TfmdList', 'TfmdDS']\n",
      " \n",
      " #Cell\n",
      " from ..torch_basics import *\n",
      "diff -ru tmp_lib1/data/source.py local/data/source.py\n",
      "--- tmp_lib1/data/source.py\t2019-09-09 10:28:30.958441516 -0700\n",
      "+++ local/data/source.py\t2019-09-09 10:26:50.966514046 -0700\n",
      "@@ -1,6 +1,6 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/06_data_source.ipynb (unless otherwise specified).\n",
      " \n",
      "-__all__ = ['DataSource']\n",
      "+__all__ = ['DataSource', 'DataSource']\n",
      " \n",
      " #Cell\n",
      " from ..torch_basics import *\n",
      "diff -ru tmp_lib1/data/transform.py local/data/transform.py\n",
      "--- tmp_lib1/data/transform.py\t2019-09-09 10:28:30.742441774 -0700\n",
      "+++ local/data/transform.py\t2019-09-09 10:26:50.610514124 -0700\n",
      "@@ -1,6 +1,8 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/02_data_transforms.ipynb (unless otherwise specified).\n",
      " \n",
      " __all__ = ['type_hints', 'anno_ret', 'cmp_instance', 'subplots', 'TensorImageBase', 'TensorImage', 'TensorImageBW',\n",
      "+           'TensorMask', 'TypeDispatch', 'Transform', 'InplaceTransform', 'TupleTransform', 'ItemTransform',\n",
      "+           'type_hints', 'anno_ret', 'cmp_instance', 'subplots', 'TensorImageBase', 'TensorImage', 'TensorImageBW',\n",
      "            'TensorMask', 'TypeDispatch', 'Transform', 'InplaceTransform', 'TupleTransform', 'ItemTransform']\n",
      " \n",
      " #Cell\n",
      "diff -ru tmp_lib1/layers.py local/layers.py\n",
      "--- tmp_lib1/layers.py\t2019-09-09 10:28:31.538440820 -0700\n",
      "+++ local/layers.py\t2019-09-09 10:26:51.994513811 -0700\n",
      "@@ -5,6 +5,12 @@\n",
      "            'BatchNorm1dFlat', 'BnDropLin', 'init_default', 'ConvLayer', 'FlattenedLoss', 'CrossEntropyLossFlat',\n",
      "            'BCEWithLogitsLossFlat', 'BCELossFlat', 'MSELossFlat', 'trunc_normal_', 'Embedding', 'SelfAttention',\n",
      "            'PooledSelfAttention2d', 'icnr_init', 'PixelShuffle_ICNR', 'SequentialEx', 'MergeLayer', 'SimpleCNN',\n",
      "+           'ResBlock', 'ParameterModule', 'children_and_parameters', 'TstModule', 'tst', 'children', 'flatten_model',\n",
      "+           'Module', 'Lambda', 'PartialLambda', 'View', 'ResizeBatch', 'Flatten', 'Debugger', 'sigmoid_range',\n",
      "+           'SigmoidRange', 'AdaptiveConcatPool2d', 'pool_layer', 'PoolFlatten', 'NormType', 'BatchNorm',\n",
      "+           'BatchNorm1dFlat', 'BnDropLin', 'init_default', 'ConvLayer', 'FlattenedLoss', 'CrossEntropyLossFlat',\n",
      "+           'BCEWithLogitsLossFlat', 'BCELossFlat', 'MSELossFlat', 'trunc_normal_', 'Embedding', 'SelfAttention',\n",
      "+           'PooledSelfAttention2d', 'icnr_init', 'PixelShuffle_ICNR', 'SequentialEx', 'MergeLayer', 'SimpleCNN',\n",
      "            'ResBlock', 'ParameterModule', 'children_and_parameters', 'TstModule', 'tst', 'children', 'flatten_model']\n",
      " \n",
      " #Cell\n",
      "diff -ru tmp_lib1/learner.py local/learner.py\n",
      "--- tmp_lib1/learner.py\t2019-09-09 10:28:31.762440551 -0700\n",
      "+++ local/learner.py\t2019-09-09 10:26:52.386513717 -0700\n",
      "@@ -2,7 +2,10 @@\n",
      " \n",
      " __all__ = ['CancelFitException', 'CancelEpochException', 'CancelTrainException', 'CancelValidException',\n",
      "            'CancelBatchException', 'class2attr', 'Callback', 'TrainEvalCallback', 'GatherPredsCallback', 'Learner',\n",
      "-           'VerboseCallback', 'Metric', 'AvgMetric', 'AvgLoss', 'AvgSmoothLoss', 'Recorder']\n",
      "+           'VerboseCallback', 'Metric', 'AvgMetric', 'AvgLoss', 'AvgSmoothLoss', 'Recorder', 'CancelFitException',\n",
      "+           'CancelEpochException', 'CancelTrainException', 'CancelValidException', 'CancelBatchException', 'class2attr',\n",
      "+           'Callback', 'TrainEvalCallback', 'GatherPredsCallback', 'Learner', 'VerboseCallback', 'Metric', 'AvgMetric',\n",
      "+           'AvgLoss', 'AvgSmoothLoss', 'Recorder']\n",
      " \n",
      " #Cell\n",
      " from .torch_basics import *\n",
      "diff -ru tmp_lib1/metrics.py local/metrics.py\n",
      "--- tmp_lib1/metrics.py\t2019-09-09 10:28:32.118440124 -0700\n",
      "+++ local/metrics.py\t2019-09-09 10:26:53.362513480 -0700\n",
      "@@ -5,7 +5,12 @@\n",
      "            'Recall', 'RocAuc', 'Perplexity', 'perplexity', 'accuracy_multi', 'APScoreMulti', 'BrierScoreMulti',\n",
      "            'F1ScoreMulti', 'FBetaMulti', 'HammingLossMulti', 'JaccardMulti', 'MatthewsCorrCoefMulti', 'PrecisionMulti',\n",
      "            'RecallMulti', 'RocAucMulti', 'mse', 'rmse', 'mae', 'msle', 'exp_rmspe', 'ExplainedVariance', 'R2Score',\n",
      "-           'foreground_acc', 'Dice', 'JaccardCoeff']\n",
      "+           'foreground_acc', 'Dice', 'JaccardCoeff', 'AccumMetric', 'skm_to_fastai', 'accuracy', 'error_rate',\n",
      "+           'top_k_accuracy', 'APScore', 'BalancedAccuracy', 'BrierScore', 'CohenKappa', 'F1Score', 'FBeta',\n",
      "+           'HammingLoss', 'Jaccard', 'MatthewsCorrCoef', 'Precision', 'Recall', 'RocAuc', 'Perplexity', 'perplexity',\n",
      "+           'accuracy_multi', 'APScoreMulti', 'BrierScoreMulti', 'F1ScoreMulti', 'FBetaMulti', 'HammingLossMulti',\n",
      "+           'JaccardMulti', 'MatthewsCorrCoefMulti', 'PrecisionMulti', 'RecallMulti', 'RocAucMulti', 'mse', 'rmse',\n",
      "+           'mae', 'msle', 'exp_rmspe', 'ExplainedVariance', 'R2Score', 'foreground_acc', 'Dice', 'JaccardCoeff']\n",
      " \n",
      " #Cell\n",
      " from .torch_basics import *\n",
      "diff -ru tmp_lib1/notebook/core.py local/notebook/core.py\n",
      "--- tmp_lib1/notebook/core.py\t2019-09-09 10:28:32.594439548 -0700\n",
      "+++ local/notebook/core.py\t2019-09-09 10:26:54.838513100 -0700\n",
      "@@ -1,6 +1,7 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/90_notebook_core.ipynb (unless otherwise specified).\n",
      " \n",
      "-__all__ = ['in_ipython', 'IN_IPYTHON', 'in_colab', 'IN_COLAB', 'in_notebook', 'IN_NOTEBOOK', 'DocsTestClass']\n",
      "+__all__ = ['in_ipython', 'IN_IPYTHON', 'in_colab', 'IN_COLAB', 'in_notebook', 'IN_NOTEBOOK', 'DocsTestClass',\n",
      "+           'in_ipython', 'IN_IPYTHON', 'in_colab', 'IN_COLAB', 'in_notebook', 'IN_NOTEBOOK', 'DocsTestClass']\n",
      " \n",
      " #Cell\n",
      " from ..imports import *\n",
      "diff -ru tmp_lib1/notebook/export2html.py local/notebook/export2html.py\n",
      "--- tmp_lib1/notebook/export2html.py\t2019-09-09 10:28:32.818439277 -0700\n",
      "+++ local/notebook/export2html.py\t2019-09-09 10:26:55.966512796 -0700\n",
      "@@ -3,7 +3,10 @@\n",
      " __all__ = ['remove_widget_state', 'hide_cells', 'clean_exports', 'treat_backticks', 'convert_links', 'add_jekyll_notes',\n",
      "            'copy_images', 'remove_hidden', 'find_default_level', 'add_show_docs', 'remove_fake_headers', 'remove_empty',\n",
      "            'get_metadata', 'ExecuteShowDocPreprocessor', 'execute_nb', 'process_cells', 'process_cell', 'notebook_path',\n",
      "-           'convert_nb', 'convert_all', 'convert_post']\n",
      "+           'convert_nb', 'convert_all', 'convert_post', 'remove_widget_state', 'hide_cells', 'clean_exports',\n",
      "+           'treat_backticks', 'convert_links', 'add_jekyll_notes', 'copy_images', 'remove_hidden', 'find_default_level',\n",
      "+           'add_show_docs', 'remove_fake_headers', 'remove_empty', 'get_metadata', 'ExecuteShowDocPreprocessor',\n",
      "+           'execute_nb', 'process_cells', 'process_cell', 'notebook_path', 'convert_nb', 'convert_all', 'convert_post']\n",
      " \n",
      " #Cell\n",
      " from ..imports import *\n",
      "diff -ru tmp_lib1/notebook/export.py local/notebook/export.py\n",
      "--- tmp_lib1/notebook/export.py\t2019-09-09 10:28:32.682439442 -0700\n",
      "+++ local/notebook/export.py\t2019-09-09 10:26:55.286512980 -0700\n",
      "@@ -1,7 +1,9 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/91_notebook_export.ipynb (unless otherwise specified).\n",
      " \n",
      " __all__ = ['read_nb', 'check_re', 'is_export', 'find_default_export', 'export_names', 'extra_add', 'notebook2script',\n",
      "-           'get_name', 'qual_name', 'source_nb', 'script2notebook', 'diff_nb_script']\n",
      "+           'get_name', 'qual_name', 'source_nb', 'script2notebook', 'diff_nb_script', 'read_nb', 'check_re',\n",
      "+           'is_export', 'find_default_export', 'export_names', 'extra_add', 'notebook2script', 'get_name', 'qual_name',\n",
      "+           'source_nb', 'script2notebook', 'diff_nb_script']\n",
      " \n",
      " #Cell\n",
      " from ..imports import *\n",
      "@@ -356,13 +358,30 @@\n",
      "     if (Path.cwd()/'lib.pkl').exists(): os.remove(Path.cwd()/'lib.pkl')\n",
      "     notebook2script(all_fs=True, silent=True, to_pkl=True)\n",
      "     dic = pickle.load(open(Path.cwd()/'lib.pkl', 'rb'))\n",
      "-    os.remove(Path.cwd()/'lib.pkl')\n",
      "     if os.environ.get('IN_TEST',0): return  # don't export if running tests\n",
      "-    for f in (Path.cwd()/folder).glob('**/*.py'):\n",
      "+    for f in Path(folder).glob('**/*.py'):\n",
      "         if f.name not in _manual_mods: _script2notebook(f, dic, silent=silent)\n",
      " \n",
      " #Cell\n",
      "-import subprocess\n",
      "+def _print_diff_py(code1, code2, fname):\n",
      "+    split1,split2 = _split(code1),_split(code2)\n",
      "+    diff = []\n",
      "+    for (i1,f1,c1),(i2,f2,c2) in zip_longest(split1, split2):\n",
      "+        if c1 != c2: diff.append(f'Cell {i1} in {f1}:\\n{c1}\\n\\nExported code from cell {i2} of {f2}:\\n{c2}')\n",
      "+    if len(diff) > 0:\n",
      "+        print(f\"Diff notebook and script in {fname}\")\n",
      "+        print(\"\\n\\n\\n\".join(diff))\n",
      "+\n",
      "+#Cell\n",
      "+def _print_diff_txt(code1, code2, fname):\n",
      "+    split1,split2 = code1.split(\"\\n\"),code2.split(\"\\n\")\n",
      "+    diff = []\n",
      "+    for t1,t2 in zip(split1, split2):\n",
      "+        if t1 is None: diff.append(f'Exported code cell {t2[0]} in {t2[1]}:\\n{c1}\\n\\nExported code from cell {i2} of {f2}:\\n{c2}')\n",
      "+        if c1 != c2: diff.append(f'Cell {i1} in {f1}:\\n{c1}\\n\\nExported code from cell {i2} of {f2}:\\n{c2}')\n",
      "+    if len(diff) > 0:\n",
      "+        print(f\"Diff notebook and script in {fname}\")\n",
      "+        print(\"\\n\\n\\n\".join(diff))\n",
      " \n",
      " #Cell\n",
      " def _print_diff(code1, code2, fname):\n",
      "diff -ru tmp_lib1/notebook/showdoc.py local/notebook/showdoc.py\n",
      "--- tmp_lib1/notebook/showdoc.py\t2019-09-09 10:28:32.754439354 -0700\n",
      "+++ local/notebook/showdoc.py\t2019-09-09 10:26:55.566512904 -0700\n",
      "@@ -2,7 +2,9 @@\n",
      " \n",
      " __all__ = ['is_enum', 'add_pytorch_index', 'is_fastai_module', 'FASTAI_DOCS', 'doc_link', 'add_doc_links',\n",
      "            'get_function_source', 'SOURCE_URL', 'get_source_link', 'FASTAI_NB_DEV', 'source_link', 'type_repr',\n",
      "-           'format_param', 'show_doc', 'md2html', 'doc']\n",
      "+           'format_param', 'show_doc', 'md2html', 'doc', 'is_enum', 'add_pytorch_index', 'is_fastai_module',\n",
      "+           'FASTAI_DOCS', 'doc_link', 'add_doc_links', 'get_function_source', 'SOURCE_URL', 'get_source_link',\n",
      "+           'FASTAI_NB_DEV', 'source_link', 'type_repr', 'format_param', 'show_doc', 'md2html', 'doc']\n",
      " \n",
      " #Cell\n",
      " from ..imports import *\n",
      "diff -ru tmp_lib1/optimizer.py local/optimizer.py\n",
      "--- tmp_lib1/optimizer.py\t2019-09-09 10:28:31.614440729 -0700\n",
      "+++ local/optimizer.py\t2019-09-09 10:26:52.158513772 -0700\n",
      "@@ -2,7 +2,9 @@\n",
      " \n",
      " __all__ = ['Optimizer', 'sgd_step', 'weight_decay', 'l2_reg', 'average_grad', 'average_sqr_grad', 'momentum_step',\n",
      "            'SGD', 'rms_prop_step', 'RMSProp', 'step_stat', 'adam_step', 'Adam', 'larc_layer_lr', 'larc_step', 'Larc',\n",
      "-           'lamb_step', 'Lamb']\n",
      "+           'lamb_step', 'Lamb', 'Optimizer', 'sgd_step', 'weight_decay', 'l2_reg', 'average_grad', 'average_sqr_grad',\n",
      "+           'momentum_step', 'SGD', 'rms_prop_step', 'RMSProp', 'step_stat', 'adam_step', 'Adam', 'larc_layer_lr',\n",
      "+           'larc_step', 'Larc', 'lamb_step', 'Lamb']\n",
      " \n",
      " #Cell\n",
      " from .torch_basics import *\n",
      "diff -ru tmp_lib1/script.py local/script.py\n",
      "--- tmp_lib1/script.py\t2019-09-09 10:28:30.642441893 -0700\n",
      "+++ local/script.py\t2019-09-09 10:26:50.466514156 -0700\n",
      "@@ -1,6 +1,6 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/01b_script.ipynb (unless otherwise specified).\n",
      " \n",
      "-__all__ = ['Param', 'anno_parser', 'call_parse']\n",
      "+__all__ = ['Param', 'anno_parser', 'call_parse', 'Param', 'anno_parser', 'call_parse']\n",
      " \n",
      " #Cell\n",
      " from .imports import *\n",
      "diff -ru tmp_lib1/tabular/core.py local/tabular/core.py\n",
      "--- tmp_lib1/tabular/core.py\t2019-09-09 10:28:32.430439746 -0700\n",
      "+++ local/tabular/core.py\t2019-09-09 10:26:54.578513168 -0700\n",
      "@@ -1,7 +1,8 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/40_tabular_core.ipynb (unless otherwise specified).\n",
      " \n",
      " __all__ = ['Tabular', 'TabularProc', 'Categorify', 'Normalize', 'FillStrategy', 'FillMissing', 'process_df',\n",
      "-           'ReadTabBatch', 'TabDataLoader']\n",
      "+           'ReadTabBatch', 'TabDataLoader', 'Tabular', 'TabularProc', 'Categorify', 'Normalize', 'FillStrategy',\n",
      "+           'FillMissing', 'process_df', 'ReadTabBatch', 'TabDataLoader']\n",
      " \n",
      " #Cell\n",
      " from ..torch_basics import *\n",
      "diff -ru tmp_lib1/test.py local/test.py\n",
      "--- tmp_lib1/test.py\t2019-09-09 10:28:30.222442393 -0700\n",
      "+++ local/test.py\t2019-09-09 10:26:49.946514269 -0700\n",
      "@@ -1,7 +1,9 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/00_test.ipynb (unless otherwise specified).\n",
      " \n",
      " __all__ = ['test_fail', 'test', 'nequals', 'test_eq', 'test_eq_type', 'test_ne', 'is_close', 'test_close', 'test_is',\n",
      "-           'test_shuffled', 'test_stdout', 'TEST_IMAGE', 'test_fig_exists']\n",
      "+           'test_shuffled', 'test_stdout', 'TEST_IMAGE', 'test_fig_exists', 'test_fail', 'test', 'nequals', 'test_eq',\n",
      "+           'test_eq_type', 'test_ne', 'is_close', 'test_close', 'test_is', 'test_shuffled', 'test_stdout', 'TEST_IMAGE',\n",
      "+           'test_fig_exists']\n",
      " \n",
      " #Cell\n",
      " from .imports import *\n",
      "diff -ru tmp_lib1/text/core.py local/text/core.py\n",
      "--- tmp_lib1/text/core.py\t2019-09-09 10:28:32.246439969 -0700\n",
      "+++ local/text/core.py\t2019-09-09 10:26:53.866513353 -0700\n",
      "@@ -3,7 +3,11 @@\n",
      " __all__ = ['ProcessPoolExecutor', 'parallel', 'parallel_gen', 'spec_add_spaces', 'rm_useless_spaces', 'replace_rep',\n",
      "            'replace_wrep', 'fix_html', 'replace_all_caps', 'replace_maj', 'lowercase', 'replace_space', 'BaseTokenizer',\n",
      "            'SpacyTokenizer', 'apply_rules', 'TokenizeBatch', 'tokenize1', 'parallel_tokenize', 'tokenize_folder',\n",
      "-           'tokenize_df', 'tokenize_csv', 'SentencePieceTokenizer']\n",
      "+           'tokenize_df', 'tokenize_csv', 'SentencePieceTokenizer', 'ProcessPoolExecutor', 'parallel', 'parallel_gen',\n",
      "+           'spec_add_spaces', 'rm_useless_spaces', 'replace_rep', 'replace_wrep', 'fix_html', 'replace_all_caps',\n",
      "+           'replace_maj', 'lowercase', 'replace_space', 'BaseTokenizer', 'SpacyTokenizer', 'apply_rules',\n",
      "+           'TokenizeBatch', 'tokenize1', 'parallel_tokenize', 'tokenize_folder', 'tokenize_df', 'tokenize_csv',\n",
      "+           'SentencePieceTokenizer']\n",
      " \n",
      " #Cell\n",
      " from ..imports import *\n",
      "diff -ru tmp_lib1/text/data.py local/text/data.py\n",
      "--- tmp_lib1/text/data.py\t2019-09-09 10:28:32.282439925 -0700\n",
      "+++ local/text/data.py\t2019-09-09 10:26:53.914513341 -0700\n",
      "@@ -1,6 +1,7 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/31_text_data.ipynb (unless otherwise specified).\n",
      " \n",
      "-__all__ = ['make_vocab', 'TensorText', 'Numericalize', 'LMDataLoader', 'pad_collate']\n",
      "+__all__ = ['make_vocab', 'TensorText', 'Numericalize', 'LMDataLoader', 'pad_collate', 'make_vocab', 'TensorText',\n",
      "+           'Numericalize', 'LMDataLoader', 'pad_collate']\n",
      " \n",
      " #Cell\n",
      " from ..imports import *\n",
      "diff -ru tmp_lib1/text/models/awdlstm.py local/text/models/awdlstm.py\n",
      "--- tmp_lib1/text/models/awdlstm.py\t2019-09-09 10:28:32.314439887 -0700\n",
      "+++ local/text/models/awdlstm.py\t2019-09-09 10:26:54.066513302 -0700\n",
      "@@ -2,7 +2,9 @@\n",
      " \n",
      " __all__ = ['dropout_mask', 'RNNDropout', 'WeightDropout', 'EmbeddingDropout', 'AWD_LSTM', 'awd_lstm_lm_split',\n",
      "            'awd_lstm_lm_config', 'awd_lstm_clas_split', 'awd_lstm_clas_config', 'AWD_QRNN', 'awd_qrnn_lm_config',\n",
      "-           'awd_qrnn_clas_config']\n",
      "+           'awd_qrnn_clas_config', 'dropout_mask', 'RNNDropout', 'WeightDropout', 'EmbeddingDropout', 'AWD_LSTM',\n",
      "+           'awd_lstm_lm_split', 'awd_lstm_lm_config', 'awd_lstm_clas_split', 'awd_lstm_clas_config', 'AWD_QRNN',\n",
      "+           'awd_qrnn_lm_config', 'awd_qrnn_clas_config']\n",
      " \n",
      " #Cell\n",
      " from ...imports import *\n",
      "diff -ru tmp_lib1/text/models/core.py local/text/models/core.py\n",
      "--- tmp_lib1/text/models/core.py\t2019-09-09 10:28:32.334439862 -0700\n",
      "+++ local/text/models/core.py\t2019-09-09 10:26:54.126513286 -0700\n",
      "@@ -1,6 +1,7 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/33_test_models_core.ipynb (unless otherwise specified).\n",
      " \n",
      "-__all__ = ['LinearDecoder', 'SequentialRNN', 'get_language_model']\n",
      "+__all__ = ['LinearDecoder', 'SequentialRNN', 'get_language_model', 'LinearDecoder', 'SequentialRNN',\n",
      "+           'get_language_model']\n",
      " \n",
      " #Cell\n",
      " from ...imports import *\n",
      "diff -ru tmp_lib1/text/models/qrnn.py local/text/models/qrnn.py\n",
      "--- tmp_lib1/text/models/qrnn.py\t2019-09-09 10:28:32.386439799 -0700\n",
      "+++ local/text/models/qrnn.py\t2019-09-09 10:26:54.426513208 -0700\n",
      "@@ -1,6 +1,7 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/36_text_models_qrnn.ipynb (unless otherwise specified).\n",
      " \n",
      "-__all__ = ['dispatch_cuda', 'forget_mult_CPU', 'ForgetMultGPU', 'QRNNLayer', 'QRNN']\n",
      "+__all__ = ['dispatch_cuda', 'forget_mult_CPU', 'ForgetMultGPU', 'QRNNLayer', 'QRNN', 'dispatch_cuda', 'forget_mult_CPU',\n",
      "+           'ForgetMultGPU', 'QRNNLayer', 'QRNN']\n",
      " \n",
      " #Cell\n",
      " from ...imports import *\n",
      "diff -ru tmp_lib1/torch_core.py local/torch_core.py\n",
      "--- tmp_lib1/torch_core.py\t2019-09-09 10:28:32.090440157 -0700\n",
      "+++ local/torch_core.py\t2019-09-09 10:26:53.022513564 -0700\n",
      "@@ -3,7 +3,11 @@\n",
      " __all__ = ['tensor', 'set_seed', 'TensorBase', 'concat', 'Chunks', 'apply', 'to_detach', 'to_half', 'to_float',\n",
      "            'default_device', 'to_device', 'to_cpu', 'to_np', 'item_find', 'find_device', 'find_bs', 'Module', 'one_hot',\n",
      "            'one_hot_decode', 'trainable_params', 'bn_bias_params', 'make_cross_image', 'show_title', 'show_image',\n",
      "-           'show_titled_image', 'show_image_batch', 'flatten_check']\n",
      "+           'show_titled_image', 'show_image_batch', 'flatten_check', 'tensor', 'set_seed', 'TensorBase', 'concat',\n",
      "+           'Chunks', 'apply', 'to_detach', 'to_half', 'to_float', 'default_device', 'to_device', 'to_cpu', 'to_np',\n",
      "+           'item_find', 'find_device', 'find_bs', 'Module', 'one_hot', 'one_hot_decode', 'trainable_params',\n",
      "+           'bn_bias_params', 'make_cross_image', 'show_title', 'show_image', 'show_titled_image', 'show_image_batch',\n",
      "+           'flatten_check']\n",
      " \n",
      " #Cell\n",
      " from .test import *\n",
      "diff -ru tmp_lib1/utils/test.py local/utils/test.py\n",
      "--- tmp_lib1/utils/test.py\t2019-09-09 10:28:32.834439258 -0700\n",
      "+++ local/utils/test.py\t2019-09-09 10:26:56.026512779 -0700\n",
      "@@ -1,6 +1,6 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/95_utils_test.ipynb (unless otherwise specified).\n",
      " \n",
      "-__all__ = ['synth_data', 'RegModel', 'synth_learner']\n",
      "+__all__ = ['synth_data', 'RegModel', 'synth_learner', 'synth_data', 'RegModel', 'synth_learner']\n",
      " \n",
      " #Cell\n",
      " from ..torch_basics import *\n",
      "diff -ru tmp_lib1/vision/augment.py local/vision/augment.py\n",
      "--- tmp_lib1/vision/augment.py\t2019-09-09 10:28:31.430440950 -0700\n",
      "+++ local/vision/augment.py\t2019-09-09 10:26:51.682513883 -0700\n",
      "@@ -3,7 +3,11 @@\n",
      " __all__ = ['RandTransform', 'PILFlip', 'PILDihedral', 'clip_remove_empty', 'CropPad', 'RandomCrop', 'Resize',\n",
      "            'RandomResizedCrop', 'AffineCoordTfm', 'affine_mat', 'mask_tensor', 'flip_mat', 'Flip', 'dihedral_mat',\n",
      "            'Dihedral', 'rotate_mat', 'Rotate', 'zoom_mat', 'Zoom', 'find_coeffs', 'apply_perspective', 'Warp', 'logit',\n",
      "-           'LightingTfm', 'Brightness', 'Contrast', 'setup_aug_tfms', 'aug_transforms']\n",
      "+           'LightingTfm', 'Brightness', 'Contrast', 'setup_aug_tfms', 'aug_transforms', 'RandTransform', 'PILFlip',\n",
      "+           'PILDihedral', 'clip_remove_empty', 'CropPad', 'RandomCrop', 'Resize', 'RandomResizedCrop', 'AffineCoordTfm',\n",
      "+           'affine_mat', 'mask_tensor', 'flip_mat', 'Flip', 'dihedral_mat', 'Dihedral', 'rotate_mat', 'Rotate',\n",
      "+           'zoom_mat', 'Zoom', 'find_coeffs', 'apply_perspective', 'Warp', 'logit', 'LightingTfm', 'Brightness',\n",
      "+           'Contrast', 'setup_aug_tfms', 'aug_transforms']\n",
      " \n",
      " #Cell\n",
      " from ..torch_basics import *\n",
      "diff -ru tmp_lib1/vision/core.py local/vision/core.py\n",
      "--- tmp_lib1/vision/core.py\t2019-09-09 10:28:31.042441416 -0700\n",
      "+++ local/vision/core.py\t2019-09-09 10:26:51.134514008 -0700\n",
      "@@ -2,7 +2,10 @@\n",
      " \n",
      " __all__ = ['Image', 'ToTensor', 'n_px', 'shape', 'aspect', 'load_image', 'PILBase', 'PILImage', 'PILImageBW', 'PILMask',\n",
      "            'TensorPoint', 'get_annotations', 'BBox', 'TensorBBox', 'image2byte', 'encodes', 'encodes', 'encodes',\n",
      "-           'PointScaler', 'BBoxScaler', 'BBoxCategorize', 'bb_pad']\n",
      "+           'PointScaler', 'BBoxScaler', 'BBoxCategorize', 'bb_pad', 'Image', 'ToTensor', 'n_px', 'shape', 'aspect',\n",
      "+           'load_image', 'PILBase', 'PILImage', 'PILImageBW', 'PILMask', 'TensorPoint', 'get_annotations', 'BBox',\n",
      "+           'TensorBBox', 'image2byte', 'encodes', 'encodes', 'encodes', 'PointScaler', 'BBoxScaler', 'BBoxCategorize',\n",
      "+           'bb_pad']\n",
      " \n",
      " #Cell\n",
      " from ..torch_basics import *\n",
      "diff -ru tmp_lib1/vision/models/xresnet.py local/vision/models/xresnet.py\n",
      "--- tmp_lib1/vision/models/xresnet.py\t2019-09-09 10:28:31.554440801 -0700\n",
      "+++ local/vision/models/xresnet.py\t2019-09-09 10:26:52.022513804 -0700\n",
      "@@ -1,6 +1,7 @@\n",
      " #AUTOGENERATED! DO NOT EDIT! File to edit: dev/11a_vision_models_xresnet.ipynb (unless otherwise specified).\n",
      " \n",
      "-__all__ = ['init_cnn', 'XResNet', 'xresnet18', 'xresnet34', 'xresnet50', 'xresnet101', 'xresnet152']\n",
      "+__all__ = ['init_cnn', 'XResNet', 'xresnet18', 'xresnet34', 'xresnet50', 'xresnet101', 'xresnet152', 'init_cnn',\n",
      "+           'XResNet', 'xresnet18', 'xresnet34', 'xresnet50', 'xresnet101', 'xresnet152']\n",
      " \n",
      " #Cell\n",
      " from ...torch_basics import *\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diff_nb_script()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_test.ipynb.\n",
      "Converted 01_core.ipynb.\n",
      "Converted 01a_torch_core.ipynb.\n",
      "Converted 01b_script.ipynb.\n",
      "Converted 01c_dataloader.ipynb.\n",
      "Converted 02_data_transforms.ipynb.\n",
      "Converted 03_data_pipeline.ipynb.\n",
      "Converted 05_data_core.ipynb.\n",
      "Converted 06_data_source.ipynb.\n",
      "Converted 07_vision_core.ipynb.\n",
      "Converted 08_pets_tutorial.ipynb.\n",
      "Converted 09_vision_augment.ipynb.\n",
      "Converted 11_layers.ipynb.\n",
      "Converted 11a_vision_models_xresnet.ipynb.\n",
      "Converted 12_optimizer.ipynb.\n",
      "Converted 13_learner.ipynb.\n",
      "Converted 14_callback_schedule.ipynb.\n",
      "Converted 15_callback_hook.ipynb.\n",
      "Converted 16_callback_progress.ipynb.\n",
      "Converted 17_callback_tracker.ipynb.\n",
      "Converted 18_callback_fp16.ipynb.\n",
      "Converted 19_callback_mixup.ipynb.\n",
      "Converted 20_metrics.ipynb.\n",
      "Converted 21_tutorial_imagenette.ipynb.\n",
      "Converted 30_text_core.ipynb.\n",
      "Converted 31_text_data.ipynb.\n",
      "Converted 32_text_models_awdlstm.ipynb.\n",
      "Converted 33_test_models_core.ipynb.\n",
      "Converted 34_callback_rnn.ipynb.\n",
      "Converted 35_tutorial_wikitext.ipynb.\n",
      "Converted 36_text_models_qrnn.ipynb.\n",
      "Converted 40_tabular_core.ipynb.\n",
      "Converted 41_tabular_model.ipynb.\n",
      "Converted 50_data_block.ipynb.\n",
      "Converted 90_notebook_core.ipynb.\n",
      "Converted 91_notebook_export.ipynb.\n",
      "Converted 92_notebook_showdoc.ipynb.\n",
      "Converted 93_notebook_export2html.ipynb.\n",
      "Converted 94_index.ipynb.\n",
      "Converted 95_utils_test.ipynb.\n",
      "Converted 96_data_external.ipynb.\n",
      "Converted notebook2jekyll.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "notebook2script(all_fs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
