{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/usr/local/google/home/jekbradbury/fastai_docs/dev_swift/FastaiNotebook_06_cuda\")\n",
      "\t\tFastaiNotebook_06_cuda\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmp65juqaje\n",
      "Fetching https://github.com/mxcl/Path.swift\n",
      "Fetching https://github.com/JustHTTP/Just\n",
      "Completed resolution in 3.80s\n",
      "Cloning https://github.com/mxcl/Path.swift\n",
      "Resolving https://github.com/mxcl/Path.swift at 0.16.2\n",
      "Cloning https://github.com/JustHTTP/Just\n",
      "Resolving https://github.com/JustHTTP/Just at 0.7.1\n",
      "Compile Swift Module 'Just' (1 sources)\n",
      "Compile Swift Module 'Path' (9 sources)\n",
      "Compile Swift Module 'FastaiNotebook_06_cuda' (10 sources)\n",
      "Compile Swift Module 'jupyterInstalledPackages' (1 sources)\n",
      "Linking ./.build/x86_64-unknown-linux/debug/libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Loading library...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install '.package(path: \"$cwd/FastaiNotebook_06_cuda\")' FastaiNotebook_06_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('inline', 'module://ipykernel.pylab.backend_inline')\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import FastaiNotebook_06_cuda\n",
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "import Path\n",
    "import TensorFlow\n",
    "import Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "let data = mnistDataBunch(flat: false, bs: 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model without batch normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: [0.31861317, 0.9038]                                                   \n",
      "5498.708262 ms                                                                \n"
     ]
    }
   ],
   "source": [
    "func optFunc(_ model: CnnModel) -> SGD<CnnModel> { return SGD(for: model, learningRate: 0.4) }\n",
    "func modelInit() -> CnnModel { return CnnModel(channelIn: 1, nOut: 10, filters: [8, 16, 32, 32]) }\n",
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.addDelegates([learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std),\n",
    "                      learner.makeAddChannel()])\n",
    "time { try! learner.fit(1) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by building our own batchnorm layer from scratch. Eventually we want something like this to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlmostBatchNorm<Scalar: TensorFlowFloatingPoint> { // : Layer\n",
    "    // Configuration hyperparameters\n",
    "    let momentum, epsilon: Scalar\n",
    "    // Running statistics\n",
    "    var runningMean, runningVariance: Tensor<Scalar>\n",
    "    // Trainable parameters\n",
    "    var scale, offset: Tensor<Scalar>\n",
    "    \n",
    "    init(featureCount: Int, momentum: Scalar = 0.9, epsilon: Scalar = 1e-5) {\n",
    "        (self.momentum, self.epsilon) = (momentum, epsilon)\n",
    "        (scale, offset) = (Tensor(ones: [featureCount]), Tensor(zeros: [featureCount]))\n",
    "        (runningMean, runningVariance) = (Tensor(0), Tensor(1))\n",
    "    }\n",
    "\n",
    "    func call(_ input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        let mean, variance: Tensor<Scalar>\n",
    "        switch Context.local.learningPhase {\n",
    "        case .training:\n",
    "            mean = input.mean(alongAxes: [0, 1, 2])\n",
    "            variance = input.variance(alongAxes: [0, 1, 2])\n",
    "            runningMean += (mean - runningMean) * (1 - momentum)\n",
    "            runningVariance += (variance - runningVariance) * (1 - momentum)\n",
    "        case .inference:\n",
    "            (mean, variance) = (runningMean, runningVariance)\n",
    "        }\n",
    "        let normalizer = rsqrt(variance + epsilon) * scale\n",
    "        return (input - mean) * normalizer + offset\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are some automatic differentiation limitations (lack of support for classes and control flow) that make this impossible for now, so we'll need a few workarounds. A `Reference` will let us update running statistics without making the layer a class or declaring the `applied` method `mutating`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public class Reference<T> {\n",
    "    public var value: T\n",
    "    public init(_ value: T) { self.value = value }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet will let us differentiate a layer's `applied` method if it's composed of training and inference implementations that are each differentiable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public protocol LearningPhaseDependent: FALayer {\n",
    "    @differentiable func forwardTraining(to input: Input) -> Output\n",
    "    @differentiable func forwardInference(to input: Input) -> Output\n",
    "}\n",
    "\n",
    "extension LearningPhaseDependent {\n",
    "    public func forward(_ input: Input) -> Output {\n",
    "        switch Context.local.learningPhase {\n",
    "        case .training: return forwardTraining(to: input)\n",
    "        case .inference: return forwardInference(to: input)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    @differentiating(forward)\n",
    "    func gradForward(_ input: Input) ->\n",
    "        (value: Output, pullback: (Output.CotangentVector) ->\n",
    "            (Self.CotangentVector, Input.CotangentVector)) {\n",
    "        switch Context.local.learningPhase {\n",
    "        case .training: return valueWithPullback(at: input) { $0.forwardTraining(to: $1) }\n",
    "        case .inference: return valueWithPullback(at: input) { $0.forwardInference(to: $1) }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement a BatchNorm that we can use in our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public protocol Norm: FALayer where Input == Tensor<Float>, Output == Tensor<Float> {\n",
    "    init(featureCount: Int, epsilon: Float)\n",
    "}\n",
    "\n",
    "public struct FABatchNorm: LearningPhaseDependent, Norm {\n",
    "    // Configuration hyperparameters\n",
    "    @noDerivative var momentum, epsilon: Float\n",
    "    // Running statistics\n",
    "    @noDerivative let runningMean, runningVariance: Reference<Tensor<Float>>\n",
    "    // Trainable parameters\n",
    "    public var scale, offset: Tensor<Float>\n",
    "    \n",
    "    public init(featureCount: Int, momentum: Float, epsilon: Float = 1e-5) {\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.scale = Tensor(ones: [featureCount])\n",
    "        self.offset = Tensor(zeros: [featureCount])\n",
    "        self.runningMean = Reference(Tensor(0))\n",
    "        self.runningVariance = Reference(Tensor(1))\n",
    "    }\n",
    "    \n",
    "    public init(featureCount: Int, epsilon: Float = 1e-5) {\n",
    "        self.init(featureCount: featureCount, momentum: 0.9, epsilon: epsilon)\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    public func forwardTraining(to input: Tensor<Float>) -> Tensor<Float> {\n",
    "        let mean = input.mean(alongAxes: [0, 1, 2])\n",
    "        let variance = input.variance(alongAxes: [0, 1, 2])\n",
    "        runningMean.value += (mean - runningMean.value) * (1 - momentum)\n",
    "        runningVariance.value += (variance - runningVariance.value) * (1 - momentum)\n",
    "        let normalizer = rsqrt(variance + epsilon) * scale\n",
    "        return (input - mean) * normalizer + offset\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func forwardInference(to input: Tensor<Float>) -> Tensor<Float> {\n",
    "        let (mean, variance) = (runningMean.value, runningVariance.value)\n",
    "        let normalizer = rsqrt(variance + epsilon) * scale\n",
    "        return (input - mean) * normalizer + offset\n",
    "    }\n",
    "    \n",
    "    // Things that should probably be synthesized/inherited, but aren't\n",
    "    @noDerivative public var delegate: LayerDelegate<Output> = LayerDelegate()\n",
    "    public typealias Input = Tensor<Float>\n",
    "    public typealias Output = Tensor<Float>\n",
    "    @differentiable\n",
    "    public func call(_ input: Input) -> Output {\n",
    "        let activation = forward(input)\n",
    "        delegate.didProduceActivation(activation)\n",
    "        return activation\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct ConvNorm<NormType: Norm & FALayer>: FALayer\n",
    "    where NormType.AllDifferentiableVariables == NormType.CotangentVector {\n",
    "    public var conv: FANoBiasConv2D<Float>\n",
    "    public var norm: NormType\n",
    "    \n",
    "    public init(_ cIn: Int, _ cOut: Int, ks: Int = 3, stride: Int = 2){\n",
    "        self.conv = FANoBiasConv2D(\n",
    "            filterShape: (ks, ks, cIn, cOut), \n",
    "            strides: (stride, stride), \n",
    "            padding: .same, \n",
    "            activation: relu)\n",
    "        self.norm = NormType(featureCount: cOut, epsilon: 1e-5)\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    public func forward(_ input: Tensor<Float>) -> Tensor<Float> {\n",
    "        return norm(conv(input))\n",
    "    }\n",
    "\n",
    "    // Things that should probably be synthesized/inherited, but aren't\n",
    "    @noDerivative public var delegate: LayerDelegate<Output> = LayerDelegate()\n",
    "    @differentiable\n",
    "    public func call(_ input: Tensor<Float>) -> Tensor<Float> {\n",
    "        let activation = forward(input)\n",
    "        delegate.didProduceActivation(activation)\n",
    "        return activation\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct CnnModelNormed<NormType: Norm & FALayer>: FALayer\n",
    "    where NormType.AllDifferentiableVariables == NormType.CotangentVector {\n",
    "    public var convs: [ConvNorm<NormType>]\n",
    "    public var pool = FAAdaptiveAvgPool2D<Float>()\n",
    "    public var flatten = Flatten<Float>()\n",
    "    public var linear: FADense<Float>\n",
    "    \n",
    "    public init(channelIn: Int, nOut: Int, filters: [Int]){\n",
    "        convs = []\n",
    "        let allFilters = [channelIn] + filters\n",
    "        for i in 0..<filters.count { convs.append(ConvNorm<NormType>(allFilters[i], allFilters[i+1])) }\n",
    "        linear = FADense<Float>(inputSize: filters.last!, outputSize: nOut)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func forward(_ input: TF) -> TF {\n",
    "        return input.sequenced(through: convs, pool, flatten, linear)\n",
    "    }\n",
    "    \n",
    "    // Things that should probably be synthesized/inherited, but aren't\n",
    "    @noDerivative public var delegate: LayerDelegate<Output> = LayerDelegate()\n",
    "    @differentiable\n",
    "    public func call(_ input: Tensor<Float>) -> Tensor<Float> {\n",
    "        let activation = forward(input)\n",
    "        delegate.didProduceActivation(activation)\n",
    "        return activation\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: [0.24306941, 0.9217]                                                   \n",
      "5116.78742 ms                                                                 \n"
     ]
    }
   ],
   "source": [
    "func optFunc(_ model: CnnModelNormed<FABatchNorm>) -> SGD<CnnModelNormed<FABatchNorm>> { return SGD(for: model, learningRate: 0.4) }\n",
    "func modelInit() -> CnnModelNormed<FABatchNorm> { return CnnModelNormed<FABatchNorm>(channelIn: 1, nOut: 10, filters: [8, 16, 32, 32]) }\n",
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.addDelegates([learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std),\n",
    "                      learner.makeAddChannel()])\n",
    "time { try! learner.fit(1) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's benchmark this batchnorm implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "func benchmark<NormType: Norm>(norm: NormType) {\n",
    "    let input = Tensor<Float>(randomUniform: [64, 28, 28, 32])\n",
    "    Context.local.learningPhase = .inference\n",
    "    print(\"inference:\")\n",
    "    time { let output = norm(input) }\n",
    "    Context.local.learningPhase = .training\n",
    "    print(\"training forward:\")\n",
    "    time { let output = norm(input) }\n",
    "    print(\"training backward:\")\n",
    "    let pb = pullback(at: input) { x in norm(x) }\n",
    "    time { let gradInput = pb(input) }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference:\r\n",
      "2.351998 ms\r\n",
      "training forward:\r\n",
      "6.613989 ms\r\n",
      "training backward:\r\n",
      "26.235124 ms\r\n"
     ]
    }
   ],
   "source": [
    "benchmark(norm: FABatchNorm(featureCount: 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes, that's pretty bad. Luckily, TensorFlow has a built-in fused batchnorm layer. Let's see how the performance looks for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training forward:\n",
      "8.70998 ms\n",
      "training backward:\n",
      "14.752692 ms\n"
     ]
    }
   ],
   "source": [
    "let input = Tensor<Float>(randomUniform: [64, 28, 28, 32])\n",
    "let norm = FABatchNorm(featureCount: 32)\n",
    "print(\"training forward:\")\n",
    "time {\n",
    "    let ret = Raw.fusedBatchNormV2(\n",
    "        input, scale: norm.scale, offset: norm.offset, \n",
    "        mean: Tensor<Float>([] as [Float]), variance: Tensor<Float>([] as [Float]), \n",
    "        epsilon: Double(norm.epsilon))\n",
    "    let output = ret.y\n",
    "}\n",
    "let bnresult = Raw.fusedBatchNormV2(\n",
    "    input, scale: norm.scale, offset: norm.offset, \n",
    "    mean: Tensor<Float>([] as [Float]), variance: Tensor<Float>([] as [Float]), \n",
    "    epsilon: Double(norm.epsilon))\n",
    "print(\"training backward:\")\n",
    "time {\n",
    "    let res = Raw.fusedBatchNormGradV2(\n",
    "        yBackprop: input, input, scale: Tensor<Float>(norm.scale), \n",
    "        reserveSpace1: bnresult.reserveSpace1, \n",
    "        reserveSpace2: bnresult.reserveSpace2, \n",
    "        epsilon: Double(norm.epsilon))\n",
    "    let gradInput = res.xBackprop\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct PullbackArgs<T : TensorGroup, U : TensorGroup> : TensorGroup {\n",
    "    let input: T\n",
    "    let cotangent: U\n",
    "}\n",
    "\n",
    "class CompiledFunction<Input: Differentiable & TensorGroup, Output: Differentiable & TensorGroup> {\n",
    "    let f: @differentiable (Input) -> Output\n",
    "    init(_ f: @escaping @differentiable (Input) -> Output) {\n",
    "        self.f = f\n",
    "    }\n",
    "}\n",
    "\n",
    "func xlaCompiled<T : Differentiable & TensorGroup, U : Differentiable & TensorGroup>(\n",
    "    _ fn: @escaping @differentiable (T) -> U) -> CompiledFunction<T, U>\n",
    "    where T.CotangentVector : TensorGroup, U.CotangentVector : TensorGroup {\n",
    "    let xlaCompiledFn: (T) -> U = _graph(fn, useXLA: true)\n",
    "    let xlaCompiledPullback = _graph(\n",
    "        { (pbArgs: PullbackArgs<T, U.CotangentVector>) in\n",
    "            pullback(at: pbArgs.input, in: fn)(pbArgs.cotangent) },\n",
    "        useXLA: true\n",
    "    )\n",
    "    return CompiledFunction(differentiableFunction { x in\n",
    "        (value: xlaCompiledFn(x), pullback: { v in\n",
    "            xlaCompiledPullback(PullbackArgs(input: x, cotangent: v))})\n",
    "    })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct TrainingKernelInput: TensorGroup, Differentiable, AdditiveArithmetic {\n",
    "    let input: Tensor<Float>\n",
    "    let scale: Tensor<Float>\n",
    "    let offset: Tensor<Float>\n",
    "    let runningMean: Tensor<Float>\n",
    "    let runningVariance: Tensor<Float>\n",
    "    let momentum: Tensor<Float>\n",
    "    let epsilon: Tensor<Float>\n",
    "}\n",
    "\n",
    "struct TrainingKernelOutput: TensorGroup, Differentiable, AdditiveArithmetic {\n",
    "    let normalized: Tensor<Float>\n",
    "    let newRunningMean: Tensor<Float>\n",
    "    let newRunningVariance: Tensor<Float>\n",
    "}\n",
    "\n",
    "@differentiable\n",
    "func trainingKernel(_ input: TrainingKernelInput) -> TrainingKernelOutput {\n",
    "    let mean = input.input.mean(alongAxes: [0, 1, 2])\n",
    "    let variance = input.input.variance(alongAxes: [0, 1, 2])\n",
    "    let invMomentum = Tensor<Float>(1) - input.momentum\n",
    "    let newRunningMean = input.runningMean * input.momentum + mean * invMomentum\n",
    "    let newRunningVariance = input.runningVariance * input.momentum + variance * invMomentum\n",
    "    let normalizer = rsqrt(variance + input.epsilon) * input.scale\n",
    "    let normalized = (input.input - mean) * normalizer + input.offset\n",
    "    return TrainingKernelOutput(\n",
    "        normalized: normalized,\n",
    "        newRunningMean: newRunningMean,\n",
    "        newRunningVariance: newRunningVariance\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training forward:\n",
      "6.951443 ms\n",
      "training backward:\n",
      "52.754287 ms\n"
     ]
    }
   ],
   "source": [
    "let input = Tensor<Float>(randomUniform: [64, 28, 28, 32])\n",
    "let norm = FABatchNorm(featureCount: 32)\n",
    "let compiledTrainingKernel = xlaCompiled(trainingKernel)\n",
    "let kernelInput = TrainingKernelInput(\n",
    "    input: input,\n",
    "    scale: norm.scale,\n",
    "    offset: norm.offset,\n",
    "    runningMean: norm.runningMean.value,\n",
    "    runningVariance: norm.runningVariance.value,\n",
    "    momentum: Tensor(norm.momentum),\n",
    "    epsilon: Tensor(norm.epsilon))\n",
    "print(\"training forward:\")\n",
    "time {\n",
    "    let ret = compiledTrainingKernel.f(kernelInput)\n",
    "    let output = ret.normalized\n",
    "}\n",
    "let pb = pullback(at: kernelInput) { x in compiledTrainingKernel.f(x) }\n",
    "let kernelOutput = compiledTrainingKernel.f(kernelInput)\n",
    "print(\"training backward:\")\n",
    "time {\n",
    "    let res = pb(kernelOutput)\n",
    "    let gradInput = res.input\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
