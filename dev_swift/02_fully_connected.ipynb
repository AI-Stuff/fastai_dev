{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/home/ubuntu/dev_swift/FastaiNotebook_01a_fastai_layers\")\n",
      "\t\tFastaiNotebook_01a_fastai_layers\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmp06qvc72w/swift-install\n",
      "Updating https://github.com/latenitesoft/NotebookExport\n",
      "Updating https://github.com/mxcl/Path.swift\n",
      "Updating https://github.com/JustHTTP/Just\n",
      "Completed resolution in 1.68s\n",
      "Compile Swift Module 'FastaiNotebook_01a_fastai_layers' (3 sources)\n",
      "Compile Swift Module 'jupyterInstalledPackages' (1 sources)\n",
      "Linking ./.build/x86_64-unknown-linux/debug/libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install-location $cwd/swift-install\n",
    "%install '.package(path: \"$cwd/FastaiNotebook_01a_fastai_layers\")' FastaiNotebook_01a_fastai_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "import Path\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebook_01a_fastai_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forward and backward passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typing `Tensor<Float>` all the time is tedious. The S4TF team expects to make `Float` be the default so we can just say `Tensor`.  Until that happens though, we can define our own alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public typealias TF=Tensor<Float>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to normalize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func normalize(_ x:TF, mean:TF, std:TF) -> TF {\n",
    "    return (x-mean)/std\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var (xTrain, yTrain, xValid, yValid) = loadMNIST(path: mnistPath, flat: true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the training and validation sets with the training set statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13066047 0.3081079\r\n"
     ]
    }
   ],
   "source": [
    "let trainMean = xTrain.mean()\n",
    "let trainStd  = xTrain.std()\n",
    "print(trainMean, trainStd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = normalize(xTrain, mean: trainMean, std: trainStd)\n",
    "xValid = normalize(xValid, mean: trainMean, std: trainStd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test everything is going well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func testNearZero(_ a: TF, tolerance: Float = 1e-3) {\n",
    "    assert(abs(a) < tolerance, \"Near zero: \\(a)\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNearZero(xTrain.mean())\n",
    "testNearZero(xTrain.std() - 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 784 10\r\n"
     ]
    }
   ],
   "source": [
    "let (n,m) = (xTrain.shape[0],xTrain.shape[1])\n",
    "let c = yTrain.max()+1\n",
    "print(n, m, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundations version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//num hidden\n",
    "let nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// simplified kaiming init / he init\n",
    "let w1 = TF(randomNormal: [m, nh]) / sqrt(Float(m))\n",
    "let b1 = TF(zeros: [nh])\n",
    "let w2 = TF(randomNormal: [nh,1]) / sqrt(Float(nh))\n",
    "let b2 = TF(zeros: [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNearZero(w1.mean())\n",
    "testNearZero(w1.std()-1/sqrt(Float(m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ 2 elements\n",
       "  - .0 : 0.006017743\n",
       "  - .1 : 1.0076997\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This should be ~ (0,1) (mean,std)...\n",
    "(xValid.mean(),xValid.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of `@` in python we use `•` (or `matmul`) in Swift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func lin(_ x: TF, _ w: TF, _ b: TF) -> TF { return x•w+b }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let t = lin(xValid, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.115693174 0.98209363\r\n"
     ]
    }
   ],
   "source": [
    "//...so should this, because we used kaiming init, which is designed to do this\n",
    "print(t.mean(), t.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func myRelu(_ x:TF) -> TF { return max(x, 0) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let t = myRelu(lin(xValid, w1, b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33261237 0.5209647\r\n"
     ]
    }
   ],
   "source": [
    "//...actually it really should be this!\n",
    "print(t.mean(),t.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// kaiming init / he init for relu\n",
    "let w1 = TF(randomNormal: [m,nh]) * sqrt(2.0/Float(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0006689982 0.05056813\r\n"
     ]
    }
   ],
   "source": [
    "print(w1.mean(), w1.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6856287 0.93599206\r\n"
     ]
    }
   ],
   "source": [
    "let t = myRelu(lin(xValid, w1, b1))\n",
    "print(t.mean(), t.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple basic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func model(_ xb: TF) -> TF {\n",
    "    let l1 = lin(xb, w1, b1)\n",
    "    let l2 = myRelu(l1)\n",
    "    let l3 = lin(l2, w2, b2)\n",
    "    return l3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average: 0.9829972000000001 ms,   min: 0.941824 ms,   max: 1.104874 ms\r\n"
     ]
    }
   ],
   "source": [
    "time(repeating: 10) { _ = model(xValid) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with the mean squared error to have easier gradient computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let preds = model(xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func mse(_ out: TF, _ targ: TF) -> TF {\n",
    "    return (out.squeezingShape(at: -1) - targ).squared().mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more step comapred to python, we have to make sure our labels are properly converted to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Convert these to Float dtype.\n",
    "var yTrainF = TF(yTrain)\n",
    "var yValidF = TF(yValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.934517\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(preds, yTrainF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients and backward pass\n",
    "\n",
    "Here we should how to calculate gradients for a simple model the hard way, manually.\n",
    "\n",
    "To store the gradients a bit like in PyTorch we introduce a `TensorWithGrad` class that has two attributes: the original tensor and the gradient. We choose a class to easily replicate the Python notebook: classes are reference types (which means they are mutable) while structures are value types.\n",
    "\n",
    "In fact, since this is the first time we're discovering Swift classes, let's jump into a [sidebar discussion about Value Semantics vs Reference Semantics](https://docs.google.com/presentation/d/1dc6o2o-uYGnJeCeyvgsgyk05dBMneArxdICW5vF75oU/edit#slide=id.g5669969ead_0_145) since it is a pretty fundamental part of the programming model and a huge step forward that Swift takes.\n",
    "\n",
    "When we get back, we'll keep charging on, even though this is very non-idiomatic Swift code!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// WARNING: This is designed to be similar to the PyTorch 02_fully_connected lesson,\n",
    "/// this isn't idiomatic Swift code.\n",
    "class TensorWithGrad {\n",
    "    var inner, grad:  TF\n",
    "    \n",
    "    init(_ x: TF) {\n",
    "        inner = x\n",
    "        grad = TF(zeros: x.shape)\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Redefine our functions on TensorWithGrad.\n",
    "func lin(_ x: TensorWithGrad, _ w: TensorWithGrad, _ b: TensorWithGrad) -> TensorWithGrad {\n",
    "    return TensorWithGrad(matmul(x.inner, w.inner) + b.inner)\n",
    "}\n",
    "func myRelu(_ x: TensorWithGrad) -> TensorWithGrad {\n",
    "    return TensorWithGrad(max(x.inner, 0))\n",
    "}\n",
    "func mse(_ inp: TensorWithGrad, _ targ: TF) -> TF {\n",
    "    //grad of loss with respect to output of previous layer\n",
    "    return (inp.inner.squeezingShape(at: -1) - targ).squared().mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Define our gradient functions.\n",
    "func mseGrad(_ inp: TensorWithGrad, _ targ: TF) {\n",
    "    //grad of loss with respect to output of previous layer\n",
    "    inp.grad = 2.0 * (inp.inner.squeezingShape(at: -1) - targ).expandingShape(at: -1) / Float(inp.inner.shape[0])\n",
    "}\n",
    "\n",
    "func reluGrad(_ inp: TensorWithGrad, _ out: TensorWithGrad) {\n",
    "    //grad of relu with respect to input activations\n",
    "    inp.grad = (inp.inner .> 0).selecting(out.grad, TF(zeros: inp.inner.shape))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our python version (we've renamed the python `g` to `grad` for consistency):\n",
    "\n",
    "```python\n",
    "def lin_grad(inp, out, w, b):\n",
    "    inp.grad = out.grad @ w.t()\n",
    "    w.grad = (inp.unsqueeze(-1) * out.grad.unsqueeze(1)).sum(0)\n",
    "    b.grad = out.grad.sum(0)\n",
    "```\n",
    "\n",
    "In Swift `@` is spelled `•`, which is <kbd>option</kbd>-<kbd>8</kbd> on Mac or <kbd>compose</kbd>-<kbd>.</kbd>-<kbd>=</kbd> elsewhere. Or just use the `matmul()` function we've seen already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func linGrad(_ inp:TensorWithGrad, _ out:TensorWithGrad, _ w:TensorWithGrad, _ b:TensorWithGrad){\n",
    "    // grad of linear layer with respect to input activations, weights and bias\n",
    "    inp.grad = out.grad • w.inner.transposed()\n",
    "    w.grad = inp.inner.transposed() • out.grad\n",
    "    b.grad = out.grad.sum(squeezingAxes: 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let w1a = TensorWithGrad(w1)\n",
    "let b1a = TensorWithGrad(b1)\n",
    "let w2a = TensorWithGrad(w2)\n",
    "let b2a = TensorWithGrad(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func forwardAndBackward(_ inp:TensorWithGrad, _ targ:TF){\n",
    "    //forward pass:\n",
    "    let l1 = lin(inp, w1a, b1a)\n",
    "    let l2 = myRelu(l1)\n",
    "    let out = lin(l2, w2a, b2a)\n",
    "    //we don't actually need the loss in backward!\n",
    "    let loss = mse(out, targ)\n",
    "    \n",
    "    //backward pass:\n",
    "    mseGrad(out, targ)\n",
    "    linGrad(l2, out, w2a, b2a)\n",
    "    reluGrad(l1, l2)\n",
    "    linGrad(inp, l1, w1a, b1a)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let inp = TensorWithGrad(xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forwardAndBackward(inp, yTrainF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The swift way\n",
    "\n",
    "As we said before, swift operates in a different way. If we go back to what is happening in the backward pass, we go from the end result (our loss) which allows us to compute the gradient of that loss with respect to the last activations. Then consider all the layers we went through during the forward pass in the reversed order (from the last one to the first one) and for each of them, compute the gradients of the loss with respect to the inputs (and potentially parameters) from the gradients of the loss with respect to the outputs.\n",
    "\n",
    "For instance if we go back to the basic `relu_grad` function we had in python:\n",
    "```\n",
    "def relu_grad(inp, out):\n",
    "    # grad of relu with respect to input activations\n",
    "    inp.g = (inp>0).float() * out.g\n",
    "```\n",
    "we explain how we infer the gradients of the loss with respect to the inputs of the relu (`inp.g`) from the gradients of the loss with respect to the outputs of that same relu (`out.g`).\n",
    "\n",
    "Swift implements differentation in a more functional way than PyTorch: there is no grad attribute, instead we just provide that function that will take the gradients with respect to the outputs and returns the gradients with respect to the inputs (and potentially, parameters).\n",
    "\n",
    "The tricky thing is that that gradient computation often requires to have the inputs/outputs of the layer: if we look at `relu_grad` up there, it needs to know the value of `inp`. That's why we don't just write a function that does `𝛁Out -> 𝛁Inp`, but a function that will take the input and return that pullback:\n",
    "```\n",
    "(Inp) -> ((𝛁Out) -> 𝛁Inp)\n",
    "```\n",
    "\n",
    "Let's look at what it gives us for the relu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func diffRelu(_ inp: TF) -> ((TF) -> TF) {\n",
    "    return {𝛁out -> TF in\n",
    "        (inp .> 0).selecting(𝛁out, TF(zeros: inp.shape))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we go through our relu layer, we won't just ask for `y = relu(x)`, but will also store the pullback `pb = diffRelu(x)` for the backward pass. This will automatically capture a reference to the value of `x` that is used inside that pullback. \n",
    "\n",
    "Other differentiation functions look a bit the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func diffMse(_ inp: TF, _ targ: TF) -> ((TF) -> TF) {\n",
    "    return { 𝛁loss in\n",
    "        2.0 * (inp.squeezingShape(at: -1) - targ).expandingShape(at: -1) / Float(inp.shape[0])\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`diffMse` doesn't return gradients for targ because the function isn't differentiable with respect to that variable in general (we could of course differentiate it in this case, but we don't need those gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func diffLin(_ inp: TF, _ w: TF, _ b: TF) -> ((TF) -> (TF, TF, TF)) {\n",
    "    return { 𝛁out in\n",
    "        (𝛁out • w.transposed(), inp.transposed() • 𝛁out, 𝛁out.sum(squeezingAxes: 0))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`diffLin` returns the gradients with respect to all its inputs (more like inputs and parameters).\n",
    "\n",
    "Then the backward and forward pass is written like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func forwardAndBackward(_ inp: TF, _ targ: TF) -> (TF, TF, TF, TF, TF){\n",
    "    //forward pass:\n",
    "    let (l1, pbL1)    = (lin(inp, w1, b1), diffLin(inp, w1, b1))\n",
    "    let (l2, pbL2)    = (myRelu(l1), diffRelu(l1))\n",
    "    let (out, pbOut)  = (lin(l2, w2, b2), diffLin(l2, w2, b2))\n",
    "    //we don't actually need the loss in backward, but we need the pullback.\n",
    "    let (loss, pbLoss) = (mse(out, targ), diffMse(out, targ))\n",
    "    \n",
    "    //backward pass:\n",
    "    let 𝛁loss = TF(1) //We don't really need it but the gradient of the loss with respect to itself is 1\n",
    "    let 𝛁out = pbLoss(𝛁loss)\n",
    "    let (𝛁l2, 𝛁w2, 𝛁b2) = pbOut(𝛁out)\n",
    "    let 𝛁l1 = pbL2(𝛁l2)\n",
    "    let (𝛁inp, 𝛁w1, 𝛁b1) = pbL1(𝛁l1)\n",
    "    return (𝛁inp, 𝛁w1, 𝛁b1, 𝛁w2, 𝛁b2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (𝛁xTrain, 𝛁w1, 𝛁b1, 𝛁w2, 𝛁b2) = forwardAndBackward(xTrain, yTrainF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Check the gradients computed both way are the same.\n",
    "testNearZero(𝛁xTrain - inp.grad)\n",
    "testNearZero(𝛁w1     - w1a.grad)\n",
    "testNearZero(𝛁b1     - b1a.grad)\n",
    "testNearZero(𝛁w2     - w2a.grad)\n",
    "testNearZero(𝛁b2     - b2a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the S4TF Language Integrated Autodiff\n",
    "\n",
    "Let's compare to the language-integrated Swift for TensorFlow autodiff now. We have to mark the function as `@differentiable`.  This informs the compiler that we want it to automatically generate its gradients, and causes it to emit errors if there is anything contributing to the result of the function that cannot be differentiated.\n",
    "\n",
    "The `@differentiable` attribute is normally optional in a S4TF standalone environment, but is currently required in Jupyter notebooks.  The S4TF team is planning to relax this limitation over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@differentiable\n",
    "func forward(_ inp: TF, _ targ: TF, w1: TF, b1: TF, w2: TF, b2: TF) -> TF {\n",
    "    let l1 = matmul(inp, w1) + b1\n",
    "    let l2 = relu(l1)\n",
    "    let l3 = matmul(l2, w2) + b2\n",
    "    return (l3.squeezingShape(at: -1) - targ).squared().mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can ask for the gradients w.r.t. any individual parameter like this: (𝛁₂ is for second way of computing gradients, not gradients squared, or second order gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let 𝛁₂xTrain = gradient(at: xTrain) {xTrain in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)}\n",
    "let 𝛁₂w1 = gradient(at: w1) {w1 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)}\n",
    "let 𝛁₂b1 = gradient(at: b1) {b1 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)}\n",
    "let 𝛁₂w2 = gradient(at: w2) {w2 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)}\n",
    "let 𝛁₂b2 = gradient(at: b2) {b2 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)}\n",
    "\n",
    "// Check that they agree with the manually calculated gradients.\n",
    "testNearZero(𝛁₂xTrain - 𝛁xTrain)\n",
    "testNearZero(𝛁₂w1     - 𝛁w1)\n",
    "testNearZero(𝛁₂b1     - 𝛁b1)\n",
    "testNearZero(𝛁₂w2     - 𝛁w2)\n",
    "testNearZero(𝛁₂b2     - 𝛁b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also ask for gradients with respect to multiple things at the same time, but unfortunately, current AD bugs prevent getting more than two gradients at a time.  We can do a little bit better than the above code like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (𝛁₃xTrain, 𝛁₃w1) = gradient(at: xTrain, w1) {\n",
    "    xTrain, w1 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)\n",
    "}\n",
    "let (𝛁₃b1, 𝛁₃w2) = gradient(at: b1, w2) {\n",
    "    b1, w2 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)\n",
    "}\n",
    "let 𝛁₃b2 = gradient(at: b2) {b2 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)}\n",
    "\n",
    "// Check that they agree.\n",
    "testNearZero(𝛁₂xTrain - 𝛁₃xTrain)\n",
    "testNearZero(𝛁₂w1     - 𝛁₃w1)\n",
    "testNearZero(𝛁₂b1     - 𝛁₃b1)\n",
    "testNearZero(𝛁₂w2     - 𝛁₃w2)\n",
    "testNearZero(𝛁₂b2     - 𝛁₃b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is currently pretty ugly, and even when the bugs are fixed, it still won't be very idiomatic.  A more common thing is to wrap up all your parameters into a struct, and differentiate w.r.t. all of them at the same time (which, when we refactor the code, will be our model itself).\n",
    "\n",
    "Here is an example of that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct myParams: Differentiable {\n",
    "    public var x, w1, b1, w2, b2: TF\n",
    "}\n",
    "\n",
    "let allParams = myParams(x: xTrain, w1: w1, b1: b1, w2: w2, b2: b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// We can now get all of the gradients at once with a single call, and a single forward computation.\n",
    "let grads = gradient(at: allParams) {\n",
    "  allParams in\n",
    "    forward(allParams.x, yTrainF,\n",
    "            w1: allParams.w1, \n",
    "            b1: allParams.b1,\n",
    "            w2: allParams.w2, \n",
    "            b2: allParams.b2)\n",
    "}\n",
    "\n",
    "// Check that this still calculates the same thing.\n",
    "testNearZero(𝛁₂xTrain  - grads.x)\n",
    "testNearZero(𝛁₂w1      - grads.w1)\n",
    "testNearZero(𝛁₂b1      - grads.b1)\n",
    "testNearZero(𝛁₂w2      - grads.w2)\n",
    "testNearZero(𝛁₂b2      - grads.b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted the value for your loss as well as the gradients, you just have to use `valueWithGradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (loss,grads) = valueWithGradient(at: allParams) { \n",
    "    allParams in forward(allParams.x, yTrainF, w1: allParams.w1, b1: allParams.b1, w2: allParams.w2, b2: allParams.b2)\n",
    "}\n",
    "\n",
    "testNearZero(𝛁₂xTrain  - grads.x)\n",
    "testNearZero(𝛁₂w1      - grads.w1)\n",
    "testNearZero(𝛁₂b1      - grads.b1)\n",
    "testNearZero(𝛁₂w2      - grads.w2)\n",
    "testNearZero(𝛁₂b2      - grads.b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of timing our implementation gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average: 23.8023758 ms,   min: 22.71083 ms,   max: 24.415096 ms\r\n"
     ]
    }
   ],
   "source": [
    "time(repeating: 10) { _ = forwardAndBackward(xTrain, yTrainF) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average: 22.7956517 ms,   min: 21.799479 ms,   max: 24.44094 ms\r\n"
     ]
    }
   ],
   "source": [
    "time(repeating: 10) {\n",
    "    _ = valueWithGradient(at: allParams) { \n",
    "        allParams in forward(allParams.x, \n",
    "                             yTrainF, \n",
    "                             w1: allParams.w1, \n",
    "                             b1: allParams.b1, \n",
    "                             w2: allParams.w2, \n",
    "                             b2: allParams.b2)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactor with valueWithPullback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one thing you will have noticed, is that in our forward and backward, we often ask for a value and the pullback at the same time, that's why it's often implemented together in the primitives of S4TF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func reluWithPb(_ inp: TF) -> (TF, (TF) -> TF) {\n",
    "    return (max(inp, 0), {𝛁out -> TF in\n",
    "        (inp .> 0).selecting(𝛁out, TF(zeros: inp.shape))\n",
    "    })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func linWithPb(_ inp: TF, _ w: TF, _ b: TF) -> (TF, (TF) -> (TF, TF, TF)) {\n",
    "    return (inp • w + b, { 𝛁out in\n",
    "        (𝛁out • w.transposed(), inp.transposed() • 𝛁out, 𝛁out.sum(squeezingAxes: 0))\n",
    "    })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func mseWithPb(_ inp: TF, _ targ: TF) -> (TF, (TF) -> (TF)) {\n",
    "    return ((inp.squeezingShape(at: -1) - targ).squared().mean(), { 𝛁loss in\n",
    "        2.0 * (inp.squeezingShape(at: -1) - targ).expandingShape(at: -1) / Float(inp.shape[0])\n",
    "    })\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then our forward and backward can be refactored in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func forwardAndBackward(_ inp: TF, _ targ: TF) -> (TF, TF, TF, TF, TF){\n",
    "    //forward pass:\n",
    "    let (l1, pbL1)    = linWithPb(inp, w1, b1)\n",
    "    let (l2, pbL2)    = reluWithPb(l1)\n",
    "    let (out, pbOut)  = linWithPb(l2, w2, b2)\n",
    "    //we don't actually need the loss in backward, but we need the pullback.\n",
    "    let (loss, pbLoss) = mseWithPb(out, targ)\n",
    "    \n",
    "    //backward pass:\n",
    "    let 𝛁loss = TF(1) //We don't really need it but the gradient of the loss with respect to itself is 1\n",
    "    let 𝛁out = pbLoss(𝛁loss)\n",
    "    let (𝛁l2, 𝛁w2, 𝛁b2) = pbOut(𝛁out)\n",
    "    let 𝛁l1 = pbL2(𝛁l2)\n",
    "    let (𝛁inp, 𝛁w1, 𝛁b1) = pbL1(𝛁l1)\n",
    "    return (𝛁inp, 𝛁w1, 𝛁b1, 𝛁w2, 𝛁b2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (𝛁xTrain, 𝛁w1, 𝛁b1, 𝛁w2, 𝛁b2) = forwardAndBackward(xTrain, yTrainF)\n",
    "// Check this is still all correct\n",
    "testNearZero(𝛁₂xTrain - 𝛁xTrain)\n",
    "testNearZero(𝛁₂w1     - 𝛁w1)\n",
    "testNearZero(𝛁₂b1     - 𝛁b1)\n",
    "testNearZero(𝛁₂w2     - 𝛁w2)\n",
    "testNearZero(𝛁₂b2     - 𝛁b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\r\n"
     ]
    }
   ],
   "source": [
    "import NotebookExport\n",
    "let exporter = NotebookExport(Path.cwd/\"02_fully_connected.ipynb\")\n",
    "print(exporter.export(usingPrefix: \"FastaiNotebook_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
