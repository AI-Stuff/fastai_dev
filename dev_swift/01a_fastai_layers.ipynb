{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"fast.ai\" layers\n",
    "\n",
    "This notebook defines layers similar to those defined in the [Swift for TensorFlow Deep Learning Library](https://github.com/tensorflow/swift-apis), but with some experimental extra features for the fast.ai course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%install '.package(path: \"$cwd/FastaiNotebook_00_load_data\")' FastaiNotebook_00_load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "\n",
    "import TensorFlow\n",
    "\n",
    "/// `Layer` with some experimental extra features.\n",
    "///\n",
    "/// This refines `Layer` so that it can be used with deep learning library utilities. If this becomes\n",
    "/// too restrictive, we can remove the refinement and re-implement `Optimizer`.\n",
    "public protocol FALayer: Layer {\n",
    "    var delegate: LayerDelegate<Output> { get set }\n",
    "    \n",
    "    /// Returns the output obtained from applying the layer to the given input.\n",
    "    ///\n",
    "    /// - Parameters:\n",
    "    ///   - input: The input to the layer.\n",
    "    ///   - context: The contextual information for the layer application, e.g. the current learning\n",
    "    ///     phase.\n",
    "    /// - Returns: The output.\n",
    "    @differentiable\n",
    "    func forward(_ input: Input, in context: Context) -> Output\n",
    "}\n",
    "\n",
    "// TODO: This doesn't actually work. So we'll just paste it into every layer definition for now.\n",
    "// /// Handles delegation when the layer is applied.\n",
    "// extension FALayer {\n",
    "//     @differentiable\n",
    "//     public func applied(to input: Input, in context: Context) -> Output {\n",
    "//         let activation = forward(input, in: context)\n",
    "//         delegate.didProduceActivation(activation, in: context)\n",
    "//         return activation\n",
    "//     }\n",
    "// }\n",
    "\n",
    "open class LayerDelegate<Output> {\n",
    "    public init() {}\n",
    "    \n",
    "    /// Called when a layer produces an activation.\n",
    "    open func didProduceActivation(_ activation: Output, in context: Context) {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "\n",
    "/// A densely-connected neural network layer.\n",
    "///\n",
    "/// `Dense` implements the operation `activation(matmul(input, weight) + bias)`, where `weight` is\n",
    "/// a weight matrix, `bias` is a bias vector, and `activation` is an element-wise activation\n",
    "/// function.\n",
    "@_fixed_layout\n",
    "public struct FADense<Scalar: TensorFlowFloatingPoint>: FALayer {       \n",
    "    /// The weight matrix.\n",
    "    public var weight: Tensor<Scalar>\n",
    "    /// The bias vector.\n",
    "    public var bias: Tensor<Scalar>\n",
    "    public typealias Activation = @differentiable (Tensor<Scalar>) -> Tensor<Scalar>\n",
    "    /// The element-wise activation function.\n",
    "    @noDerivative public let activation: Activation\n",
    "    \n",
    "    @noDerivative public var delegate: LayerDelegate<Output> = LayerDelegate()\n",
    "\n",
    "    public init(\n",
    "        weight: Tensor<Scalar>,\n",
    "        bias: Tensor<Scalar>,\n",
    "        activation: @escaping Activation\n",
    "    ) {\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "    }\n",
    "\n",
    "    /// Returns the output obtained from applying the layer to the given input.\n",
    "    ///\n",
    "    /// - Parameters:\n",
    "    ///   - input: The input to the layer.\n",
    "    ///   - context: The contextual information for the layer application, e.g. the current learning\n",
    "    ///     phase.\n",
    "    /// - Returns: The output.\n",
    "    @differentiable\n",
    "    public func forward(_ input: Tensor<Scalar>, in _: Context) -> Tensor<Scalar> {\n",
    "        return activation(matmul(input, weight) + bias)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func applied(to input: Tensor<Scalar>, in context: Context) -> Tensor<Scalar> {\n",
    "        let activation = forward(input, in: context)\n",
    "        delegate.didProduceActivation(activation, in: context)\n",
    "        return activation\n",
    "    }\n",
    "}\n",
    "\n",
    "public extension FADense {\n",
    "    /// Creates a `Dense` layer with the specified input size, output size, and element-wise\n",
    "    /// activation function. The weight matrix is created with shape `[inputSize, outputSize]` and\n",
    "    /// is initialized using Glorot uniform initialization with the specified seed. The bias vector\n",
    "    /// is created with shape `[outputSize]` and is initialized with zeros.\n",
    "    ///\n",
    "    /// - Parameters:\n",
    "    ///   - inputSize: The dimensionality of the input space.\n",
    "    ///   - outputSize: The dimensionality of the output space.\n",
    "    ///   - activation: The activation function to use. The default value is `identity(_:)`.\n",
    "    ///   - seed: The random seed for initialization. The default value is random.\n",
    "    init(\n",
    "        inputSize: Int,\n",
    "        outputSize: Int,\n",
    "        activation: @escaping Activation = identity,\n",
    "        seed: (Int64, Int64) = (Int64.random(in: Int64.min..<Int64.max),\n",
    "                                Int64.random(in: Int64.min..<Int64.max))\n",
    "    ) {\n",
    "        self.init(weight: Tensor(glorotUniform: [Int32(inputSize), Int32(outputSize)],\n",
    "                                 seed: seed),\n",
    "                  bias: Tensor(zeros: [Int32(outputSize)]),\n",
    "                  activation: activation)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "\n",
    "/// A 2-D convolution layer (e.g. spatial convolution over images).\n",
    "///\n",
    "/// This layer creates a convolution filter that is convolved with the layer input to produce a\n",
    "/// tensor of outputs.\n",
    "@_fixed_layout\n",
    "public struct FAConv2D<Scalar: TensorFlowFloatingPoint>: FALayer {\n",
    "    /// The 4-D convolution kernel.\n",
    "    public var filter: Tensor<Scalar>\n",
    "    /// The bias vector.\n",
    "    public var bias: Tensor<Scalar>\n",
    "    /// An activation function.\n",
    "    public typealias Activation = @differentiable (Tensor<Scalar>) -> Tensor<Scalar>\n",
    "    /// The element-wise activation function.\n",
    "    @noDerivative public let activation: Activation\n",
    "    /// The strides of the sliding window for spatial dimensions.\n",
    "    @noDerivative public let strides: (Int32, Int32)\n",
    "    /// The padding algorithm for convolution.\n",
    "    @noDerivative public let padding: Padding\n",
    "    \n",
    "    @noDerivative public var delegate: LayerDelegate<Output> = LayerDelegate()\n",
    "\n",
    "    /// Creates a `Conv2D` layer with the specified filter, bias, activation function, strides, and\n",
    "    /// padding.\n",
    "    ///\n",
    "    /// - Parameters:\n",
    "    ///   - filter: The 4-D convolution kernel.\n",
    "    ///   - bias: The bias vector.\n",
    "    ///   - activation: The element-wise activation function.\n",
    "    ///   - strides: The strides of the sliding window for spatial dimensions.\n",
    "    ///   - padding: The padding algorithm for convolution.\n",
    "    public init(\n",
    "        filter: Tensor<Scalar>,\n",
    "        bias: Tensor<Scalar>,\n",
    "        activation: @escaping Activation,\n",
    "        strides: (Int, Int),\n",
    "        padding: Padding\n",
    "    ) {\n",
    "        self.filter = filter\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        (self.strides.0, self.strides.1) = (Int32(strides.0), Int32(strides.1))\n",
    "        self.padding = padding\n",
    "    }\n",
    "\n",
    "    /// Returns the output obtained from applying the layer to the given input.\n",
    "    ///\n",
    "    /// - Parameters:\n",
    "    ///   - input: The input to the layer.\n",
    "    ///   - context: The contextual information for the layer application, e.g. the current learning\n",
    "    ///     phase.\n",
    "    /// - Returns: The output.\n",
    "    @differentiable\n",
    "    public func forward(_ input: Tensor<Scalar>, in _: Context) -> Tensor<Scalar> {\n",
    "        return activation(input.convolved2D(withFilter: filter,\n",
    "                                            strides: (1, strides.0, strides.1, 1),\n",
    "                                            padding: padding) + bias)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func applied(to input: Tensor<Scalar>, in context: Context) -> Tensor<Scalar> {\n",
    "        let activation = forward(input, in: context)\n",
    "        delegate.didProduceActivation(activation, in: context)\n",
    "        return activation\n",
    "    }\n",
    "}\n",
    "\n",
    "public extension FAConv2D {\n",
    "    /// Creates a `Conv2D` layer with the specified filter shape, strides, padding, and\n",
    "    /// element-wise activation function. The filter tensor is initialized using Glorot uniform\n",
    "    /// initialization with the specified generator. The bias vector is initialized with zeros.\n",
    "    ///\n",
    "    /// - Parameters:\n",
    "    ///   - filterShape: The shape of the 4-D convolution kernel.\n",
    "    ///   - strides: The strides of the sliding window for spatial dimensions.\n",
    "    ///   - padding: The padding algorithm for convolution.\n",
    "    ///   - activation: The element-wise activation function.\n",
    "    ///   - generator: The random number generator for initialization.\n",
    "    ///\n",
    "    /// - Note: Use `init(filterShape:strides:padding:activation:seed:)` for faster random\n",
    "    ///   initialization.\n",
    "    init<G: RandomNumberGenerator>(\n",
    "        filterShape: (Int, Int, Int, Int),\n",
    "        strides: (Int, Int) = (1, 1),\n",
    "        padding: Padding = .valid,\n",
    "        activation: @escaping Activation = identity,\n",
    "        generator: inout G\n",
    "    ) {\n",
    "        let filterTensorShape = TensorShape([\n",
    "            Int32(filterShape.0), Int32(filterShape.1),\n",
    "            Int32(filterShape.2), Int32(filterShape.3)])\n",
    "        self.init(\n",
    "            filter: Tensor(glorotUniform: filterTensorShape, generator: &generator),\n",
    "            bias: Tensor(zeros: TensorShape([Int32(filterShape.3)])),\n",
    "            activation: activation,\n",
    "            strides: strides,\n",
    "            padding: padding)\n",
    "    }\n",
    "}\n",
    "\n",
    "public extension FAConv2D {\n",
    "    /// Creates a `Conv2D` layer with the specified filter shape, strides, padding, and\n",
    "    /// element-wise activation function. The filter tensor is initialized using Glorot uniform\n",
    "    /// initialization with the specified seed. The bias vector is initialized with zeros.\n",
    "    ///\n",
    "    /// - Parameters:\n",
    "    ///   - filterShape: The shape of the 4-D convolution kernel.\n",
    "    ///   - strides: The strides of the sliding window for spatial dimensions.\n",
    "    ///   - padding: The padding algorithm for convolution.\n",
    "    ///   - activation: The element-wise activation function.\n",
    "    ///   - seed: The random seed for initialization. The default value is random.\n",
    "    init(\n",
    "        filterShape: (Int, Int, Int, Int),\n",
    "        strides: (Int, Int) = (1, 1),\n",
    "        padding: Padding = .valid,\n",
    "        activation: @escaping Activation = identity,\n",
    "        seed: (Int64, Int64) = (Int64.random(in: Int64.min..<Int64.max),\n",
    "                                Int64.random(in: Int64.min..<Int64.max))\n",
    "    ) {\n",
    "        let filterTensorShape = TensorShape([\n",
    "            Int32(filterShape.0), Int32(filterShape.1),\n",
    "            Int32(filterShape.2), Int32(filterShape.3)])\n",
    "        self.init(\n",
    "            filter: Tensor(glorotUniform: filterTensorShape, seed: seed),\n",
    "            bias: Tensor(zeros: TensorShape([Int32(filterShape.3)])),\n",
    "            activation: activation,\n",
    "            strides: (strides.0, strides.1),\n",
    "            padding: padding)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AvgPool2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "\n",
    "/// An average pooling layer for spatial data.\n",
    "@_fixed_layout\n",
    "public struct FAAvgPool2D<Scalar: TensorFlowFloatingPoint>: FALayer {\n",
    "    /// The size of the sliding reduction window for pooling.\n",
    "    @noDerivative let poolSize: (Int32, Int32, Int32, Int32)\n",
    "    /// The strides of the sliding window for each dimension of a 4-D input.\n",
    "    /// Strides in non-spatial dimensions must be `1`.\n",
    "    @noDerivative let strides: (Int32, Int32, Int32, Int32)\n",
    "    /// The padding algorithm for pooling.\n",
    "    @noDerivative let padding: Padding\n",
    "    \n",
    "    @noDerivative public var delegate: LayerDelegate<Output> = LayerDelegate()\n",
    "\n",
    "    /// Creates a average pooling layer.\n",
    "    public init(\n",
    "        poolSize: (Int, Int, Int, Int),\n",
    "        strides: (Int, Int, Int, Int),\n",
    "        padding: Padding\n",
    "    ) {\n",
    "        (self.poolSize.0, self.poolSize.1, self.poolSize.2, self.poolSize.3)\n",
    "            = (Int32(poolSize.0), Int32(poolSize.1), Int32(poolSize.2), Int32(poolSize.3))\n",
    "        (self.strides.0, self.strides.1, self.strides.2, self.strides.3)\n",
    "            = (Int32(strides.0), Int32(strides.1), Int32(strides.2), Int32(strides.3))\n",
    "        self.padding = padding\n",
    "    }\n",
    "\n",
    "    /// Creates a average pooling layer.\n",
    "    ///\n",
    "    /// - Parameters:\n",
    "    ///   - poolSize: Vertical and horizontal factors by which to downscale.\n",
    "    ///   - strides: The strides.\n",
    "    ///   - padding: The padding.\n",
    "    public init(poolSize: (Int, Int), strides: (Int, Int), padding: Padding = .valid) {\n",
    "        self.poolSize = (1, Int32(poolSize.0), Int32(poolSize.1), 1)\n",
    "        self.strides = (1, Int32(strides.0), Int32(strides.1), 1)\n",
    "        self.padding = padding\n",
    "    }\n",
    "\n",
    "    /// Returns the output obtained from applying the layer to the given input.\n",
    "    ///\n",
    "    /// - Parameters:\n",
    "    ///   - input: The input to the layer.\n",
    "    ///   - context: The contextual information for the layer application, e.g. the current learning\n",
    "    ///     phase.\n",
    "    /// - Returns: The output.\n",
    "    @differentiable\n",
    "    public func forward(_ input: Tensor<Scalar>, in _: Context) -> Tensor<Scalar> {\n",
    "        return input.averagePooled(kernelSize: poolSize, strides: strides, padding: padding)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func applied(to input: Tensor<Scalar>, in context: Context) -> Tensor<Scalar> {\n",
    "        let activation = forward(input, in: context)\n",
    "        delegate.didProduceActivation(activation, in: context)\n",
    "        return activation\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebook_00_load_data\n",
    "import Path\n",
    "notebookToScript(fname: (Path.cwd / \"01a_fastai_layers.ipynb\").string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
