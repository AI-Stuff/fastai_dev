{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/home/clattner/fastai_docs/dev_swift/FastaiNotebook_00_load_data\")\n",
      "\t\tFastaiNotebook_00_load_data\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmp0u49m3ue\n",
      "/home/clattner/swift/usr/bin/swift-build: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/lib/swift/linux/libFoundation.so)\n",
      "Fetching https://github.com/mxcl/Path.swift\n",
      "Fetching https://github.com/JustHTTP/Just\n",
      "Completed resolution in 1.96s\n",
      "Cloning https://github.com/JustHTTP/Just\n",
      "Resolving https://github.com/JustHTTP/Just at 0.7.1\n",
      "Cloning https://github.com/mxcl/Path.swift\n",
      "Resolving https://github.com/mxcl/Path.swift at 0.16.2\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "Compile Swift Module 'Path' (9 sources)\n",
      "Compile Swift Module 'Just' (1 sources)\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "Compile Swift Module 'FastaiNotebook_00_load_data' (1 sources)\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "Compile Swift Module 'jupyterInstalledPackages' (1 sources)\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "Linking ./.build/x86_64-unknown-linux/debug/libjupyterInstalledPackages.so\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift-autolink-extract: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift-autolink-extract)\n",
      "\n",
      "Initializing Swift...\n",
      "Loading library...\n",
      "Installation complete!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "error: <Cell 1>:1:18: error: consecutive statements on a line must be separated by ';'\n%install-location $cwd/swift-install\n                 ^\n                 ;\n\n"
     ]
    }
   ],
   "source": [
    "%install-location $cwd/swift-install\n",
    "%install '.package(path: \"$cwd/FastaiNotebook_00_load_data\")' FastaiNotebook_00_load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "import Path\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebook_00_load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some Tensors to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.033045642,   0.04449675,  0.015506563,   0.02996331, -0.039954513,  0.086654566,\r\n",
      "  0.007328817, 0.0069880965,  0.022950158, -0.014337598]\r\n"
     ]
    }
   ],
   "source": [
    "let xTrain = Tensor<Float>(randomNormal: [5, 784])\n",
    "//let (xTrain, yTrain, xValid, yValid) = loadMNIST(path: mnistPath, flat: true)\n",
    "var weights = Tensor<Float>(randomNormal: [784, 10]) / sqrt(784)\n",
    "print(weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we know how floating point types and arrays work, we can finally build our own matmul from scratch, using a few loops.  We will take the two input matrices as single dimentional arrays so we can show manual indexing into them, the hard way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// a and b are the flattened array elements, aDims/bDims are the #rows/columns of the arrays.\n",
    "func swiftMatmul(a: [Float], b: [Float], aDims: (Int,Int), bDims: (Int,Int)) -> [Float] {\n",
    "    assert(aDims.1 == bDims.0, \"matmul shape mismatch\")\n",
    "    \n",
    "    var res = Array(repeating: Float(0.0), count: aDims.0 * bDims.1)\n",
    "    for i in 0 ..< aDims.0 {\n",
    "        for j in 0 ..< bDims.1 {\n",
    "            for k in 0 ..< aDims.1 {\n",
    "                res[i+aDims.0*j] += a[i+aDims.0*k] * b[k+bDims.0*j]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return res\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// To try it out, we extract the scalars out of our MNIST data as an array.\n",
    "let flatA = xTrain[0..<5].scalars\n",
    "let flatB = weights.scalars\n",
    "let (aDims,bDims) = ((5, 784), (784, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Now that we've got everything together, we can try it out!\n",
    "var resultArray = swiftMatmul(a: flatA, b: flatB, aDims: aDims, bDims: bDims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average: 0.12145337000000003 ms,   min: 0.110486 ms,   max: 0.153928 ms\r\n"
     ]
    }
   ],
   "source": [
    "time(repeating: 100) {\n",
    "    _ = swiftMatmul(a: flatA, b: flatB, aDims: aDims, bDims: bDims)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, that is pretty fast - compare that to **835 ms** with Python!\n",
    "\n",
    "You might be wondering what that `time(repeating:)` builtin is.  As you might guess, this is actually a Swift function - one that is using \"trailing closure\" syntax to specify the body of the timing block.  Trailing closures are passed as arguments to the function, and in this case, the function was defined in our ✅**00_load_data** workbook.  Let's take a look!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the performance of C 💯\n",
    "\n",
    "This performance is pretty great, but we can do better.  Swift is a memory safe language (like Python), which means it has to do array bounds checks and some other stuff.  Fortunately, Swift is a pragmatic language that allows you to drop through this to get peak performance - check out Jeremy's article [High Performance Numeric Programming with Swift: Explorations and Reflections](https://www.fast.ai/2019/01/10/swift-numerics/) for a deep dive.\n",
    "\n",
    "One thing you can do is use `UnsafePointer` (which is basically a raw C pointer) instead of using a bounds checked array.  This isn't memory safe, but gives us about a 2x speedup in this case!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average: 0.07645685999999997 ms,   min: 0.073942 ms,   max: 0.105068 ms\r\n"
     ]
    }
   ],
   "source": [
    "// a and b are the flattened array elements, aDims/bDims are the #rows/columns of the arrays.\n",
    "func swiftMatmulUnsafe(a: [Float], b: [Float], aDims: (Int,Int), bDims: (Int,Int)) -> [Float] {\n",
    "    assert(aDims.1 == bDims.0, \"matmul shape mismatch\")\n",
    "    \n",
    "    var res = Array(repeating: Float(0.0), count: aDims.0 * bDims.1)\n",
    "    res.withUnsafeMutableBufferPointer { res in \n",
    "        for i in 0 ..< aDims.0 {\n",
    "            for j in 0 ..< bDims.1 {\n",
    "                for k in 0 ..< aDims.1 {\n",
    "                    res[i+aDims.0*j] += a[i+aDims.0*k] * b[k+bDims.0*j]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return res\n",
    "}\n",
    "time(repeating: 100) {\n",
    "    _ = swiftMatmulUnsafe(a: flatA, b: flatB, aDims: aDims, bDims: bDims)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the other cool things about this is that we can provide a nice idiomatic API to the caller of this, and keep all the unsafe shenanigans inside the implementation of this function.\n",
    "\n",
    "If you really want to fall down the rabbit hole, you can look at the [implementation of `UnsafePointer`](https://github.com/apple/swift/blob/tensorflow/stdlib/public/core/UnsafePointer.swift), which is of written in Swift wrapping LLVM pointer operations.  This means you can literally get the performance of C code directly in Swift, while providing easy to use high level APIs!\n",
    "\n",
    "## Swift 💖 C APIs too: you get the full utility of the C ecosystem\n",
    "\n",
    "Swift even lets you transparently work with C APIs, just like it does with Python.  This can be used for both good and evil.  For example, here we directly call the `malloc` function, dereference the uninitialized pointer, and print it out:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☠️☠️ Uninitialized garbage = 192\r\n"
     ]
    }
   ],
   "source": [
    "import Glibc\n",
    "\n",
    "let ptr : UnsafeMutableRawPointer = malloc(42)\n",
    "\n",
    "print(\"☠️☠️ Uninitialized garbage =\", ptr.load(as: UInt8.self))\n",
    "\n",
    "free(ptr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `UnsafeMutableRawPointer` ([implementation](https://github.com/apple/swift/blob/tensorflow/stdlib/public/core/UnsafeRawPointer.swift)) isn't something you should use lightly, but when you work with C APIs, you'll see various types like that in the function signatures.\n",
    "\n",
    "Calling `malloc` and `free` directly aren't recommended in Swift, but is useful and important when you're working with C APIs that expect to get malloc'd memory, which comes up when you're written a safe Swift wrapper for some existing code.\n",
    "\n",
    "Speaking of existing code, let's take a look at that **Python interop** we touched on before:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🐍list =     b'\\x80\\x03]q\\x00(K\\x01K\\x02K\\x03e.'\r\n",
      "🐍ndarray =  b'\\x80\\x03cnumpy.core.multiarray\\n_reconstruct\\nq\\x00cnumpy\\nndarray\\nq\\x01K\\x00\\x85q\\x02C\\x01bq\\x03\\x87q\\x04Rq\\x05(K\\x01K\\x02K\\x02\\x86q\\x06cnumpy\\ndtype\\nq\\x07X\\x02\\x00\\x00\\x00i8q\\x08K\\x00K\\x01\\x87q\\tRq\\n(K\\x03X\\x01\\x00\\x00\\x00<q\\x0bNNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00tq\\x0cb\\x89C \\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x00q\\rtq\\x0eb.'\r\n"
     ]
    }
   ],
   "source": [
    "import Python\n",
    "let np = Python.import(\"numpy\")\n",
    "let pickle = Python.import(\"pickle\")\n",
    "let sys = Python.import(\"sys\")\n",
    "\n",
    "print(\"🐍list =    \", pickle.dumps([1, 2, 3]))\n",
    "print(\"🐍ndarray = \", pickle.dumps(np.array([[1, 2], [3, 4]])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this is [all written in Swift](https://github.com/apple/swift/tree/tensorflow/stdlib/public/Python) as well.  You can probably guess how this works now: `PythonObject` is a Swift struct that wraps a pointer to the Python interpreter's notion of a Python object.\n",
    "\n",
    "```swift\n",
    "@dynamicCallable\n",
    "@dynamicMemberLookup\n",
    "public struct PythonObject {\n",
    "  var reference: PyReference\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "The `@dynamicMemberLookup` attribute allows it to dynamically handle all member lookups (like `x.y`) by calling into [the `PyObject_GetAttrString` runtime call](https://github.com/apple/swift/blob/tensorflow/stdlib/public/Python/Python.swift#L427).  Similarly, the `@dynamicCallable` attribute allows the type to intercept all calls to a PythonObject (like `x()`), which it implements using the [`PyObject_Call` runtime call](https://github.com/apple/swift/blob/tensorflow/stdlib/public/Python/Python.swift#L324).  \n",
    "\n",
    "Because Swift has such simple and transparent access to C, it allows building very nice first-class Swift APIs that talk directly to the lower level implementation, and these implementations can have very little overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Tensor\n",
    "\n",
    "Lets get back into matmul and explore more of the `Tensor` type as provided by the TensorFlow module.  You can see all things `Tensor` can do in the [official documentation](https://www.tensorflow.org/swift/api_docs/Structs/Tensor).\n",
    "\n",
    "Here are some highlights: You can create Tensor's in many ways using the initializer, for example you can get zeros or random data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1:  TensorShape(dimensions: [5, 784])\r\n",
      "m2:  TensorShape(dimensions: [784, 10])\r\n"
     ]
    }
   ],
   "source": [
    "var bias = Tensor<Float>(zeros: [10])\n",
    "\n",
    "let m1 = Tensor<Float>(randomNormal: [5, 784])\n",
    "let m2 = Tensor<Float>(randomNormal: [784, 10])\n",
    "\n",
    "// Tensors carry data and a shape.\n",
    "print(\"m1: \", m1.shape)\n",
    "print(\"m2: \", m2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Tensor` type provides all the normal stuff you'd expect as methods.  Including arithemic, convolutions, etc and this includes full support for broadcasting:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢2x2:\r\n",
      " [[1.0, 2.0],\r\n",
      " [3.0, 4.0]]\r\n",
      "\r\n",
      "⊞ matmul:\r\n",
      " [[ 7.0, 10.0],\r\n",
      " [15.0, 22.0]]\r\n",
      "\r\n",
      "➕add:\r\n",
      " [[2.0, 4.0],\r\n",
      " [6.0, 8.0]]\r\n",
      "\r\n",
      "💠➕broadcast add:\r\n",
      " [[101.0, 102.0],\r\n",
      " [103.0, 104.0]]\r\n"
     ]
    }
   ],
   "source": [
    "let small = Tensor<Float>([[1, 2],\n",
    "                           [3, 4]])\n",
    "\n",
    "print(\"🔢2x2:\\n\",             small)\n",
    "print(\"\\n⊞ matmul:\\n\",        matmul(small, small))\n",
    "print(\"\\n➕add:\\n\",           small + small)\n",
    "print(\"\\n💠➕broadcast add:\\n\", small + 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MatMul Operator:** In addition to using the global `matmul(a, b)` function, you can also use the `a • b` operator to matmul together two things.  This is just like the `@` operator in Python.  You can get it with the <kbd>option</kbd>-<kbd>8</kbd> on Mac or <kbd>compose</kbd>-<kbd>.</kbd>-<kbd>=</kbd> elsewhere. Or if you prefer, just use the `matmul()` function we've seen already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⊞ matmul:\n",
      " [[ 7.0, 10.0],\n",
      " [15.0, 22.0]]\n",
      "\n",
      "⊞ again:\n",
      " [[ 7.0, 10.0],\n",
      " [15.0, 22.0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"⊞ matmul:\\n\",        matmul(small, small))\n",
    "print(\"\\n⊞ again:\\n\",        small • small)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reshaping** works the way you'd expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 2.0, 3.0],\r\n",
      " [4.0, 5.0, 6.0],\r\n",
      " [7.0, 8.0, 9.0]]\r\n",
      "\r\n",
      "16.881943016134134\r\n"
     ]
    }
   ],
   "source": [
    "var m = Tensor([1.0, 2, 3, 4, 5, 6, 7, 8, 9]).reshaped(to: [3, 3])\n",
    "print(m)\n",
    "print()\n",
    "\n",
    "print(sqrt((m * m).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elementwise ops and comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard math operators (`+`,`-`,`*`,`/`) are all element-wise, and there are a bunch of standard math functions like `sqrt` and `pow`.  Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ 2 elements\n",
       "  - .0 : [10.0,  6.0, -4.0]\n",
       "  - .1 : [2.0, 8.0, 7.0]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var a = Tensor([10.0, 6, -4])\n",
    "var b = Tensor([2.0, 8, 7])\n",
    "(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add:   [12.0, 14.0,  3.0]\r\n",
      "mul:   [ 20.0,  48.0, -28.0]\r\n",
      "sqrt:  [3.1622776601683795,  2.449489742783178,               -nan]\r\n",
      "pow:   [             100.0, 1679616.0000000002,           -16384.0]\r\n"
     ]
    }
   ],
   "source": [
    "print(\"add:  \", a + b)\n",
    "print(\"mul:  \", a * b)\n",
    "print(\"sqrt: \", sqrt(a))\n",
    "print(\"pow:  \", pow(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Comparison operators** (`>`,`<`,`==`,`!=`,...) in Swift are supposed to return a single `Bool` value, so they are `true` if all the elements of the tensors satisfy the comparison.\n",
    "\n",
    "Elementwise versions have the `.` prefix, which is read as \"pointwise\": `.>`, `.<`, `.==`, etc.  You can merge a tensor of bools into a single Bool with the `any()` and `all()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a < b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[false,  true,  true]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a .< b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ true,  true, false]\r\n"
     ]
    }
   ],
   "source": [
    "print(a .> 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false\r\n"
     ]
    }
   ],
   "source": [
    "print((a .> 0).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\r\n"
     ]
    }
   ],
   "source": [
    "print((a .> 0).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting with a scalar works just like in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var a = Tensor([10.0, 6.0, -4.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.0,  7.0, -3.0]\r\n"
     ]
    }
   ],
   "source": [
    "print(a+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ 2.0,  4.0,  6.0],\n",
       " [ 8.0, 10.0, 12.0],\n",
       " [14.0, 16.0, 18.0]]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting a vector with a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let c = Tensor([10.0,20.0,30.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, broadcasting is done by adding 1 dimensions to the beginning until dimensions of both objects match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11.0, 22.0, 33.0],\n",
       " [14.0, 25.0, 36.0],\n",
       " [17.0, 28.0, 39.0]]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11.0, 22.0, 33.0],\n",
       " [14.0, 25.0, 36.0],\n",
       " [17.0, 28.0, 39.0]]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c + m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To broadcast on the other dimensions, one has to use `expandingShape` to add the dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11.0, 12.0, 13.0],\n",
       " [24.0, 25.0, 26.0],\n",
       " [37.0, 38.0, 39.0]]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m + c.expandingShape(at: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10.0],\n",
       " [20.0],\n",
       " [30.0]]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.expandingShape(at: 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcasting rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape(dimensions: [1, 3])\r\n"
     ]
    }
   ],
   "source": [
    "print(c.expandingShape(at: 0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape(dimensions: [3, 1])\r\n"
     ]
    }
   ],
   "source": [
    "print(c.expandingShape(at: 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[100.0, 200.0, 300.0],\n",
       " [200.0, 400.0, 600.0],\n",
       " [300.0, 600.0, 900.0]]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.expandingShape(at: 0) * c.expandingShape(at: 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[false,  true,  true],\n",
       " [false, false,  true],\n",
       " [false, false, false]]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.expandingShape(at: 0) .> c.expandingShape(at: 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matmul using `Tensor`\n",
    "\n",
    "Coming back to our matmul algorithm, we can implement exactly what we had before by using subscripting into a tensor, instead of subscripting into an array.  Let's see how that works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func tensorMatmul(_ a: Tensor<Float>, _ b: Tensor<Float>) -> Tensor<Float> {\n",
    "    var res = Tensor<Float>(zeros: [a.shape[0], b.shape[1]])\n",
    "\n",
    "    for i in 0 ..< a.shape[0] {\n",
    "        for j in 0 ..< b.shape[1] {\n",
    "            for k in 0 ..< a.shape[1] {\n",
    "                res[i, j] += a[i, k] * b[k, j]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return res\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average: 6805.451182 ms,   min: 6805.451182 ms,   max: 6805.451182 ms\r\n"
     ]
    }
   ],
   "source": [
    "time { \n",
    "    let tmp = tensorMatmul(m1, m2)\n",
    "    \n",
    "    // Copy a scalar back to the host to force a GPU sync.\n",
    "    _ = tmp[0, 0].scalar\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What, what just happened?? We used to be less than a **tenth of a millisecond**, now we're taking **multiple seconds**.  It turns out that Tensor's are very good at bulk data processing, but they are not good at doing one float at a time.  Make sure to use the coarse-grained operations.  We can make this faster by vectorizing each loop in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the inner loop into a multiply + sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func elementWiseMatmul(_ a:Tensor<Float>, _ b:Tensor<Float>) -> Tensor<Float>{\n",
    "    let (ar, ac) = (a.shape[0], a.shape[1])\n",
    "    let (br, bc) = (b.shape[0], b.shape[1])\n",
    "    var res = Tensor<Float>(zeros: [ac, br])\n",
    "    \n",
    "    for i in 0 ..< ar {\n",
    "        let row = a[i]\n",
    "        for j in 0 ..< bc {\n",
    "            res[i, j] = (row * b.slice(lowerBounds: [0,j], upperBounds: [ac,j+1]).squeezingShape(at: 1)).sum()\n",
    "        }\n",
    "    }\n",
    "    return res\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = elementWiseMatmul(m1, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average: 374.769131 ms,   min: 374.769131 ms,   max: 374.769131 ms\r\n"
     ]
    }
   ],
   "source": [
    "time { \n",
    "    let tmp = elementWiseMatmul(m1, m2)\n",
    "\n",
    "    // Copy a scalar back to the host to force a GPU sync.\n",
    "    _ = tmp[0, 0].scalar\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the inner two loops with broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func broadcastMatmult(_ a:Tensor<Float>, _ b:Tensor<Float>) -> Tensor<Float>{\n",
    "    var res = Tensor<Float>(zeros: [a.shape[0], b.shape[1]])\n",
    "    for i in 0..<a.shape[0] {\n",
    "        res[i] = (a[i].expandingShape(at: 1) * b).sum(squeezingAxes: 0)\n",
    "    }\n",
    "    return res\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = broadcastMatmult(m1, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average: 1.8719801600000006 ms,   min: 1.477153 ms,   max: 2.784409 ms\r\n"
     ]
    }
   ],
   "source": [
    "time(repeating: 100) {\n",
    "    let tmp = broadcastMatmult(m1, m2)\n",
    "\n",
    "    // Copy a scalar back to the host to force a GPU sync.\n",
    "    _ = tmp[0, 0].scalar\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the whole thing with one Tensorflow op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average: 0.01714434 ms,   min: 0.013983 ms,   max: 0.033012 ms\r\n"
     ]
    }
   ],
   "source": [
    "time(repeating: 100) { _ = m1 • m2 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason that TensorFlow works in practice is that it can scale way up to large matrices, for example, lets try some thing a bit larger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1x1:\n",
      "  ⏰average: 0.19747569999999998 ms,   min: 0.134466 ms,   max: 0.406267 ms\n",
      "\n",
      "10x10:\n",
      "  ⏰average: 0.16372099999999998 ms,   min: 0.148772 ms,   max: 0.204141 ms\n",
      "\n",
      "100x100:\n",
      "  ⏰average: 0.1633207 ms,   min: 0.144609 ms,   max: 0.192945 ms\n",
      "\n",
      "1000x1000:\n",
      "  ⏰average: 0.49224870000000004 ms,   min: 0.475513 ms,   max: 0.517417 ms\n",
      "\n",
      "5000x5000:\n",
      "  ⏰average: 29.394594 ms,   min: 29.305466 ms,   max: 29.509257 ms\n"
     ]
    }
   ],
   "source": [
    "func timeMatmulTensor(size: Int) {\n",
    "    var matrix = Tensor<Float>(randomNormal: [size, size])\n",
    "    print(\"\\n\\(size)x\\(size):\\n  ⏰\", terminator: \"\")\n",
    "    time(repeating: 10) { \n",
    "        let matrix = matrix • matrix \n",
    "        _ = matrix[0, 0].scalar\n",
    "    }\n",
    "}\n",
    "\n",
    "timeMatmulTensor(size: 1)     // Tiny\n",
    "timeMatmulTensor(size: 10)    // Bigger\n",
    "timeMatmulTensor(size: 100)   // Even Bigger\n",
    "timeMatmulTensor(size: 1000)  // Biggerest\n",
    "timeMatmulTensor(size: 5000)  // Even Biggerest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In constrast, our simple CPU implementation takes a lot longer to do the same work.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1x1:\n",
      "  ⏰average: 0.00022530000000000003 ms,   min: 0.000202 ms,   max: 0.000286 ms\n",
      "\n",
      "10x10:\n",
      "  ⏰average: 0.0020340000000000002 ms,   min: 0.002015 ms,   max: 0.002108 ms\n",
      "\n",
      "100x100:\n",
      "  ⏰average: 1.7244304000000004 ms,   min: 1.67428 ms,   max: 1.829252 ms\n",
      "\n",
      "1000x1000:\n",
      "  ⏰average: 2287.614728 ms,   min: 2287.614728 ms,   max: 2287.614728 ms\n",
      "\n",
      "5000x5000: skipped, it takes tooo long!\n"
     ]
    }
   ],
   "source": [
    "func timeMatmulSwift(size: Int, repetitions: Int = 10) {\n",
    "    var matrix = Tensor<Float>(randomNormal: [size, size])\n",
    "    let matrixFlatArray = matrix.scalars\n",
    "\n",
    "    print(\"\\n\\(size)x\\(size):\\n  ⏰\", terminator: \"\")\n",
    "    time(repeating: repetitions) { \n",
    "       _ = swiftMatmulUnsafe(a: matrixFlatArray, b: matrixFlatArray, aDims: (size,size), bDims: (size,size))\n",
    "    }\n",
    "}\n",
    "\n",
    "timeMatmulSwift(size: 1)     // Tiny\n",
    "timeMatmulSwift(size: 10)    // Bigger\n",
    "timeMatmulSwift(size: 100)   // Even Bigger\n",
    "timeMatmulSwift(size: 1000, repetitions: 1)  // Biggerest\n",
    "\n",
    "print(\"\\n5000x5000: skipped, it takes tooo long!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is TensorFlow *so so so* much faster than our CPU implementation?  Well there are two reasons: the first of which is that it uses GPU hardware, which is much faster for math like this.  That said, there are a ton of tricks (involving memory hierarchies, cache blocking, and other tricks) that make matrix multiplications go fast on CPUs and other hardware.\n",
    "\n",
    "For example, try using TensorFlow on the CPU to do the same computation as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1x1:\n",
      "  ⏰average: 0.037117899999999995 ms,   min: 0.033852 ms,   max: 0.045559 ms\n",
      "\n",
      "10x10:\n",
      "  ⏰average: 0.0428332 ms,   min: 0.038875 ms,   max: 0.057927 ms\n",
      "\n",
      "100x100:\n",
      "  ⏰average: 0.11528369999999999 ms,   min: 0.087688 ms,   max: 0.148416 ms\n",
      "\n",
      "1000x1000:\n",
      "  ⏰average: 6.723326900000001 ms,   min: 5.79201 ms,   max: 7.207141 ms\n",
      "\n",
      "5000x5000:\n",
      "  ⏰average: 828.9578357999999 ms,   min: 751.708676 ms,   max: 856.148943 ms\n"
     ]
    }
   ],
   "source": [
    "withDevice(.cpu) {\n",
    "    timeMatmulTensor(size: 1)     // Tiny\n",
    "    timeMatmulTensor(size: 10)    // Bigger\n",
    "    timeMatmulTensor(size: 100)   // Even Bigger\n",
    "    timeMatmulTensor(size: 1000)  // Biggerest\n",
    "    timeMatmulTensor(size: 5000)  // Even Biggerest\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty big difference.  On my hardware, it takes 2287ms for Swift to do a 1000x1000 multiply on the CPU, it takes TensorFlow 6.7ms to do the same work on the CPU, and takes TensorFlow 0.49ms to do it on a GPU.\n",
    "\n",
    "# Accelerators vs Flexibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the big challenges with machine learning frameworks today is that they provide a fixed set of \"ops\" that you can use with high performance.  There is a lot of work underway to fix this.  The [XLA compiler in TensorFlow](https://www.tensorflow.org/xla) is an important piece of this, which allows more flexibility in the programming model while still providing high performance by using compilers to target the hardware accelerator.  If you're interested \n",
    "\n",
    "There are other efforts as well, and one of the major things we're working on is called MLIR.\n",
    "\n",
    "\n",
    "\n",
    "Halide video snippet here - Jeremy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebookToScript(fname: Path.cwd / \"01_matmul.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
