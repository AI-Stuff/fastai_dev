{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%install-location $cwd/swift-install\n",
    "%install '.package(path: \"$cwd/FastaiNotebook_08a_heterogeneous_dictionary\")' FastaiNotebook_08a_heterogeneous_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "import Path\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebook_08a_heterogeneous_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let path = downloadImagenette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let il = ItemList(fromFolder: path, extensions: [\"jpeg\", \"jpg\"])\n",
    "let sd = SplitData(il, fromFunc: {grandParentSplitter(fName: $0, valid: \"val\")})\n",
    "var procLabel = CategoryProcessor()\n",
    "let sld = makeLabeledData(sd, fromFunc: parentLabeler, procLabel: &procLabel)\n",
    "let rawData = sld.toDataBunch(itemToTensor: pathsToTensor, labelToTensor: intsToTensor)\n",
    "let data = transformData(rawData, tfmItem: { openAndResize(fname: $0, size: 128) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func modelInit() -> CNNModel { return CNNModel(channelIn: 3, nOut: 10, filters: [64, 64, 128, 256]) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "//Expandable enum to have tab-complete and typo-proof for the hyper-param names\n",
    "public struct HyperParams {\n",
    "    public static let lr = \"learningRate\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public protocol StatDelegate {\n",
    "    var name: String {get}\n",
    "    var defaultHPs: [String:Float] {get}\n",
    "    \n",
    "    func update(_ state: inout [String:TF], p: TF, g: TF, hps: inout [String:Float])\n",
    "}\n",
    "\n",
    "public protocol StepDelegate {\n",
    "    var defaultHPs: [String:Float] {get}\n",
    "    \n",
    "    func update(_ p: inout TF, g: inout TF, state: [String:TF], hps: inout [String:Float])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public extension Tensor where Scalar: Numeric {\n",
    "    mutating func resetTo0() {\n",
    "        self = Tensor(0)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func mergeDicts(_ dicts: inout [[String:Float]], _ newDict: [String:Float]) {\n",
    "    for i in dicts.indices { \n",
    "        dicts[i].merge(newDict) { (_, new) in new } \n",
    "    }\n",
    "}\n",
    "\n",
    "public func mergeDicts(_ dicts: inout [[String:Float]], _ newDicts: [[String:Float]]) {\n",
    "    for i in dicts.indices { \n",
    "        dicts[i].merge(newDicts[i]) { (_, new) in new } \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public func initState<Model: Layer>(for model: Model) -> Model.AllDifferentiableVariables{\n",
    "    var res = model.allDifferentiableVariables\n",
    "    for kp in res.keyPaths { res[keyPath: kp].resetTo0() }\n",
    "    return res\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public class StatefulOptimizer<Model: Layer>\n",
    "    where Model.AllDifferentiableVariables == Model.CotangentVector {\n",
    "    public typealias SplitDict = [WritableKeyPath<Model.AllDifferentiableVariables, Tensor<Float>>: Int]\n",
    "    public var hpGroups: [[String:Float]]\n",
    "    public var splitDict: SplitDict\n",
    "    public var states: [String: Model.AllDifferentiableVariables]\n",
    "    public var stats: [StatDelegate]\n",
    "    public var steppers: [StepDelegate]\n",
    "    public init(        \n",
    "        for model: __shared Model,\n",
    "        steppers: [StepDelegate],\n",
    "        stats: [StatDelegate],\n",
    "        hpGroups: [[String:Float]],\n",
    "        splitArray: [[WritableKeyPath<Model.AllDifferentiableVariables, Tensor<Float>>]]\n",
    "    ) {\n",
    "        self.hpGroups = Array(repeating: [:], count: hpGroups.count)\n",
    "        (self.steppers,self.stats) = (steppers,stats)\n",
    "        self.splitDict = SplitDict(uniqueKeysWithValues: splitArray.enumerated().flatMap { i, arr in\n",
    "                                                                                          arr.map { ($0, i) } })\n",
    "        states = [:]\n",
    "        steppers.forEach { mergeDicts(&self.hpGroups, $0.defaultHPs) }\n",
    "        stats.forEach { stat in\n",
    "            mergeDicts(&self.hpGroups, stat.defaultHPs)\n",
    "            states[stat.name] = initState(for: model)\n",
    "        }\n",
    "        mergeDicts(&self.hpGroups, hpGroups)\n",
    "    }\n",
    "        \n",
    "    public func update(\n",
    "        _ model: inout Model.AllDifferentiableVariables,\n",
    "        along direction: Model.CotangentVector\n",
    "    ) {\n",
    "        for kp in model.keyPaths {\n",
    "            var grad = direction[keyPath: kp]\n",
    "            var state = states.mapValues(){$0[keyPath: kp]}\n",
    "            var hps = hpGroups[splitDict[kp]!]\n",
    "            stats.forEach() { $0.update(&state, p: model[keyPath: kp], g: grad, hps: &hps) }\n",
    "            for n in states.keys { states[n]![keyPath: kp] = state[n]! }\n",
    "            steppers.forEach() { $0.update(&model[keyPath: kp], g: &grad, state: state, hps: &hps) }\n",
    "            hpGroups[splitDict[kp]!] = hps\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conformance to the optimizer protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "extension StatefulOptimizer: Optimizer{\n",
    "    public var learningRate: Float {\n",
    "        get { return hpGroups.last![HyperParams.lr]! } \n",
    "        set { \n",
    "            for i in hpGroups.indices {self.hpGroups[i][HyperParams.lr] = newValue }\n",
    "        }\n",
    "    }\n",
    "    //For discriminative learning rates\n",
    "    public var learningRates: [Float] {\n",
    "        get { return hpGroups.map { $0[HyperParams.lr]! } }\n",
    "        set { \n",
    "            for i in hpGroups.indices {self.hpGroups[i][HyperParams.lr] = newValue[i] } \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convenience init when there are no param groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "extension StatefulOptimizer{\n",
    "    public convenience init (for model: __shared Model,\n",
    "                             steppers: [StepDelegate],\n",
    "                             stats: [StatDelegate],\n",
    "                             hps: [String:Float]) {\n",
    "        self.init(for: model,\n",
    "                  steppers: steppers,\n",
    "                  stats: stats,\n",
    "                  hpGroups: [hps],\n",
    "                  splitArray: [model.allDifferentiableVariables.recursivelyAllWritableKeyPaths(to: Tensor<Float>)])\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct SGDStep: StepDelegate {\n",
    "    public var defaultHPs: [String: Float] { return [HyperParams.lr: 3e-3] }\n",
    "    public init() {}\n",
    "    public func update(_ p: inout TF, g: inout TF, state: [String:TF], hps: inout [String:Float]) {\n",
    "        p -= g * hps[HyperParams.lr]!\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//func splitFunc(_ a: Int) -> Int { return a < 2 ? 0 : 1 } to split\n",
    "//var configs = [HeterogeneousDictionary(HyperParams.lr, 0.0), HeterogeneousDictionary(HyperParams.lr, 0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var hps: [String:Float] = [HyperParams.lr: 0.01]\n",
    "func optFunc(_ model: CNNModel) -> StatefulOptimizer<CNNModel> {\n",
    "    return StatefulOptimizer(for: model, steppers: [SGDStep()], stats: [], hps: hps)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "var recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public extension HyperParams {\n",
    "    static let wd = \"weightDecay\"\n",
    "}\n",
    "\n",
    "public struct WeightDecay: StepDelegate {\n",
    "    public var defaultHPs: [String: Float] { return [HyperParams.wd: 0] }\n",
    "    public init() {}\n",
    "    public func update(_ p: inout TF, g: inout TF, state: [String:TF], hps: inout [String:Float]) {\n",
    "        p *= 1 - hps[HyperParams.lr]! * hps[HyperParams.wd]!\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct L2Regularization: StepDelegate {\n",
    "    public var defaultHPs: [String: Float] { return [HyperParams.wd: 0] }\n",
    "    public init() {}\n",
    "    public func update(_ p: inout TF, g: inout TF, state: [String:TF], hps: inout [String:Float]) {\n",
    "        g += hps[HyperParams.wd]! * p\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "//Expandable enum to have tab completes/typo-proof for state variable names.\n",
    "public struct StateKeys {\n",
    "    public static let avgGrad = \"averageGrad\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public extension HyperParams {\n",
    "    static let mom = \"momentum\"\n",
    "    static let momDamp = \"dampening\"\n",
    "}\n",
    "\n",
    "public struct AverageGrad: StatDelegate {\n",
    "    public var defaultHPs: [String: Float] { return [HyperParams.mom: 0.9] }\n",
    "    public let dampened: Bool\n",
    "    public init(dampened: Bool = false) { self.dampened = dampened }\n",
    "    public var name: String { return StateKeys.avgGrad }\n",
    "    public func update(_ state: inout [String: TF], p: TF, g: TF, hps: inout [String:Float]) {\n",
    "        state[StateKeys.avgGrad]! *= hps[HyperParams.mom]!\n",
    "        hps[HyperParams.momDamp] = 1.0 - (dampened ? hps[HyperParams.mom]! : 0.0)\n",
    "        state[StateKeys.avgGrad]! += hps[HyperParams.momDamp]! * g\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct MomentumStep: StepDelegate {\n",
    "    public var defaultHPs: [String: Float] = [:]\n",
    "    public init() {}\n",
    "    public func update(_ p: inout TF, g: inout TF, state: [String: TF], hps: inout [String:Float]) {\n",
    "        p -= state[StateKeys.avgGrad]! * hps[HyperParams.lr]!\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let hps: [String:Float] = [HyperParams.lr: 0.01]\n",
    "func optFunc(_ model: CNNModel) -> StatefulOptimizer<CNNModel> {\n",
    "    return StatefulOptimizer(for: model, steppers: [MomentumStep()], stats: [AverageGrad()], hps: hps)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "var recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.opt.hpGroups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public extension HyperParams {\n",
    "    static let ²mom = \"momentumSquares\"\n",
    "    static let ²momDamp = \"dampeningSquares\"\n",
    "}\n",
    "\n",
    "public extension StateKeys {\n",
    "    static let avgSqr = \"averageSquaredGrad\"\n",
    "}\n",
    "\n",
    "public struct AverageSquaredGrad: StatDelegate {\n",
    "    let dampened: Bool\n",
    "    public init(dampened: Bool = true) { self.dampened = dampened }\n",
    "    public var name: String { return StateKeys.avgSqr }\n",
    "    public var defaultHPs: [String: Float] { return [HyperParams.²mom: 0.99] }\n",
    "    public func update(_ state: inout [String: TF], p: TF, g: TF, hps: inout [String:Float]) {\n",
    "        state[StateKeys.avgSqr]! *= hps[HyperParams.²mom]!\n",
    "        hps[HyperParams.²momDamp] = 1.0 - (dampened ? hps[HyperParams.²mom]! : 0.0)\n",
    "        state[StateKeys.avgSqr]! += hps[HyperParams.²momDamp]! * g.squared()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public extension StateKeys {\n",
    "    static let step = \"stepCount\"\n",
    "}\n",
    "\n",
    "public struct StepCount: StatDelegate {\n",
    "    public var name: String { return StateKeys.step }\n",
    "    public var defaultHPs: [String:Float] = [:]\n",
    "    public init() {}\n",
    "    public func update(_ state: inout [String: TF], p: TF, g: TF, hps: inout [String:Float]) {\n",
    "        state[StateKeys.step]! += 1.0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "//public struct Epsilon: HetDictKey { public static var defaultValue: Float = 1e-5 }\n",
    "public extension HyperParams {\n",
    "    static let eps = \"epsilon\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct AdamStep: StepDelegate {\n",
    "    public var defaultHPs: [String: Float] { return [HyperParams.eps: 1e-5] }\n",
    "    public init() {}\n",
    "    public func update(_ p: inout TF, g: inout TF, state: [String: TF], hps: inout [String:Float]) {\n",
    "        let step = state[StateKeys.step]!\n",
    "        let (mom,damp) = (hps[HyperParams.mom]!,hps[HyperParams.momDamp]!)\n",
    "        let debias1 = damp * (1 - pow(mom, step)) / (1 - mom)\n",
    "        let num = debias1 * state[StateKeys.avgGrad]!\n",
    "        \n",
    "        let (²mom,²damp) = (hps[HyperParams.²mom]!,hps[HyperParams.²momDamp]!)\n",
    "        let debias2 = ²damp * (1 - pow(²mom, step)) / (1 - ²mom)\n",
    "        let denom = sqrt(state[StateKeys.avgSqr]!/debias2) + hps[HyperParams.eps]!\n",
    "        \n",
    "        p -= hps[HyperParams.lr]! * num / denom\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func optFunc(_ model: CNNModel) -> StatefulOptimizer<CNNModel> {\n",
    "    return StatefulOptimizer(\n",
    "        for: model,\n",
    "        steppers: [AdamStep()], \n",
    "        stats: [AverageGrad(dampened: true), AverageSquaredGrad(), StepCount()], \n",
    "        hps: [HyperParams.lr: 1e-3])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.opt.hpGroups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct LambStep: StepDelegate {\n",
    "    public var defaultHPs: [String: Float] { return [HyperParams.eps: 1e-6, HyperParams.wd: 0.0] }\n",
    "    public func update(_ p: inout TF, g: inout TF, state: [String: TF], hps: inout [String:Float]) {\n",
    "        let stepCount = state[StateKeys.step]!\n",
    "        let (mom,damp) = (hps[HyperParams.mom]!,hps[HyperParams.momDamp]!)\n",
    "        let debias1 = damp * (1 - pow(mom, stepCount)) / (1 - mom)\n",
    "        let num = debias1 * state[StateKeys.avgGrad]!\n",
    "        \n",
    "        let (²mom,²damp) = (hps[HyperParams.²mom]!,hps[HyperParams.²momDamp]!)\n",
    "        let debias2 = ²damp * (1 - pow(²mom, stepCount)) / (1 - ²mom)\n",
    "        let denom = sqrt(state[StateKeys.avgSqr]!/debias2) + hps[HyperParams.eps]!\n",
    "        \n",
    "        let step = num / denom + hps[HyperParams.wd]! * p\n",
    "        let r1 = sqrt((p * p).mean())\n",
    "        let r2 = sqrt((step * step).mean())\n",
    "        let factor = min(r1 / r2, Float(10.0))\n",
    "        p -= hps[HyperParams.lr]! * factor * step\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making convenience functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func sgdOpt<Model>(lr: Float, mom: Float = 0.9, wd: Float = 0.0, dampening: Bool = false\n",
    "                         ) -> ((Model) -> StatefulOptimizer<Model>) {\n",
    "    var steppers: [StepDelegate] = (mom != 0) ? [MomentumStep()] : [SGDStep()]\n",
    "    if wd != 0 { steppers.append(WeightDecay()) }\n",
    "    let stats = (mom != 0) ? [AverageGrad(dampened: dampening)] : []\n",
    "    var hps: [String: Float] = [HyperParams.lr: lr]\n",
    "    if mom != 0 { hps[HyperParams.mom] = mom }\n",
    "    if wd != 0  { hps[HyperParams.wd ] = wd  }\n",
    "    return {model in \n",
    "        return StatefulOptimizer(for: model, steppers: steppers, stats: stats, hps: hps)}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func adamOpt<Model>(lr: Float, mom: Float = 0.9, beta: Float=0.99, wd: Float = 0.0, eps: Float = 1e-5\n",
    "                         ) -> ((Model) -> StatefulOptimizer<Model>) {\n",
    "    var steppers: [StepDelegate] = [AdamStep()]\n",
    "    if wd != 0 { steppers.append(WeightDecay()) }\n",
    "    let stats: [StatDelegate] = [AverageGrad(dampened: true), AverageSquaredGrad(), StepCount()]\n",
    "    var hps: [String: Float] = [HyperParams.lr: lr]\n",
    "    hps[HyperParams.mom] = mom\n",
    "    hps[HyperParams.²mom] = beta\n",
    "    hps[HyperParams.eps] = eps\n",
    "    if wd != 0  { hps[HyperParams.wd ] = wd  }\n",
    "    return {model in \n",
    "        return StatefulOptimizer(for: model, steppers: steppers, stats: stats, hps: hps)}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schedule the hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public extension StatefulOptimizer {\n",
    "    func setParam(_ hp: String, _ val: Float) {\n",
    "        for i in 0..<hpGroups.count { hpGroups[i][hp] = val }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "extension Learner where Opt.Scalar: BinaryFloatingPoint, \n",
    "    Opt.Model.AllDifferentiableVariables == Opt.Model.CotangentVector{\n",
    "    public class ParamScheduler: Delegate {\n",
    "        public override var order: Int { return 1 }\n",
    "        public typealias ScheduleFunc = (Float) -> Float\n",
    "\n",
    "        // A learning rate schedule from step to float.\n",
    "        public var scheduler: ScheduleFunc\n",
    "        public let hp: String\n",
    "        \n",
    "        public init(scheduler: @escaping (Float) -> Float, hp: String) {\n",
    "            (self.scheduler,self.hp) = (scheduler,hp)\n",
    "        }\n",
    "        \n",
    "        override public func batchWillStart(learner: Learner) {\n",
    "            let val = scheduler(learner.pctEpochs/Float(learner.epochCount))\n",
    "            (learner.opt as! StatefulOptimizer<Opt.Model>).setParam(hp, val)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    public func makeParamScheduler(_ scheduler: @escaping (Float) -> Float, hp: String) -> ParamScheduler {\n",
    "        return ParamScheduler(scheduler: scheduler, hp: hp)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export \n",
    "public func oneCycleSchedulers(_ lrMax: Float, pctStart:Float=0.25, divStart: Float = 10, divEnd: Float = 1e5, \n",
    "                               moms: (Float,Float,Float) = (0.95,0.85,0.95)) \n",
    "-> ((Float) -> Float, (Float) -> Float){\n",
    "    let lrSched = combineSchedules(\n",
    "        pcts: [pctStart, 1-pctStart], \n",
    "        schedules: [makeAnnealer(start: lrMax/divStart, end: lrMax, schedule: cosineSchedule),\n",
    "                    makeAnnealer(start: lrMax, end: lrMax/divEnd, schedule: cosineSchedule)])\n",
    "    let momSched = combineSchedules(\n",
    "        pcts: [pctStart, 1-pctStart], \n",
    "        schedules: [makeAnnealer(start: moms.0, end: moms.1, schedule: cosineSchedule),\n",
    "                    makeAnnealer(start: moms.1, end: moms.2, schedule: cosineSchedule)])\n",
    "    return (lrSched, momSched)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "extension Learner where Opt.Scalar: BinaryFloatingPoint, \n",
    "    Opt.Model.AllDifferentiableVariables == Opt.Model.CotangentVector{\n",
    "\n",
    "    public func addOneCycleDelegates(_ lrMax: Float, pctStart:Float=0.25, divStart: Float = 10, divEnd: Float = 1e5, \n",
    "                               moms: (Float,Float,Float) = (0.95,0.85,0.95)) {\n",
    "        let scheds = oneCycleSchedulers(lrMax, pctStart: pctStart, divStart: divStart, divEnd: divEnd, moms: moms)\n",
    "        addDelegates([makeParamScheduler(scheds.0 , hp: HyperParams.lr), \n",
    "                      makeParamScheduler(scheds.1 , hp: HyperParams.mom)])\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let optFunc: (CNNModel) -> StatefulOptimizer<CNNModel> = adamOpt(lr: 1e-3, mom: 0.9, beta: 0.99, wd: 1e-2, eps: 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.addOneCycleDelegates(1e-3)\n",
    "learner.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder.plotLRs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NotebookExport\n",
    "let exporter = NotebookExport(Path.cwd/\"09_optimizer.ipynb\")\n",
    "print(exporter.export(usingPrefix: \"FastaiNotebook_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
