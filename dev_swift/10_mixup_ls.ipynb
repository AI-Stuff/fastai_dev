{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/home/ubuntu/fastai_docs/dev_swift/FastaiNotebook_09_optimizer\")\n",
      "\t\tFastaiNotebook_09_optimizer\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmp2c06fhas/swift-install\n",
      "Fetching https://github.com/mxcl/Path.swift\n",
      "Fetching https://github.com/JustHTTP/Just\n",
      "Completed resolution in 4.02s\n",
      "Cloning https://github.com/mxcl/Path.swift\n",
      "Resolving https://github.com/mxcl/Path.swift at 0.16.2\n",
      "Cloning https://github.com/JustHTTP/Just\n",
      "Resolving https://github.com/JustHTTP/Just at 0.7.1\n",
      "Compile Swift Module 'Path' (9 sources)\n",
      "Compile Swift Module 'Just' (1 sources)\n",
      "Compile Swift Module 'FastaiNotebook_09_optimizer' (14 sources)\n",
      "Compile Swift Module 'jupyterInstalledPackages' (1 sources)\n",
      "Linking ./.build/x86_64-unknown-linux/debug/libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install '.package(path: \"$cwd/FastaiNotebook_09_optimizer\")' FastaiNotebook_09_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('inline', 'module://ipykernel.pylab.backend_inline')\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import FastaiNotebook_09_optimizer\n",
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "import Path\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//TODO: switch to imagenette when possible to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "let data = mnistDataBunch(flat: true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (n,m) = (60000,784)\n",
    "let c = 10\n",
    "let nHid = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "func modelInit() -> BasicModel {return BasicModel(nIn: m, nHid: nHid, nOut: c)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "func optFunc(_ model: BasicModel) -> StatefulOptimizer<BasicModel> {\n",
    "    var config = HeterogeneousDictionary(HyperParams.lr, 0.1)\n",
    "    var steppers = [SGDStep()]\n",
    "    return StatefulOptimizer(for: model, stepDelegates: steppers, statDelegates: [], config: config)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "error: <Cell 8>:1:29: error: cannot convert value of type 'DataBunch<DataBatch<TF, TI>>' (aka 'DataBunch<DataBatch<Tensor<Float>, Tensor<Int32>>>') to expected argument type 'DataBunch<DataBatch<Tensor<Float>, _>>'\nlet learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n                            ^~~~\n\n"
     ]
    }
   ],
   "source": [
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "func optFunc(_ model: BasicModel) -> SGD<BasicModel> {return SGD(for: model, learningRate: 1e-2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension RandomDistribution {\n",
    "    // Returns a batch of samples.\n",
    "    func next<G: RandomNumberGenerator>(\n",
    "        _ count: Int, using generator: inout G\n",
    "    ) -> [Sample] {\n",
    "        var result: [Sample] = []\n",
    "        for _ in 0..<count {\n",
    "            result.append(next(using: &generator))\n",
    "        }\n",
    "        return result\n",
    "    }\n",
    "\n",
    "    // Returns a batch of samples, using the global Threefry RNG.\n",
    "    func next(_ count: Int) -> [Sample] {\n",
    "        return next(count, using: &ThreefryRandomNumberGenerator.global)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "error: <Cell 17>:26:10: error: class members cannot be marked with '@differentiable'\n        @differentiable\n        ~^~~~~~~~~~~~~~\n        \n\nerror: <Cell 17>:32:58: error: 'MixupDelegate' is ambiguous for type lookup in this context\n    public func makeMixupDelegate(alpha: Float = 0.4) -> MixupDelegate {\n                                                         ^~~~~~~~~~~~~\n\n<Cell 8>:2:18: note: found this candidate\n    public class MixupDelegate: Delegate {\n                 ^\n\n<REPL Input>:2:18: note: found this candidate\n    public class MixupDelegate: Delegate {\n                 ^\n\n<REPL Input>:2:18: note: found this candidate\n    public class MixupDelegate: Delegate {\n                 ^\n\n<REPL Input>:2:18: note: found this candidate\n    public class MixupDelegate: Delegate {\n                 ^\n\n<REPL Input>:2:18: note: found this candidate\n    public class MixupDelegate: Delegate {\n                 ^\n\n<REPL Input>:2:18: note: found this candidate\n    public class MixupDelegate: Delegate {\n                 ^\n\n<REPL Input>:2:18: note: found this candidate\n    public class MixupDelegate: Delegate {\n                 ^\n\n<Cell 17>:2:18: note: found this candidate\n    public class MixupDelegate: Delegate {\n                 ^\n\nwarning: <Cell 17>:21:78: warning: treating a forced downcast to 'Label' as optional will never produce 'nil'\n                    learner.currentTarget = (lambda * yb + (1-lambda) * yba) as! Label\n                                                                             ^\n\n<Cell 17>:21:80: note: use 'as?' to perform a conditional downcast to 'Label'\n                    learner.currentTarget = (lambda * yb + (1-lambda) * yba) as! Label\n                                                                               ^\n                                                                               ?\n\n<Cell 17>:21:45: note: add parentheses around the cast to silence this warning\n                    learner.currentTarget = (lambda * yb + (1-lambda) * yba) as! Label\n                                            ^\n                                            (                                         )\n\n"
     ]
    }
   ],
   "source": [
    "extension Learner{\n",
    "    public class MixupDelegate: Delegate {\n",
    "        private var distribution: BetaDistribution\n",
    "        private var lambda: Tensor<Float> = .zero\n",
    "        private var yba: Label? = nil\n",
    "        \n",
    "        public init(alpha: Float = 0.4){\n",
    "            distribution = BetaDistribution(alpha: alpha, beta: alpha)\n",
    "        }\n",
    "        \n",
    "        override public func trainingWillStart(learner: Learner) {\n",
    "            learn.lossFunction.f = \n",
    "        }\n",
    "        \n",
    "        override public func batchWillStart(learner: Learner) {\n",
    "            if let xb = learner.currentInput {\n",
    "                if let yb = learner.currentTarget as? Tensor<Float>{\n",
    "                    var lambda = Tensor<Float>(distribution.next(Int(yb.shape[0])))\n",
    "                    lambda = max(lambda, 1-lambda)\n",
    "                    let shuffle = Raw.randomShuffle(value: Tensor<Int32>(0..<Int32(yb.shape[0])))\n",
    "                    let xba = Raw.gather(params: xb, indices: shuffle)\n",
    "                    let yba = Raw.gather(params: yb, indices: shuffle)\n",
    "                    lambda = lambda.expandingShape(at: 1)\n",
    "                    learner.currentInput = lambda * xb + (1-lambda) * xba\n",
    "                    learner.lossFunction.f = { }\n",
    "                    learner.currentTarget = (lambda * yb + (1-lambda) * yba) as! Label\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    public func makeMixupDelegate(alpha: Float = 0.4) -> MixupDelegate {\n",
    "        return MixupDelegate(alpha: alpha)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (n,m) = (60000,784)\n",
    "let c = 10\n",
    "let nHid = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "func opt(_ model: BasicModel) -> SGD<BasicModel> {return SGD(for: model, learningRate: 1e-2)}\n",
    "func modelInit() -> BasicModel {return BasicModel(nIn: m, nHid: nHid, nOut: c)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossFunction: softmaxCrossEntropy, optimizer: opt, initializingWith: modelInit)\n",
    "let recorder = learner.makeRecorder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.delegates = [learner.makeTrainEvalDelegate(), learner.makeShowProgress(), \n",
    "                     learner.makeAvgMetric(metrics: [accuracy]), recorder,\n",
    "                     learner.makeMixupDelegate(alpha: 0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: [0.2437941, 0.9217]                                                    \n",
      "Epoch 1: [0.09880526, 0.9692]                                                   \n",
      "                                                                              \r"
     ]
    }
   ],
   "source": [
    "learner.fit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
