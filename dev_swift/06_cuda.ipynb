{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A CNN Mnist Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%install '.package(path: \"$cwd/FastaiNotebook_05b_early_stopping\")' FastaiNotebook_05b_early_stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebook_05b_early_stopping\n",
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "import Path\n",
    "import TensorFlow\n",
    "import Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let plt = Python.import(\"matplotlib.pyplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let data = mnistDataBunch(flat: false, bs: 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let firstBatch = data.train.first(where: { _ in true })!\n",
    "let batchShape = firstBatch.xb.shape\n",
    "let batchSize = batchShape.dimensions[0]\n",
    "let exampleSideSize = batchShape.dimensions[1]\n",
    "assert(exampleSideSize == batchShape.dimensions[2])\n",
    "print(\"Batch size: \\(batchSize)\")\n",
    "print(\"Example side size: \\(exampleSideSize)\")\n",
    "\n",
    "let classCount = firstBatch.yb.shape.dimensions[1]\n",
    "print(\"Class count: \\(classCount)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export \n",
    "public struct CnnModel: Layer {\n",
    "    public var reshapeToSquare: FAReshape<Float>\n",
    "    public var conv1: FAConv2D<Float>\n",
    "    public var conv2: FAConv2D<Float>\n",
    "    public var conv3: FAConv2D<Float>\n",
    "    public var conv4: FAConv2D<Float>\n",
    "    public var pool = FAAvgPool2D<Float>(poolSize: (2, 2), strides: (1, 1)) //TODO: replace by AvgPool\n",
    "    public var flatten = FAFlatten<Float>()\n",
    "    public var linear: FADense<Float>\n",
    "    \n",
    "    public init(sizeIn: Int, channelIn:Int, channelOut:Int, nFilters:[Int]) {\n",
    "        reshapeToSquare = FAReshape<Float>([-1, Int32(sizeIn), Int32(sizeIn), Int32(channelIn)])\n",
    "        conv1 = FAConv2D<Float>(\n",
    "            filterShape: (5, 5, 1, nFilters[0]), \n",
    "            strides: (2, 2), \n",
    "            padding: .same, \n",
    "            activation: relu)\n",
    "        conv2 = FAConv2D<Float>(\n",
    "            filterShape: (3, 3, nFilters[0], nFilters[1]),\n",
    "            strides: (2, 2),\n",
    "            padding: .same,\n",
    "            activation: relu)\n",
    "        conv3 = FAConv2D<Float>(\n",
    "            filterShape: (3, 3, nFilters[1], nFilters[2]),\n",
    "            strides: (2, 2),\n",
    "            padding: .same,\n",
    "            activation: relu)\n",
    "        conv4 = FAConv2D<Float>(\n",
    "            filterShape: (3, 3, nFilters[2], nFilters[3]),\n",
    "            strides: (2, 2),\n",
    "            padding: .same,\n",
    "            activation: relu)\n",
    "        linear = FADense<Float>(inputSize: nFilters[3], outputSize: channelOut)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func applied(to input: Tensor<Float>, in context: Context) -> Tensor<Float> {\n",
    "        // There isn't a \"sequenced\" defined with enough layers.\n",
    "        let intermediate =  input.sequenced(\n",
    "            in: context,\n",
    "            through: reshapeToSquare, conv1, conv2, conv3, conv4)\n",
    "        return intermediate.sequenced(in: context, through: pool, flatten, linear)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let model = CnnModel(sizeIn:28, channelIn: 1, channelOut: 10, nFilters: [8, 16, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Test that data goes through the model as expected.\n",
    "let predictions = model.applied(to: firstBatch.xb, in: Context(learningPhase: .training))\n",
    "print(predictions.shape)\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare training on CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let opt = SGD<CnnModel, Float>(learningRate: 0.4)\n",
    "func modelInit() -> CnnModel { return CnnModel(sizeIn:28, channelIn: 1, channelOut: 10, nFilters: [8, 16, 32, 32]) }\n",
    "\n",
    "// TODO: When TF-421 is fixed, switch back to the normal `softmaxCrossEntropy`.\n",
    "\n",
    "@differentiable(vjp: _vjpSoftmaxCrossEntropy)\n",
    "func softmaxCrossEntropy1<Scalar: TensorFlowFloatingPoint>(\n",
    "    _ features: Tensor<Scalar>, _ labels: Tensor<Scalar>\n",
    ") -> Tensor<Scalar> {\n",
    "    return Raw.softmaxCrossEntropyWithLogits(features: features, labels: labels).loss.mean()\n",
    "}\n",
    "\n",
    "@usableFromInline\n",
    "func _vjpSoftmaxCrossEntropy<Scalar: TensorFlowFloatingPoint>(\n",
    "    features: Tensor<Scalar>, labels: Tensor<Scalar>\n",
    ") -> (Tensor<Scalar>, (Tensor<Scalar>) -> (Tensor<Scalar>, Tensor<Scalar>)) {\n",
    "    let (loss, grad) = Raw.softmaxCrossEntropyWithLogits(features: features, labels: labels)\n",
    "    let batchSize = Tensor<Scalar>(features.shapeTensor[0])\n",
    "    return (loss.mean(), { v in ((v / batchSize) * grad, Tensor<Scalar>(0)) })\n",
    "}\n",
    "\n",
    "let learner = Learner(data: data, lossFunction: softmaxCrossEntropy1, optimizer: opt, initializingWith: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// This happens on the GPU (if you have one and it's configured correctly).\n",
    "// I tried this on a GCE 8vCPU 30GB + Tesla P100:\n",
    "// - time: ~4.3s\n",
    "// - nvidia-smi shows ~10% GPU-Util while this is running\n",
    "time {\n",
    "    try! learner.fit(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// This happens on the CPU.\n",
    "// I tried this on a GCE 8vCPU 30GB + Tesla P100:\n",
    "// - time: ~6.3s\n",
    "// - nvidia-smi shows 0% GPU-Util while this is running\n",
    "time {\n",
    "    withDevice(.cpu) {\n",
    "        try! learner.fit(1)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Layer Activation Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationStatistics: LayerDelegate<Tensor<Float>> {\n",
    "    var activationMeans: [Float] = []\n",
    "    var activationStds: [Float] = []    \n",
    "    override func didProduceActivation(_ activation: Tensor<Float>, in context: Context) {\n",
    "        guard context.learningPhase == .training else { return }\n",
    "        activationMeans.append(activation.mean().scalar!)\n",
    "        activationStds.append(activation.standardDeviation().reshaped(to: []).scalar!)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Utility function for getting all the delegates of a certain type of layer.\n",
    "// Alternatively, we could ask for all the delegates in the model, but then we'd also get delegates for\n",
    "// uninteresting layers like reshape layers.\n",
    "// TODO: I have no idea if it preserves order.\n",
    "extension KeyPathIterable {\n",
    "    func layerDelegates<T: FALayer>(of layer: T.Type) -> [WritableKeyPath<Self, LayerDelegate<T.Output>>] {\n",
    "        return recursivelyAllWritableKeyPaths(to: layer).map { kp in\n",
    "            return kp.appending(path: \\T.delegate)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossFunction: softmaxCrossEntropy1, optimizer: opt, initializingWith: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "\n",
    "let interestingLayerDelegates = learner.model.layerDelegates(of: FAConv2D<Float>.self) + [\n",
    "    \\CnnModel.pool.delegate,\n",
    "    \\CnnModel.linear.delegate\n",
    "]\n",
    "\n",
    "interestingLayerDelegates.forEach { learner.model[keyPath: $0] = ActivationStatistics() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// This LayerDelegate stuff slows it down to ~6s/epoch.\n",
    "time {\n",
    "    try! learner.fit(2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kp in interestingLayerDelegates {\n",
    "    plt.plot((learner.model[keyPath: kp] as! ActivationStatistics).activationMeans)\n",
    "}\n",
    "plt.legend(Array(1...interestingLayerDelegates.count))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kp in interestingLayerDelegates {\n",
    "    plt.plot((learner.model[keyPath: kp] as! ActivationStatistics).activationStds)\n",
    "}\n",
    "plt.legend(Array(1...interestingLayerDelegates.count))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebookToScript(fname: (Path.cwd / \"06_cuda.ipynb\").string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
