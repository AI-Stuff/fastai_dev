{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/home/ubuntu/fastai_docs/dev_swift/FastaiNotebook_10_mixup_ls\")\n",
      "\t\tFastaiNotebook_10_mixup_ls\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmpgqkdyi3g/swift-install\n",
      "Compile Swift Module 'jupyterInstalledPackages' (1 sources)\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install-location $cwd/swift-install\n",
    "%install '.package(path: \"$cwd/FastaiNotebook_10_mixup_ls\")' FastaiNotebook_10_mixup_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "import Path\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebook_10_mixup_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data using the functions designed in 08. The first one is a helper function to download the dataset and returns the path where it's decompressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let path = downloadImagette()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the data block API in swift: first we grab recursively all the jpeg files in `path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let il = ItemList(fromFolder: path, extensions: [\"jpeg\", \"jpg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split them according to the grandparent folder. `train` for the training set (which is the default) and `val` for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let sd = SplitData(il, fromFunc: {grandParentSplitter(fName: $0, valid: \"val\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our processors for the training set and the validation set. The difference with python is that we have to specify a noop processor (with a type) when we don't want to do anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var (procItem,procLabel) = (NoopProcessor<Path>(),CategoryProcessor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can label our data using the parent directory and those two processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let sld = SplitLabeledData(sd, fromFunc: parentLabeler, procItem: &procItem, procLabel: &procLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then convert to a databunch by specifying two function that will convert our items and our labels to `Tensor`. For the items we use `pathsToTensor` that converts the `Path` object to their string representation then `StringTensor`. For the labels,  the function `intsToTensor` just convert the indices we have in proper tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let rawData = sld.toDataBunch(itemToTensor: pathsToTensor, labelToTensor: intsToTensor, bs: 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference with python is that the transforms are all applied directly on the datasets by `tf.data`. Even opening on the image is such a transform, that will take a `StringTensor` and return a tensor of `UInt8`. We have written a function that opens the image in a filename and returns it decoded and resized to `size`, which is the transform we apply to our items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let data = transformData(rawData, tfmItem: { openAndResize(fname: $0, size: 128) })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then have a look by grabbing one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let batch = data.train.oneBatch()!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our batches have two attributes `xb` and `yb` that contain the inputs and targets respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch.xb.shape)\n",
    "print(batch.yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we decode the labels suing our processor, we can plot images with their corresponding classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let labels = batch.yb.scalars.map { procLabel.vocab![Int($0)] }\n",
    "showImages(batch.xb, labels: labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XResnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the same xresnet we had in fastai over PyTorch. Modules in S4TF are `struct` that conform to the `Layer` protocol. You define any layer it uses as attributes that you have to properly set in the `init` function. The equivalent of `forward` in PyTorch is `call`.\n",
    "\n",
    "We are using our custom fastai layers that have the prefix `FA` because they contain experimental features that might eventually be merged in S4TF. There is a NoBiasConv layer separate from the Conv Layer because S4TF doesn't yet support control flow. That means you can't have if statements or for loops in the `call` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct ConvLayer: Layer {\n",
    "    public var bn: FABatchNorm<Float>\n",
    "    public var conv: FANoBiasConv2D<Float>\n",
    "    \n",
    "    public init(_ cIn: Int, _ cOut: Int, ks: Int = 3, stride: Int = 1, zeroBn: Bool = false, act: Bool = true){\n",
    "        bn = FABatchNorm(featureCount: cOut)\n",
    "        conv = FANoBiasConv2D(cIn, cOut, ks: ks, stride: stride, activation: act ? relu : identity)\n",
    "        if zeroBn { bn.scale = Tensor(zeros: [cOut]) }\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func call(_ input: TF) -> TF {\n",
    "        return bn(conv(input))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However we will need some optional layers to have refactored resnet implementation: in the shortcuts, we sometimes apply an average pool or a convolution layer, but most of the time we don't do anything. To be able to have that in S4TF, we write a customized protocol called `SwitchableLayer`. It inherits from `Layer` and adds a boolean `isOn` and a differentiable `forward` function. A structure conforming to it will return the result of `forward` if `isOn` is `true`, otherwise it won't do anything.\n",
    "\n",
    "As we said before, if statements aren't differentiable (yet) but we can create a custom function that will manually compute the gradients of our switchable layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//A layer that you can switch off to do the identity instead\n",
    "public protocol SwitchableLayer: Layer {\n",
    "    associatedtype Input\n",
    "    var isOn: Bool {get set}\n",
    "    \n",
    "    @differentiable func forward(_ input: Input) -> Input\n",
    "}\n",
    "\n",
    "public extension SwitchableLayer {\n",
    "    func call(_ input: Input) -> Input {\n",
    "        return isOn ? forward(input) : input\n",
    "    }\n",
    "\n",
    "    @differentiating(call)\n",
    "    func gradForward(_ input: Input) ->\n",
    "        (value: Input, pullback: (Self.Input.CotangentVector) ->\n",
    "            (Self.CotangentVector, Self.Input.CotangentVector)) {\n",
    "        if isOn { return valueWithPullback(at: input) { $0.forward($1) } }\n",
    "        else { return (input, {v in return (Self.CotangentVector.zero, v)}) }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use this protocol to create a `MaybeAvgPool2D` layer and a `MaybeConv` layer. The little downside is that we will have to create a fake convolution for the `MaybeConv` that are just identity, but we will just create a 1x1x1x1 weight matrix so that it doesn't take much memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct MaybeAvgPool2D: SwitchableLayer {\n",
    "    var pool: FAAvgPool2D<Float>\n",
    "    @noDerivative public var isOn = false\n",
    "    \n",
    "    @differentiable public func forward(_ input: TF) -> TF { return pool(input) }\n",
    "    \n",
    "    public init(_ sz: Int) {\n",
    "        isOn = (sz > 1)\n",
    "        pool = FAAvgPool2D<Float>(sz)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct MaybeConv: SwitchableLayer {\n",
    "    var conv: ConvLayer\n",
    "    @noDerivative public var isOn = false\n",
    "    \n",
    "    @differentiable public func forward(_ input: TF) -> TF { return conv(input) }\n",
    "    \n",
    "    public init(_ cIn: Int, _ cOut: Int) {\n",
    "        isOn = (cIn > 1) || (cOut > 1)\n",
    "        conv = ConvLayer(cIn, cOut, ks: 1, act: false)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With those two maybe layers, we can write a `ResBlock` as we are used to. We can have array of layers that have the same type, and such an array can be treated as if it was a normal layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct ResBlock: Layer {\n",
    "    public var convs: [ConvLayer]\n",
    "    public var idConv: MaybeConv\n",
    "    public var pool: MaybeAvgPool2D\n",
    "    \n",
    "    public init(_ expansion: Int, _ ni: Int, _ nh: Int, stride: Int = 1){\n",
    "        let (nf, nin) = (nh*expansion,ni*expansion)\n",
    "        convs = [ConvLayer(nin, nh, ks: 1)]\n",
    "        convs += (expansion==1) ? [\n",
    "            ConvLayer(nh, nf, ks: 3, stride: stride, zeroBn: true, act: false)\n",
    "        ] : [\n",
    "            ConvLayer(nh, nh, ks: 3, stride: stride),\n",
    "            ConvLayer(nh, nf, ks: 1, zeroBn: true, act: false)\n",
    "        ]\n",
    "        idConv = nin==nf ? MaybeConv(1,1) : MaybeConv(nin, nf)\n",
    "        pool = MaybeAvgPool2D(stride)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func call(_ inp: TF) -> TF {\n",
    "        return relu(convs(inp) + idConv(pool(inp)))\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create our `XResnet` pretty much the same way as in pytorch. The list comprehesions are replaced by uses of `map` or `reduce`. The `makeLayer` function is out side of the `XResNet` structure because it can't be used in the `init` otherwise (in swift, you can only use self in the init when all the attributes have been properly set, and this `makeLayer` function is used to create the attribute `blocks`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func makeLayer(_ expansion: Int, _ ni: Int, _ nf: Int, _ nBlocks: Int, stride: Int) -> [ResBlock] {\n",
    "    return Array(0..<nBlocks).map { ResBlock(expansion, $0==0 ? ni : nf, nf, stride: $0==0 ? stride : 1) }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct XResNet: Layer {\n",
    "    public var stem: [ConvLayer]\n",
    "    public var maxPool = MaxPool2D<Float>(poolSize: (3,3), strides: (2,2), padding: .same)\n",
    "    public var blocks: [ResBlock]\n",
    "    public var pool = GlobalAvgPool2D<Float>()\n",
    "    public var linear: Dense<Float>\n",
    "    \n",
    "    public init(_ expansion: Int, _ layers: [Int], cIn: Int = 3, cOut: Int = 1000){\n",
    "        var nfs = [cIn, (cIn+1)*8, 64, 64]\n",
    "        stem = Array(0..<3).map{ ConvLayer(nfs[$0], nfs[$0+1], stride: $0==0 ? 2 : 1)}\n",
    "        nfs = [64/expansion,64,128,256,512]\n",
    "        blocks = Array(layers.enumerated()).map { (i,l) in \n",
    "            return makeLayer(expansion, nfs[i], nfs[i+1], l, stride: i==0 ? 1 : 2)\n",
    "        }.reduce([], +)\n",
    "        linear = Dense(inputSize: nfs.last!*expansion, outputSize: cOut)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func call(_ inp: TF) -> TF {\n",
    "        return linear(pool(blocks(maxPool(stem(inp)))))\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func xresnet18 (cIn: Int = 3, cOut: Int = 1000) -> XResNet { return XResNet(1, [2, 2, 2, 2], cIn: cIn, cOut: cOut) }\n",
    "func xresnet34 (cIn: Int = 3, cOut: Int = 1000) -> XResNet { return XResNet(1, [3, 4, 6, 3], cIn: cIn, cOut: cOut) }\n",
    "func xresnet50 (cIn: Int = 3, cOut: Int = 1000) -> XResNet { return XResNet(4, [3, 4, 6, 3], cIn: cIn, cOut: cOut) }\n",
    "func xresnet101(cIn: Int = 3, cOut: Int = 1000) -> XResNet { return XResNet(4, [3, 4, 23, 3], cIn: cIn, cOut: cOut) }\n",
    "func xresnet152(cIn: Int = 3, cOut: Int = 1000) -> XResNet { return XResNet(4, [3, 8, 36, 3], cIn: cIn, cOut: cOut) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a `Learner` we need our data, a model initializer function, and optimizer initializer function and a loss function. The model initilializer is a simple closure that returns the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func modelInit() -> XResNet { return xresnet34(cOut: 10) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer function is a convenience function we wrote in notebook 09 (like we had done in the python version) that returns a `StatefulOptimizer` with all the necessary stats/step delegates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let optFunc: (XResNet) -> StatefulOptimizer<XResNet> = AdamOpt(lr: 1e-3, mom: 0.9, beta: 0.99, wd: 1e-2, eps: 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create our `Learner`. The loss function is the classic cross entropy + softmax, and is given by S4TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@differentiable(wrt: out)\n",
    "func lossFunc(_ out: TF, _ targ: TI) -> TF { return labelSmoothingCrossEntropy(out, targ, ε: 0.1) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossFunc: lossFunc, optFunc: optFunc, modelInit: modelInit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In swift, callbacks are called delegates. We have written a convenience function to automatically add the basic ones we need (train/eval, the recorder, metrics progress bar). This function returns the `recorder` if we want to look at losses or do some plots later.\n",
    "\n",
    "Then we add the delegate to normalize our inputs with the statistics of ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.addDelegate(learner.makeNormalize(mean: imagenetStats.mean, std: imagenetStats.std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can fit with the 1cycle policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.addOneCycleDelegates(5e-3, pctStart: 0.5)\n",
    "learner.fit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
